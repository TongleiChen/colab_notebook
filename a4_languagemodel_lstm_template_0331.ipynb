{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_languagemodel_lstm_template_0331.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXV9QlMN0DQ3"
      },
      "source": [
        "## A4 - Language Model LSTM \n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "### Follow the steps to use this notebook for your A4.\n",
        "\n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of all 3 txt files from **lm-data** directory (available in a4.zip) to your Google Drive in the folder location **A4/lm-data/**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'lm-data'\n",
        "##### 3. You are all set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkySNiz1WH6"
      },
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j0qWyxlwG5h0"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JZ5dIx40GKR",
        "outputId": "6f22fd2c-b289-4f81-d9bb-1354aa9eba4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers\n",
        "import os, random\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from keras.activations import softmax\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from transformers import BertTokenizer, TFBertLMHeadModel, BertConfig, TFBertModel\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BDPzX4aj1aXm"
      },
      "outputs": [],
      "source": [
        "train_file = '/content/drive/My Drive/A4/lm-data/little-prince-train.txt'\n",
        "dev_file = '/content/drive/My Drive/A4/lm-data/little-prince-dev.txt'\n",
        "test_file = '/content/drive/My Drive/A4/lm-data/little-prince-test.txt'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkKlYFrn5db8"
      },
      "source": [
        "###Change these arguments as needed for your experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nnk3SXot5dlE"
      },
      "outputs": [],
      "source": [
        "epochs = 3 # number of epochs\n",
        "learning_rate = 0.1 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 10 # hidden layer size\n",
        "batch_size = 50 # batch size\n",
        "use_bert = True # to use the BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-aYI_pV3yYA"
      },
      "source": [
        "### Implement this function if you want to transform the input text, e.g. normalizing case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GGGvg8_K3ULB"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVfmWmC-51d8"
      },
      "source": [
        "### Implement this function to generate the next-word labels for a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g6PFtYXx51n9"
      },
      "outputs": [],
      "source": [
        "def shift_by_one(seq):\n",
        "    '''\n",
        "    input: ['<s>', 'The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>']\n",
        "    output: ['The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>', '[PAD]']\n",
        "    '''\n",
        "    output = []\n",
        "    for i in range(1,len(seq)):\n",
        "      output.append(seq[i])\n",
        "      \n",
        "    output.append(PAD)\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KtEOOC6b9--l"
      },
      "outputs": [],
      "source": [
        "# print(shift_by_one(['The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>','[PAD]']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOiLZLYQ9gjc"
      },
      "source": [
        "### Download the GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gXiKSjGw9g9D"
      },
      "outputs": [],
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip -o glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "X3R-Okr0lknB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S0bt31QJltHe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NadIw1Ve51wm"
      },
      "source": [
        "### Implement this function to load the pre-trained GloVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PHVU5gz1514x"
      },
      "outputs": [],
      "source": [
        "glove_file = 'glove.6B.100d.txt' # Change as necessary\n",
        "\n",
        "def load_pretrained_embeddings(glove_file, vocab):\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    embeddings_index = {}\n",
        "    word_index = dict(zip(vocab, range(len(vocab))))\n",
        "    with open(glove_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            # Each line will be a word and a list of floats, separated by spaces.\n",
        "            # If the word is in your vocabulary, create a numpy array from the list of floats.\n",
        "            # Assign the array to the correct row of embedding_matrix.\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "    num_tokens = len(vocab)\n",
        "\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "            \n",
        "    embedding_matrix[vocab[UNK]] = np.random.randn(embedding_size)\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82oUhFQb4__1"
      },
      "source": [
        "###Helper Functions (no need to implement)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iZwD6Xkf30aM"
      },
      "outputs": [],
      "source": [
        "def get_vocabulary_and_data_with_bert_tokenization(data_file):\n",
        "    data = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            sent = [START]\n",
        "            sent.extend(tokenizer.tokenize(line))\n",
        "            sent.append(END)\n",
        "            data.append(sent)\n",
        "    vocab = {k:v for k,v in tokenizer.vocab.items()}\n",
        "    vocab['<s>'] = 101 # alias for [CLS]\n",
        "    vocab['</s>'] = 102 # alias for [SEP]\n",
        "    return vocab, data\n",
        "\n",
        "\n",
        "def get_vocabulary_and_data(data_file, max_vocab_size=None, use_bert=False):\n",
        "    if use_bert:\n",
        "        return get_vocabulary_and_data_with_bert_tokenization(data_file)\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            sent = [START]\n",
        "            tokens = transform_text_sequence(line.split())\n",
        "            for tok in tokens:\n",
        "                sent.append(tok)\n",
        "                vocab[tok]+=1\n",
        "            sent.append(END)\n",
        "            data.append(sent)\n",
        "            vocab[START]+=1\n",
        "            vocab[END]+=1\n",
        "    vocab = [w for w in sorted(vocab, key=lambda x:vocab[x], reverse=True)]\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, data\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, vocab):\n",
        "    vec = [1.0 if l==label else 0.0 for l in vocab]\n",
        "    return vec\n",
        "\n",
        "\n",
        "def batch_generator_lm(data, vocab, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent in data:\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(token, vocab) for token in shift_by_one(sent)])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, vocab))\n",
        "                batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
        "                yield batch_x, batch_y.astype('float32')\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[49])\n",
        "    print('Data size',len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x\n",
        "\n",
        "\n",
        "def generate_text(language_model, vocab):\n",
        "    prediction = [START]\n",
        "    while not (prediction[-1] == END or len(prediction)>=50):\n",
        "        next_token_one_hot = language_model.predict(np.array([[vocab[p] for p in prediction]]), batch_size=1)[0][-1]\n",
        "        threshold = random.random()\n",
        "        sum = 0\n",
        "        next_token = 0\n",
        "        for i,p in enumerate(next_token_one_hot):\n",
        "            sum += p\n",
        "            if sum>threshold:\n",
        "                next_token = i\n",
        "                break\n",
        "        for w, i in vocab.items():\n",
        "            if i==next_token:\n",
        "                prediction.append(w)\n",
        "                break\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "    # https://stackoverflow.com/questions/41881308/how-to-calculate-perplexity-of-rnn-in-tensorflow\n",
        "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
        "    perp = K.exp(cross_entropy)\n",
        "    return perp\n",
        "\n",
        "\n",
        "class BERT_Wrapper(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BERT_Wrapper, self).__init__()\n",
        "    self.encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", trainable=False)\n",
        "    self.dense = Dense(hidden_size)\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "      outputs = self.encoder(inputs)\n",
        "      last_hidden_states = outputs[0] # The last hidden-state is the first element of the output tuple\n",
        "      output = self.dense(last_hidden_states)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KfWOhCQYmkXF"
      },
      "outputs": [],
      "source": [
        "# BERT_Wrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsxavcjCApA"
      },
      "source": [
        "###Check the GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWhEuSeB5TPy",
        "outputId": "9de8fe5d-b3a2-4226-da30-d1e667d0696c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1gia3C2Onp5I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wxm5waVCMPc"
      },
      "source": [
        "###Main procedure call: Implement the keras model here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXomgwnACH3P",
        "outputId": "2bfca41c-8dfc-41cf-fd40-05344bc87624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'next', ',', 'the', 'lamp', '##light', '##ers', 'of', 'china', 'and', 'siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 33)\n",
            "Batch output shape: (50, 33, 30524)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 3\n",
            "24/27 [=========================>....] - ETA: 21s - loss: 6.9550 - accuracy: 0.4782 - perplexity: 2617366.7500"
          ]
        }
      ],
      "source": [
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}