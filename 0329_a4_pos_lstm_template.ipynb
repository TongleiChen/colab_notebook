{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/0329_a4_pos_lstm_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - POS LSTM Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of all 3 tsv files in **pos-data** directory (available in a4.zip) to your Google Drive in the folder location **A4/pos-data/**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'pos-data'\n",
        "##### 3. You are all set!"
      ],
      "metadata": {
        "id": "naWvCo-lnKX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tCImn0GnNC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed,Input,Dropout\n",
        "from keras.activations import softmax\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "-BoD2K5jnNeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024c5b31-4910-4f4f-d1e1-47be5eba31db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/My Drive/A4/pos-data/en-ud-train.upos.tsv'\n",
        "dev_file = '/content/drive/My Drive/A4/pos-data/en-ud-dev.upos.tsv'\n",
        "test_file = '/content/drive/My Drive/A4/pos-data/en-ud-test.upos.tsv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n"
      ],
      "metadata": {
        "id": "QzNDuv4kqG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement this function if you want to transform the input text, e.g. normalizing case\n"
      ],
      "metadata": {
        "id": "A1nXpu8Is33Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "ckEzn6vCs2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)\n",
        "\n"
      ],
      "metadata": {
        "id": "KKRtY7VwsVJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    pos_vocab = {'<s>','</s>'}\n",
        "    vocab[UNK] = 1\n",
        "    vocab[PAD] = 1\n",
        "    data = []\n",
        "    gold_labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        sent = ['<s>']\n",
        "        sent_pos = ['<s>']\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                tok, pos = line.strip().split('\\t')[0], line.strip().split('\\t')[1]\n",
        "                sent.append(tok)\n",
        "                sent_pos.append(pos)\n",
        "                vocab[tok]+=1\n",
        "                vocab['<s>'] += 1\n",
        "                vocab['</s>'] += 1\n",
        "                pos_vocab.add(pos)\n",
        "            elif sent:\n",
        "                sent.append('</s>')\n",
        "                sent_pos.append('</s>')\n",
        "                sent = transform_text_sequence(sent)\n",
        "                data.append(sent)\n",
        "                gold_labels.append(sent_pos)\n",
        "                sent = ['<s>']\n",
        "                sent_pos = ['<s>']\n",
        "    vocab = sorted(vocab.keys(), key = lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "    return {k:v for v,k in enumerate(vocab)}, list(pos_vocab), data, gold_labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "def clean(seqs, vocab, unk):\n",
        "    for i,seq in enumerate(seqs):\n",
        "        for j,tok in enumerate(seq):\n",
        "            if tok>=len(vocab):\n",
        "                seq[j] = unk\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent, sent_pos in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(label, label_set) for label in sent_pos])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                clean(batch_x, vocab, vocab[UNK])\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, label_set))\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "EozeUctXrxin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "E6y3DqEJtKgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20 # number of epochs\n",
        "learning_rate = 10 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 10 # hidden layer size\n",
        "batch_size = 40 # batch size"
      ],
      "metadata": {
        "id": "xgih5BIas_UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "w2LUBOl_tMzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "nHsrmiMItM8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d868f1-f99c-48cf-c10d-5248ab68cfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(train_file)\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(dev_file)\n",
        "describe_data(train_data, train_labels, labels,\n",
        "              batch_generator(train_data, train_labels, vocab, labels, batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQV3hSLFbW6G",
        "outputId": "1a8ca291-4c9b-4eb9-ab6a-fd5a1df31266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.', '</s>']\n",
            "Label: ['<s>', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', '</s>']\n",
            "Label count: 19\n",
            "Data size 12543\n",
            "Batch input shape: (40, 53)\n",
            "Batch output shape: (40, 53, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ahKeOLW7ny_",
        "outputId": "5e51df94-32c8-4a93-9a63-a8c4b5419ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19676"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n"
      ],
      "metadata": {
        "id": "ogLr4rVev-5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(train_file)\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(dev_file)\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "              batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    sgd = optimizers.SGD(learning_rate=learning_rate)\n",
        "    pos_tagger.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "id": "MR7RBP6atSUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acaf3c2-a949-4f00-e082-00d5a87ff14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.', '</s>']\n",
            "Label: ['<s>', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', '</s>']\n",
            "Label count: 19\n",
            "Data size 12543\n",
            "Batch input shape: (40, 53)\n",
            "Batch output shape: (40, 53, 19)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, None, 20)         8880      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, None, 19)         399       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,976,879\n",
            "Trainable params: 1,976,879\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 20\n",
            "313/313 [==============================] - 7s 10ms/step - loss: 1.7555 - accuracy: 0.1790\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.5706 - accuracy: 0.8207\n",
            "Dev Loss: 0.5705733299255371 Dev Acc: 0.8206763863563538\n",
            "Epoch 2 / 20\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.5901 - accuracy: 0.2350\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4553 - accuracy: 0.8562\n",
            "Dev Loss: 0.45533090829849243 Dev Acc: 0.8562118411064148\n",
            "Epoch 3 / 20\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 1.5412 - accuracy: 0.2475\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3674 - accuracy: 0.8950\n",
            "Dev Loss: 0.36736300587654114 Dev Acc: 0.8950058221817017\n",
            "Epoch 4 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5310 - accuracy: 0.2521\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3731 - accuracy: 0.8974\n",
            "Dev Loss: 0.3730902671813965 Dev Acc: 0.8974411487579346\n",
            "Epoch 5 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5182 - accuracy: 0.2543\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3461 - accuracy: 0.9042\n",
            "Dev Loss: 0.3460513651371002 Dev Acc: 0.9042326807975769\n",
            "Epoch 6 / 20\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.5127 - accuracy: 0.2560\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3367 - accuracy: 0.9117\n",
            "Dev Loss: 0.33671507239341736 Dev Acc: 0.9117445349693298\n",
            "Epoch 7 / 20\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.5143 - accuracy: 0.2564\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3323 - accuracy: 0.9116\n",
            "Dev Loss: 0.3322646915912628 Dev Acc: 0.911573052406311\n",
            "Epoch 8 / 20\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.5098 - accuracy: 0.2570\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3381 - accuracy: 0.9128\n",
            "Dev Loss: 0.3380751609802246 Dev Acc: 0.912807822227478\n",
            "Epoch 9 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5151 - accuracy: 0.2572\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3419 - accuracy: 0.9134\n",
            "Dev Loss: 0.3418513834476471 Dev Acc: 0.9133566617965698\n",
            "Epoch 10 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5181 - accuracy: 0.2572\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3458 - accuracy: 0.9160\n",
            "Dev Loss: 0.3458097577095032 Dev Acc: 0.9159635305404663\n",
            "Epoch 11 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5131 - accuracy: 0.2576\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3564 - accuracy: 0.9132\n",
            "Dev Loss: 0.3563529849052429 Dev Acc: 0.913185179233551\n",
            "Epoch 12 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5061 - accuracy: 0.2583\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3492 - accuracy: 0.9164\n",
            "Dev Loss: 0.3491969704627991 Dev Acc: 0.9163751006126404\n",
            "Epoch 13 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5076 - accuracy: 0.2584\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3457 - accuracy: 0.9178\n",
            "Dev Loss: 0.3457365334033966 Dev Acc: 0.917781412601471\n",
            "Epoch 14 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5085 - accuracy: 0.2586\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3536 - accuracy: 0.9183\n",
            "Dev Loss: 0.3535851836204529 Dev Acc: 0.9183302521705627\n",
            "Epoch 15 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.4989 - accuracy: 0.2593\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3469 - accuracy: 0.9185\n",
            "Dev Loss: 0.3469255864620209 Dev Acc: 0.9185017347335815\n",
            "Epoch 16 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5127 - accuracy: 0.2586\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3564 - accuracy: 0.9185\n",
            "Dev Loss: 0.3563694357872009 Dev Acc: 0.9184674620628357\n",
            "Epoch 17 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5035 - accuracy: 0.2593\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3522 - accuracy: 0.9196\n",
            "Dev Loss: 0.3521673381328583 Dev Acc: 0.9195993542671204\n",
            "Epoch 18 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5144 - accuracy: 0.2589\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3424 - accuracy: 0.9195\n",
            "Dev Loss: 0.3423873484134674 Dev Acc: 0.919496476650238\n",
            "Epoch 19 / 20\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 1.5077 - accuracy: 0.2591\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3628 - accuracy: 0.9193\n",
            "Dev Loss: 0.3628048300743103 Dev Acc: 0.9192906618118286\n",
            "Epoch 20 / 20\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5101 - accuracy: 0.2592\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3676 - accuracy: 0.9205\n",
            "Dev Loss: 0.36760106682777405 Dev Acc: 0.9204911589622498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH43AYykWdfK",
        "outputId": "256ca4db-c696-49bb-9c85-4e0f162d5d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOUN',\n",
              " 'VERB',\n",
              " 'CCONJ',\n",
              " 'SYM',\n",
              " 'INTJ',\n",
              " '</s>',\n",
              " 'ADV',\n",
              " 'DET',\n",
              " 'PUNCT',\n",
              " 'ADP',\n",
              " 'AUX',\n",
              " 'ADJ',\n",
              " 'X',\n",
              " 'PROPN',\n",
              " '<s>',\n",
              " 'PART',\n",
              " 'SCONJ',\n",
              " 'NUM',\n",
              " 'PRON']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}