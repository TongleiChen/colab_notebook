{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yZqRoIHw-PK5A0yo84Mg3xmJAb7vhzzt",
      "authorship_tag": "ABX9TyOgMkMZYwLipRO+USqgo4Yz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/COSC576_project_early_Xmas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4epz13JihL1",
        "outputId": "b2856bfb-98f6-452b-90ab-136fa345f301"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CW3VP4R2Kxl_"
      },
      "outputs": [],
      "source": [
        "# @title import library\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre-trained model reference: https://debuggercafe.com/transfer-learning-with-pytorch/"
      ],
      "metadata": {
        "id": "Go_BJ3sljNJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMAGE CLASSIFICATION\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, animal_category, image_size = 255, class_size = 100,transform = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sketch_dir (string): Directory to all the sketch images.\n",
        "            realworld_dir (string): Directory to all the real world images.\n",
        "            animal_category: list to fruit catogory\n",
        "            class_size: Num of images in each category\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.animal_category = animal_category\n",
        "        self.class_size = class_size\n",
        "        self.data_dict = dict(np.load(img_dir,allow_pickle=True))\n",
        "        self.image_size = image_size\n",
        "       \n",
        "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
        "                                                transforms.Resize((image_size,image_size)),\n",
        "                                                transforms.ToTensor(),])\n",
        "                                              # transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.class_size * len(self.animal_category)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        class_index = int(idx // self.class_size)\n",
        "        category =  self.animal_category[class_index]\n",
        "        category_idx = int(idx % self.class_size)\n",
        "        label = np.zeros((len(self.animal_category), 1))\n",
        "        label[class_index] = 1\n",
        "        \n",
        "#         label = class_index\n",
        "        image_ary =  self.data_dict[category][category_idx]\n",
        "        sample = {'image': image_ary, 'label': label}\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform_img(sample['image'])\n",
        "            sample['label'] = self.transform_label(sample['label'])\n",
        "        return sample\n",
        "        "
      ],
      "metadata": {
        "id": "idpV7W0Zia3b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load image data\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "train_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "train_realworld = ImageDataset(train_realworld_dir, QURIES, image_size = 64, class_size = 500,transform = True)\n",
        "train_loader = DataLoader(train_realworld, batch_size=128, shuffle=True, pin_memory=True)\n",
        "\n",
        "test_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/test_images.npz\"\n",
        "test_realworld = ImageDataset(test_realworld_dir, QURIES, image_size = 64, class_size = 10,transform = True)\n",
        "test_loader_realworld = DataLoader(test_realworld, batch_size=128, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "h46CpgM0jeb8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = models.vgg16(pretrained=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model.to(device)\n",
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "divQilguj1IS",
        "outputId": "ba3851c6-4036-4e58-9eac-b56800d2449c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model.classifier[6].out_features = 10\n",
        "# freeze convolution weights\n",
        "for param in image_model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "cbkHRz7ZkPNq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "num_classes = 10\n",
        "num_epochs = 30\n",
        "batch_size = 64\n",
        "learning_rate = 0.01 #0.03\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(image_model.classifier.parameters(), lr=learning_rate, weight_decay = 0.007, momentum = 0.9)  \n",
        "LOAD = False\n",
        "start_epoch =0\n",
        "train_realworld_acc = []\n",
        "train_realworld_loss = []\n",
        "val_realworld_acc = []\n",
        "val_realworld_loss = []\n",
        "import gc\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(start_epoch,start_epoch+num_epochs):\n",
        "    for data in train_loader:  \n",
        "        # Move tensors to the configured device\n",
        "        images = data['image'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        correct_t = 0\n",
        "        total_t = 0\n",
        "        # Forward pass\n",
        "        image_model.train()\n",
        "        outputs = image_model(images)\n",
        "        loss = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "        _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "        total_t += labels.size(0)\n",
        "        correct_t +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        train_realworld_loss.append(loss.item())\n",
        "    train_realworld_acc.append(correct_t/total_t)\n",
        "    print ('Epoch [{}/{}], Training Loss: {:.4f}' \n",
        "                   .format(epoch+1,start_epoch+num_epochs, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_model.eval()\n",
        "        correct_v = 0\n",
        "        total_v = 0\n",
        "        for data in test_loader_realworld:\n",
        "            images = data['image'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "        \n",
        "\n",
        "            outputs = image_model(images)\n",
        "            loss_v = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "   \n",
        "            _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "            total_v += labels.size(0)\n",
        "            correct_v +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "            \n",
        "            del images, labels, outputs\n",
        "        val_realworld_loss.append(loss_v.item())\n",
        "        val_realworld_acc.append(correct_v/total_v)\n",
        "        print(f\"Validation accuracy: {(correct_v/total_v):.3f}\")\n",
        "       \n",
        "print(f\"train_acc:{(np.mean([v for v in train_realworld_acc])):.3f},val_acc:{(np.mean([v for v in val_realworld_acc])):.3f}\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jwo605SkgiI",
        "outputId": "cc83f9f3-c306-4c0b-d4f6-ff1e6d09a50d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Training Loss: 0.7527\n",
            "Validation accuracy: 0.580\n",
            "Epoch [2/30], Training Loss: 0.8536\n",
            "Validation accuracy: 0.630\n",
            "Epoch [3/30], Training Loss: 0.9809\n",
            "Validation accuracy: 0.680\n",
            "Epoch [4/30], Training Loss: 1.2078\n",
            "Validation accuracy: 0.630\n",
            "Epoch [5/30], Training Loss: 0.4609\n",
            "Validation accuracy: 0.620\n",
            "Epoch [6/30], Training Loss: 0.7744\n",
            "Validation accuracy: 0.680\n",
            "Epoch [7/30], Training Loss: 0.6792\n",
            "Validation accuracy: 0.630\n",
            "Epoch [8/30], Training Loss: 1.7265\n",
            "Validation accuracy: 0.700\n",
            "Epoch [9/30], Training Loss: 0.1999\n",
            "Validation accuracy: 0.700\n",
            "Epoch [10/30], Training Loss: 0.1726\n",
            "Validation accuracy: 0.640\n",
            "Epoch [11/30], Training Loss: 0.2447\n",
            "Validation accuracy: 0.690\n",
            "Epoch [12/30], Training Loss: 0.6001\n",
            "Validation accuracy: 0.680\n",
            "Epoch [13/30], Training Loss: 0.3548\n",
            "Validation accuracy: 0.660\n",
            "Epoch [14/30], Training Loss: 0.2136\n",
            "Validation accuracy: 0.690\n",
            "Epoch [15/30], Training Loss: 0.5625\n",
            "Validation accuracy: 0.690\n",
            "Epoch [16/30], Training Loss: 0.2110\n",
            "Validation accuracy: 0.680\n",
            "Epoch [17/30], Training Loss: 0.1855\n",
            "Validation accuracy: 0.670\n",
            "Epoch [18/30], Training Loss: 0.2786\n",
            "Validation accuracy: 0.670\n",
            "Epoch [19/30], Training Loss: 0.0618\n",
            "Validation accuracy: 0.680\n",
            "Epoch [20/30], Training Loss: 0.0433\n",
            "Validation accuracy: 0.720\n",
            "Epoch [21/30], Training Loss: 0.0449\n",
            "Validation accuracy: 0.690\n",
            "Epoch [22/30], Training Loss: 0.1241\n",
            "Validation accuracy: 0.700\n",
            "Epoch [23/30], Training Loss: 0.3089\n",
            "Validation accuracy: 0.710\n",
            "Epoch [24/30], Training Loss: 0.0947\n",
            "Validation accuracy: 0.720\n",
            "Epoch [25/30], Training Loss: 0.0916\n",
            "Validation accuracy: 0.660\n",
            "Epoch [26/30], Training Loss: 0.1627\n",
            "Validation accuracy: 0.670\n",
            "Epoch [27/30], Training Loss: 0.1806\n",
            "Validation accuracy: 0.660\n",
            "Epoch [28/30], Training Loss: 0.2636\n",
            "Validation accuracy: 0.670\n",
            "Epoch [29/30], Training Loss: 0.1474\n",
            "Validation accuracy: 0.660\n",
            "Epoch [30/30], Training Loss: 0.0062\n",
            "Validation accuracy: 0.720\n",
            "train_acc:0.863,val_acc:0.673\n"
          ]
        }
      ]
    }
  ]
}