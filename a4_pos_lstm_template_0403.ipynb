{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_pos_lstm_template_0403.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - POS LSTM Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of all 3 tsv files in **pos-data** directory (available in a4.zip) to your Google Drive in the folder location **A4/pos-data/**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'pos-data'\n",
        "##### 3. You are all set!"
      ],
      "metadata": {
        "id": "naWvCo-lnKX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tCImn0GnNC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed,Input,Dropout\n",
        "from keras.activations import softmax\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "-BoD2K5jnNeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff0538b-f158-4f5b-9a8e-298385625469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/My Drive/A4/pos-data/en-ud-train.upos.tsv'\n",
        "dev_file = '/content/drive/My Drive/A4/pos-data/en-ud-dev.upos.tsv'\n",
        "test_file = '/content/drive/My Drive/A4/pos-data/en-ud-test.upos.tsv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n"
      ],
      "metadata": {
        "id": "QzNDuv4kqG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement this function if you want to transform the input text, e.g. normalizing case\n"
      ],
      "metadata": {
        "id": "A1nXpu8Is33Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "ckEzn6vCs2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)\n",
        "\n"
      ],
      "metadata": {
        "id": "KKRtY7VwsVJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    pos_vocab = {'<s>','</s>'}\n",
        "    vocab[UNK] = 1\n",
        "    vocab[PAD] = 1\n",
        "    data = []\n",
        "    gold_labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        sent = ['<s>']\n",
        "        sent_pos = ['<s>']\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                tok, pos = line.strip().split('\\t')[0], line.strip().split('\\t')[1]\n",
        "                sent.append(tok)\n",
        "                sent_pos.append(pos)\n",
        "                vocab[tok]+=1\n",
        "                vocab['<s>'] += 1\n",
        "                vocab['</s>'] += 1\n",
        "                pos_vocab.add(pos)\n",
        "            elif sent:\n",
        "                sent.append('</s>')\n",
        "                sent_pos.append('</s>')\n",
        "                sent = transform_text_sequence(sent)\n",
        "                data.append(sent)\n",
        "                gold_labels.append(sent_pos)\n",
        "                sent = ['<s>']\n",
        "                sent_pos = ['<s>']\n",
        "    vocab = sorted(vocab.keys(), key = lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "    return {k:v for v,k in enumerate(vocab)}, list(pos_vocab), data, gold_labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "def clean(seqs, vocab, unk):\n",
        "    for i,seq in enumerate(seqs):\n",
        "        for j,tok in enumerate(seq):\n",
        "            if tok>=len(vocab):\n",
        "                seq[j] = unk\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent, sent_pos in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(label, label_set) for label in sent_pos])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                clean(batch_x, vocab, vocab[UNK])\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, label_set))\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "EozeUctXrxin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "E6y3DqEJtKgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 10 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "xgih5BIas_UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "w2LUBOl_tMzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "nHsrmiMItM8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30aad8d-c1dd-4747-810f-ba9e9ac77fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n"
      ],
      "metadata": {
        "id": "ogLr4rVev-5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(train_file)\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(dev_file)\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "              batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "id": "MR7RBP6atSUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce391e3-9b22-4b72-d51e-f953529b9153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.', '</s>']\n",
            "Label: ['<s>', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', '</s>']\n",
            "Label count: 19\n",
            "Data size 12543\n",
            "Batch input shape: (50, 62)\n",
            "Batch output shape: (50, 62, 19)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 20)         8880      \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 19)         399       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,976,879\n",
            "Trainable params: 1,976,879\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 61s 202ms/step - loss: 1.5758 - accuracy: 0.2022\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3728 - accuracy: 0.8974\n",
            "Dev Loss: 0.37284204363822937 Dev Acc: 0.8974411487579346\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4521 - accuracy: 0.2396\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3441 - accuracy: 0.9043\n",
            "Dev Loss: 0.34408310055732727 Dev Acc: 0.904335618019104\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4482 - accuracy: 0.2432\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3307 - accuracy: 0.9107\n",
            "Dev Loss: 0.33074426651000977 Dev Acc: 0.9107155203819275\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4380 - accuracy: 0.2447\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3367 - accuracy: 0.9122\n",
            "Dev Loss: 0.33666011691093445 Dev Acc: 0.9121561646461487\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4399 - accuracy: 0.2454\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3405 - accuracy: 0.9140\n",
            "Dev Loss: 0.3405300974845886 Dev Acc: 0.914008378982544\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4368 - accuracy: 0.2458\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3538 - accuracy: 0.9152\n",
            "Dev Loss: 0.3538013994693756 Dev Acc: 0.9152088761329651\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4284 - accuracy: 0.2467\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3621 - accuracy: 0.9142\n",
            "Dev Loss: 0.36213359236717224 Dev Acc: 0.9142484664916992\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4376 - accuracy: 0.2462\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3760 - accuracy: 0.9144\n",
            "Dev Loss: 0.3759697675704956 Dev Acc: 0.9143514037132263\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4321 - accuracy: 0.2468\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3988 - accuracy: 0.9130\n",
            "Dev Loss: 0.3987810015678406 Dev Acc: 0.9130136370658875\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4267 - accuracy: 0.2470\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4009 - accuracy: 0.9121\n",
            "Dev Loss: 0.4009334444999695 Dev Acc: 0.9120875597000122\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4284 - accuracy: 0.2471\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4023 - accuracy: 0.9141\n",
            "Dev Loss: 0.40231478214263916 Dev Acc: 0.9141455888748169\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4410 - accuracy: 0.2466\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4080 - accuracy: 0.9136\n",
            "Dev Loss: 0.4080263078212738 Dev Acc: 0.9135967493057251\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4251 - accuracy: 0.2473\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4175 - accuracy: 0.9133\n",
            "Dev Loss: 0.4174872636795044 Dev Acc: 0.9132880568504333\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4278 - accuracy: 0.2473\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4147 - accuracy: 0.9127\n",
            "Dev Loss: 0.4147450625896454 Dev Acc: 0.9127392172813416\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4248 - accuracy: 0.2476\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4302 - accuracy: 0.9125\n",
            "Dev Loss: 0.43015092611312866 Dev Acc: 0.9125334620475769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 20 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f855f731-97af-4861-bf5d-92a36d03409a",
        "id": "zXYsu3QaDQXJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, None, 40)         19360     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, None, 19)         779       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,987,739\n",
            "Trainable params: 1,987,739\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 38s 137ms/step - loss: 1.5632 - accuracy: 0.2097\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3219 - accuracy: 0.9058\n",
            "Dev Loss: 0.3218507468700409 Dev Acc: 0.9057762026786804\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4502 - accuracy: 0.2420\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2994 - accuracy: 0.9146\n",
            "Dev Loss: 0.29935628175735474 Dev Acc: 0.9145914912223816\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4480 - accuracy: 0.2451\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2960 - accuracy: 0.9195\n",
            "Dev Loss: 0.29595497250556946 Dev Acc: 0.9195307493209839\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4374 - accuracy: 0.2464\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3058 - accuracy: 0.9185\n",
            "Dev Loss: 0.3058161437511444 Dev Acc: 0.9185017347335815\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4247 - accuracy: 0.2518\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3233 - accuracy: 0.9195\n",
            "Dev Loss: 0.3233011066913605 Dev Acc: 0.9194621443748474\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4372 - accuracy: 0.2545\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3385 - accuracy: 0.9196\n",
            "Dev Loss: 0.3384605646133423 Dev Acc: 0.919633686542511\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4370 - accuracy: 0.2472\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3476 - accuracy: 0.9185\n",
            "Dev Loss: 0.34760841727256775 Dev Acc: 0.9184674620628357\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4309 - accuracy: 0.2483\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3741 - accuracy: 0.9189\n",
            "Dev Loss: 0.3740808963775635 Dev Acc: 0.9188790321350098\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4389 - accuracy: 0.2480\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3621 - accuracy: 0.9184\n",
            "Dev Loss: 0.3621068596839905 Dev Acc: 0.9183645248413086\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4339 - accuracy: 0.2476\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3590 - accuracy: 0.9209\n",
            "Dev Loss: 0.35898762941360474 Dev Acc: 0.9209027886390686\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4270 - accuracy: 0.2480\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3757 - accuracy: 0.9204\n",
            "Dev Loss: 0.37566089630126953 Dev Acc: 0.9204225540161133\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4281 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3865 - accuracy: 0.9186\n",
            "Dev Loss: 0.3864808976650238 Dev Acc: 0.9186046719551086\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4251 - accuracy: 0.2509\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3845 - accuracy: 0.9209\n",
            "Dev Loss: 0.3845173120498657 Dev Acc: 0.9209027886390686\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4336 - accuracy: 0.2483\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3817 - accuracy: 0.9225\n",
            "Dev Loss: 0.38167303800582886 Dev Acc: 0.9225149154663086\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4242 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4008 - accuracy: 0.9201\n",
            "Dev Loss: 0.4008007049560547 Dev Acc: 0.9200795888900757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 30 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999060cf-38e0-4e10-ef12-99665b8063af",
        "id": "GpOyNLi_ttiF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, None, 60)         31440     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, None, 19)         1159      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,000,199\n",
            "Trainable params: 2,000,199\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 38s 136ms/step - loss: 1.5442 - accuracy: 0.2430\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3190 - accuracy: 0.9080\n",
            "Dev Loss: 0.31902357935905457 Dev Acc: 0.9080400466918945\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 1.4433 - accuracy: 0.2885\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3100 - accuracy: 0.9158\n",
            "Dev Loss: 0.31003713607788086 Dev Acc: 0.9158263206481934\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4400 - accuracy: 0.2578\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3167 - accuracy: 0.9195\n",
            "Dev Loss: 0.31673017144203186 Dev Acc: 0.919496476650238\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4287 - accuracy: 0.2480\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3359 - accuracy: 0.9179\n",
            "Dev Loss: 0.3359452188014984 Dev Acc: 0.9179186224937439\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4307 - accuracy: 0.2475\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3549 - accuracy: 0.9203\n",
            "Dev Loss: 0.3549175560474396 Dev Acc: 0.9202854037284851\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4296 - accuracy: 0.2479\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3660 - accuracy: 0.9210\n",
            "Dev Loss: 0.36599358916282654 Dev Acc: 0.9210399985313416\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4273 - accuracy: 0.2481\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3661 - accuracy: 0.9203\n",
            "Dev Loss: 0.3661470413208008 Dev Acc: 0.9202510714530945\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4242 - accuracy: 0.2519\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4020 - accuracy: 0.9186\n",
            "Dev Loss: 0.40198779106140137 Dev Acc: 0.9186389446258545\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4272 - accuracy: 0.2496\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3852 - accuracy: 0.9211\n",
            "Dev Loss: 0.38516971468925476 Dev Acc: 0.921108603477478\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4288 - accuracy: 0.2563\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4200 - accuracy: 0.9196\n",
            "Dev Loss: 0.42004507780075073 Dev Acc: 0.9195993542671204\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 1.4292 - accuracy: 0.2678\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4307 - accuracy: 0.9170\n",
            "Dev Loss: 0.4307342767715454 Dev Acc: 0.9170268177986145\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4263 - accuracy: 0.2724\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4447 - accuracy: 0.9187\n",
            "Dev Loss: 0.44473785161972046 Dev Acc: 0.918707549571991\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4161 - accuracy: 0.3161\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4607 - accuracy: 0.9194\n",
            "Dev Loss: 0.46073848009109497 Dev Acc: 0.9193935394287109\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4347 - accuracy: 0.2730\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4382 - accuracy: 0.9194\n",
            "Dev Loss: 0.4381820857524872 Dev Acc: 0.9193935394287109\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4218 - accuracy: 0.3000\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4476 - accuracy: 0.9176\n",
            "Dev Loss: 0.4475759267807007 Dev Acc: 0.9176099300384521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 40 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6a5a6d-8fae-49e8-fe6c-1fb40b073e5c",
        "id": "BaU-wN-EtwNP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, None, 80)         45120     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, None, 19)         1539      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,014,259\n",
            "Trainable params: 2,014,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 38s 131ms/step - loss: 1.5335 - accuracy: 0.2124\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3165 - accuracy: 0.9052\n",
            "Dev Loss: 0.31649038195610046 Dev Acc: 0.9052274227142334\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4436 - accuracy: 0.2426\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2974 - accuracy: 0.9158\n",
            "Dev Loss: 0.2974037826061249 Dev Acc: 0.9157577157020569\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4335 - accuracy: 0.2476\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2937 - accuracy: 0.9189\n",
            "Dev Loss: 0.29373225569725037 Dev Acc: 0.9189133644104004\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4354 - accuracy: 0.2517\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3224 - accuracy: 0.9177\n",
            "Dev Loss: 0.3224211633205414 Dev Acc: 0.9176785349845886\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4315 - accuracy: 0.2535\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3122 - accuracy: 0.9223\n",
            "Dev Loss: 0.3122374415397644 Dev Acc: 0.9223091006278992\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4317 - accuracy: 0.2693\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3290 - accuracy: 0.9204\n",
            "Dev Loss: 0.3290424942970276 Dev Acc: 0.9203882813453674\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4343 - accuracy: 0.2679\n",
            "2002/2002 [==============================] - 9s 5ms/step - loss: 0.3617 - accuracy: 0.9202\n",
            "Dev Loss: 0.36172500252723694 Dev Acc: 0.920182466506958\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4248 - accuracy: 0.2806\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3535 - accuracy: 0.9212\n",
            "Dev Loss: 0.353481650352478 Dev Acc: 0.9212114810943604\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4180 - accuracy: 0.3023\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3417 - accuracy: 0.9236\n",
            "Dev Loss: 0.34167298674583435 Dev Acc: 0.923646867275238\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4276 - accuracy: 0.2736\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3727 - accuracy: 0.9222\n",
            "Dev Loss: 0.3727322518825531 Dev Acc: 0.9221718907356262\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4348 - accuracy: 0.2720\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3700 - accuracy: 0.9232\n",
            "Dev Loss: 0.3700093924999237 Dev Acc: 0.9231666326522827\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4279 - accuracy: 0.2610\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3806 - accuracy: 0.9199\n",
            "Dev Loss: 0.3806286156177521 Dev Acc: 0.9198737740516663\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4239 - accuracy: 0.2676\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3638 - accuracy: 0.9237\n",
            "Dev Loss: 0.3637770712375641 Dev Acc: 0.9237497448921204\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4229 - accuracy: 0.2577\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3788 - accuracy: 0.9236\n",
            "Dev Loss: 0.37875357270240784 Dev Acc: 0.9235782623291016\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4307 - accuracy: 0.2566\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3802 - accuracy: 0.9229\n",
            "Dev Loss: 0.3802127242088318 Dev Acc: 0.922857940196991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 50 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d413d8d-4ef5-47d8-c60b-4c133479a3c2",
        "id": "by1fPzBltyHq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, None, 100)        60400     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, None, 19)         1919      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,029,919\n",
            "Trainable params: 2,029,919\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 39s 139ms/step - loss: 1.5497 - accuracy: 0.2126\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.2982 - accuracy: 0.9112\n",
            "Dev Loss: 0.29820725321769714 Dev Acc: 0.9112300276756287\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4335 - accuracy: 0.2461\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2882 - accuracy: 0.9183\n",
            "Dev Loss: 0.2882048189640045 Dev Acc: 0.9182959198951721\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4374 - accuracy: 0.2470\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2889 - accuracy: 0.9209\n",
            "Dev Loss: 0.28887248039245605 Dev Acc: 0.9209027886390686\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4278 - accuracy: 0.2523\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2919 - accuracy: 0.9241\n",
            "Dev Loss: 0.2918878495693207 Dev Acc: 0.9240927696228027\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4233 - accuracy: 0.2558\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3080 - accuracy: 0.9257\n",
            "Dev Loss: 0.30802732706069946 Dev Acc: 0.9256705641746521\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 1.4306 - accuracy: 0.2524\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3151 - accuracy: 0.9243\n",
            "Dev Loss: 0.3150813579559326 Dev Acc: 0.9242642521858215\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4237 - accuracy: 0.2493\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3451 - accuracy: 0.9219\n",
            "Dev Loss: 0.3451152741909027 Dev Acc: 0.921931803226471\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4181 - accuracy: 0.2514\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3577 - accuracy: 0.9224\n",
            "Dev Loss: 0.3577215373516083 Dev Acc: 0.9224120378494263\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4255 - accuracy: 0.2512\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3865 - accuracy: 0.9212\n",
            "Dev Loss: 0.3865053355693817 Dev Acc: 0.921245813369751\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 1.4259 - accuracy: 0.2552\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3808 - accuracy: 0.9230\n",
            "Dev Loss: 0.3807852864265442 Dev Acc: 0.9230294227600098\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4256 - accuracy: 0.2509\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.3879 - accuracy: 0.9227\n",
            "Dev Loss: 0.3878689706325531 Dev Acc: 0.9226863980293274\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4324 - accuracy: 0.2543\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3976 - accuracy: 0.9229\n",
            "Dev Loss: 0.3975827991962433 Dev Acc: 0.9228922128677368\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4310 - accuracy: 0.2533\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4168 - accuracy: 0.9235\n",
            "Dev Loss: 0.416770339012146 Dev Acc: 0.9235439300537109\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4274 - accuracy: 0.2664\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4147 - accuracy: 0.9212\n",
            "Dev Loss: 0.41465237736701965 Dev Acc: 0.921245813369751\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4199 - accuracy: 0.2491\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4096 - accuracy: 0.9226\n",
            "Dev Loss: 0.40956470370292664 Dev Acc: 0.9226177930831909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 60 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082cdea4-43b7-4ca7-ba4e-d011e975a21f",
        "id": "d0wQYhkRt0Gg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, None, 120)        77280     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDis  (None, None, 19)         2299      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,047,179\n",
            "Trainable params: 2,047,179\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 38s 129ms/step - loss: 1.5357 - accuracy: 0.2123\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3177 - accuracy: 0.9080\n",
            "Dev Loss: 0.3177160918712616 Dev Acc: 0.9080057740211487\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4528 - accuracy: 0.2424\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2866 - accuracy: 0.9191\n",
            "Dev Loss: 0.2865823209285736 Dev Acc: 0.9190848469734192\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4338 - accuracy: 0.2458\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2966 - accuracy: 0.9199\n",
            "Dev Loss: 0.2965845465660095 Dev Acc: 0.9199080467224121\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4350 - accuracy: 0.2468\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2990 - accuracy: 0.9231\n",
            "Dev Loss: 0.2989758849143982 Dev Acc: 0.9230980277061462\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4155 - accuracy: 0.2485\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3201 - accuracy: 0.9234\n",
            "Dev Loss: 0.3200851082801819 Dev Acc: 0.9234410524368286\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4249 - accuracy: 0.2483\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3124 - accuracy: 0.9267\n",
            "Dev Loss: 0.3123639225959778 Dev Acc: 0.9267339110374451\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4277 - accuracy: 0.2483\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3422 - accuracy: 0.9252\n",
            "Dev Loss: 0.3422349989414215 Dev Acc: 0.9251903891563416\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 1.4286 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3412 - accuracy: 0.9273\n",
            "Dev Loss: 0.3412168622016907 Dev Acc: 0.9273170232772827\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4214 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3492 - accuracy: 0.9252\n",
            "Dev Loss: 0.3491578996181488 Dev Acc: 0.9251903891563416\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4240 - accuracy: 0.2488\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3716 - accuracy: 0.9229\n",
            "Dev Loss: 0.37164926528930664 Dev Acc: 0.9229265451431274\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4198 - accuracy: 0.2489\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3803 - accuracy: 0.9249\n",
            "Dev Loss: 0.3803344666957855 Dev Acc: 0.9249159693717957\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4307 - accuracy: 0.2485\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3891 - accuracy: 0.9254\n",
            "Dev Loss: 0.3891310691833496 Dev Acc: 0.9253618717193604\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4196 - accuracy: 0.2491\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3911 - accuracy: 0.9269\n",
            "Dev Loss: 0.3911362290382385 Dev Acc: 0.9269397258758545\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4230 - accuracy: 0.2489\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4000 - accuracy: 0.9234\n",
            "Dev Loss: 0.39999818801879883 Dev Acc: 0.923406720161438\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4239 - accuracy: 0.2489\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4191 - accuracy: 0.9236\n",
            "Dev Loss: 0.41908854246139526 Dev Acc: 0.9235782623291016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc5b258-52ea-4ccb-fe69-144357b35a65",
        "id": "MdGeUzhTzzz3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, None, 200)        160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, None, 19)         3819      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,132,219\n",
            "Trainable params: 2,132,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 37s 130ms/step - loss: 1.5363 - accuracy: 0.2137\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.2911 - accuracy: 0.9142\n",
            "Dev Loss: 0.2911223769187927 Dev Acc: 0.9142484664916992\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4464 - accuracy: 0.2433\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2799 - accuracy: 0.9217\n",
            "Dev Loss: 0.2798589766025543 Dev Acc: 0.921657383441925\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4361 - accuracy: 0.2463\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2810 - accuracy: 0.9243\n",
            "Dev Loss: 0.2810388505458832 Dev Acc: 0.9242642521858215\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 1.4326 - accuracy: 0.2472\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.2931 - accuracy: 0.9254\n",
            "Dev Loss: 0.29307183623313904 Dev Acc: 0.9253961443901062\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4239 - accuracy: 0.2482\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3052 - accuracy: 0.9244\n",
            "Dev Loss: 0.3051602244377136 Dev Acc: 0.9244014620780945\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4245 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3275 - accuracy: 0.9264\n",
            "Dev Loss: 0.327465683221817 Dev Acc: 0.9263566136360168\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4246 - accuracy: 0.2493\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3430 - accuracy: 0.9218\n",
            "Dev Loss: 0.3430311977863312 Dev Acc: 0.9218289256095886\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4191 - accuracy: 0.2570\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3369 - accuracy: 0.9256\n",
            "Dev Loss: 0.33688002824783325 Dev Acc: 0.9256362915039062\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4278 - accuracy: 0.2504\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3361 - accuracy: 0.9250\n",
            "Dev Loss: 0.3360946476459503 Dev Acc: 0.9249845743179321\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4228 - accuracy: 0.2492\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3494 - accuracy: 0.9251\n",
            "Dev Loss: 0.34939101338386536 Dev Acc: 0.9250531792640686\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4213 - accuracy: 0.2493\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3472 - accuracy: 0.9260\n",
            "Dev Loss: 0.3472464382648468 Dev Acc: 0.9260478615760803\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4316 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3723 - accuracy: 0.9236\n",
            "Dev Loss: 0.37227609753608704 Dev Acc: 0.9236125349998474\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 1.4175 - accuracy: 0.2491\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3609 - accuracy: 0.9239\n",
            "Dev Loss: 0.36090224981307983 Dev Acc: 0.9239212274551392\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4188 - accuracy: 0.2491\n",
            "2002/2002 [==============================] - 9s 5ms/step - loss: 0.3679 - accuracy: 0.9242\n",
            "Dev Loss: 0.36789682507514954 Dev Acc: 0.9242299795150757\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 1.4301 - accuracy: 0.2486\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3869 - accuracy: 0.9222\n",
            "Dev Loss: 0.3869113326072693 Dev Acc: 0.9222404956817627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 200 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10540ea3-6ca7-4564-846f-bcdf3cd0a8b2",
        "id": "1gN3kjho0-Xw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, None, 100)         1967600   \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, None, 100)         0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, None, 400)        481600    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  (None, None, 19)         7619      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,456,819\n",
            "Trainable params: 2,456,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 39s 137ms/step - loss: 1.5451 - accuracy: 0.2136\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2945 - accuracy: 0.9125\n",
            "Dev Loss: 0.2945389747619629 Dev Acc: 0.9124991297721863\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4536 - accuracy: 0.2422\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.2698 - accuracy: 0.9236\n",
            "Dev Loss: 0.2698104977607727 Dev Acc: 0.9235782623291016\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4329 - accuracy: 0.2461\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2709 - accuracy: 0.9248\n",
            "Dev Loss: 0.27087780833244324 Dev Acc: 0.9248473644256592\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4320 - accuracy: 0.2476\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2768 - accuracy: 0.9278\n",
            "Dev Loss: 0.27679160237312317 Dev Acc: 0.9277971982955933\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4261 - accuracy: 0.2494\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2888 - accuracy: 0.9291\n",
            "Dev Loss: 0.2887817323207855 Dev Acc: 0.9291349649429321\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4281 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3022 - accuracy: 0.9280\n",
            "Dev Loss: 0.30218905210494995 Dev Acc: 0.9280373454093933\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4216 - accuracy: 0.2487\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3203 - accuracy: 0.9262\n",
            "Dev Loss: 0.32033026218414307 Dev Acc: 0.9261850714683533\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4364 - accuracy: 0.2480\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3444 - accuracy: 0.9248\n",
            "Dev Loss: 0.3444391191005707 Dev Acc: 0.9248130321502686\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4342 - accuracy: 0.2481\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.3428 - accuracy: 0.9244\n",
            "Dev Loss: 0.34275636076927185 Dev Acc: 0.9244014620780945\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4299 - accuracy: 0.2483\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3499 - accuracy: 0.9234\n",
            "Dev Loss: 0.3498678207397461 Dev Acc: 0.923406720161438\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4229 - accuracy: 0.2488\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3750 - accuracy: 0.9229\n",
            "Dev Loss: 0.37498778104782104 Dev Acc: 0.9228922128677368\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4279 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 9s 5ms/step - loss: 0.3675 - accuracy: 0.9247\n",
            "Dev Loss: 0.36750486493110657 Dev Acc: 0.9246758818626404\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4252 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3933 - accuracy: 0.9244\n",
            "Dev Loss: 0.3933204412460327 Dev Acc: 0.9244357347488403\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4206 - accuracy: 0.2485\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3788 - accuracy: 0.9236\n",
            "Dev Loss: 0.3787628710269928 Dev Acc: 0.9236125349998474\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4235 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3973 - accuracy: 0.9216\n",
            "Dev Loss: 0.3973497748374939 Dev Acc: 0.9215887784957886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 200 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee95dd44-34be-4852-bd30-278cc697b446",
        "id": "VnmoiCWJ3aTd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 200)         3935200   \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, None, 200)         0         \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, None, 200)        240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, None, 19)         3819      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,179,819\n",
            "Trainable params: 4,179,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 52s 187ms/step - loss: 1.5310 - accuracy: 0.2194\n",
            "2002/2002 [==============================] - 11s 5ms/step - loss: 0.2946 - accuracy: 0.9139\n",
            "Dev Loss: 0.2945616543292999 Dev Acc: 0.9139397740364075\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4437 - accuracy: 0.2440\n",
            "2002/2002 [==============================] - 9s 5ms/step - loss: 0.2742 - accuracy: 0.9230\n",
            "Dev Loss: 0.27423909306526184 Dev Acc: 0.9230294227600098\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4349 - accuracy: 0.2467\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2726 - accuracy: 0.9274\n",
            "Dev Loss: 0.2725553810596466 Dev Acc: 0.927419900894165\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 1.4196 - accuracy: 0.2485\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.3173 - accuracy: 0.9235\n",
            "Dev Loss: 0.317348837852478 Dev Acc: 0.9234753251075745\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4180 - accuracy: 0.2492\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3091 - accuracy: 0.9251\n",
            "Dev Loss: 0.3091284930706024 Dev Acc: 0.9250874519348145\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4293 - accuracy: 0.2485\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3407 - accuracy: 0.9267\n",
            "Dev Loss: 0.340670645236969 Dev Acc: 0.9267339110374451\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4371 - accuracy: 0.2484\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3626 - accuracy: 0.9248\n",
            "Dev Loss: 0.36257702112197876 Dev Acc: 0.9248473644256592\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4367 - accuracy: 0.2495\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3829 - accuracy: 0.9262\n",
            "Dev Loss: 0.38294515013694763 Dev Acc: 0.9261507987976074\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4284 - accuracy: 0.2510\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4065 - accuracy: 0.9222\n",
            "Dev Loss: 0.40653085708618164 Dev Acc: 0.9222404956817627\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4263 - accuracy: 0.2835\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.4203 - accuracy: 0.9238\n",
            "Dev Loss: 0.4202764630317688 Dev Acc: 0.9238183498382568\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4276 - accuracy: 0.2637\n",
            "2002/2002 [==============================] - 9s 5ms/step - loss: 0.4242 - accuracy: 0.9213\n",
            "Dev Loss: 0.42419129610061646 Dev Acc: 0.9213144183158875\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 1.4229 - accuracy: 0.2513\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4367 - accuracy: 0.9236\n",
            "Dev Loss: 0.436688095331192 Dev Acc: 0.9236125349998474\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4233 - accuracy: 0.2537\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4290 - accuracy: 0.9240\n",
            "Dev Loss: 0.42898398637771606 Dev Acc: 0.9240241646766663\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4257 - accuracy: 0.2692\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.4371 - accuracy: 0.9247\n",
            "Dev Loss: 0.43714702129364014 Dev Acc: 0.9247101545333862\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4265 - accuracy: 0.2634\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4464 - accuracy: 0.9236\n",
            "Dev Loss: 0.44641953706741333 Dev Acc: 0.923646867275238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 300 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f042b45e-42c7-4844-838d-42087d9965dc",
        "id": "CflPKILo3agA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, None, 300)         5902800   \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, None, 300)         0         \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, None, 200)        320800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, None, 19)         3819      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,227,419\n",
            "Trainable params: 6,227,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 15\n",
            "250/250 [==============================] - 49s 179ms/step - loss: 1.5208 - accuracy: 0.2200\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.2849 - accuracy: 0.9180\n",
            "Dev Loss: 0.2849184572696686 Dev Acc: 0.9179872274398804\n",
            "Epoch 2 / 15\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 1.4431 - accuracy: 0.2441\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2750 - accuracy: 0.9203\n",
            "Dev Loss: 0.2750445306301117 Dev Acc: 0.920319676399231\n",
            "Epoch 3 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4297 - accuracy: 0.2471\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2668 - accuracy: 0.9261\n",
            "Dev Loss: 0.2668241262435913 Dev Acc: 0.9261164665222168\n",
            "Epoch 4 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4275 - accuracy: 0.2482\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3004 - accuracy: 0.9244\n",
            "Dev Loss: 0.3003796339035034 Dev Acc: 0.9243671298027039\n",
            "Epoch 5 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4238 - accuracy: 0.2493\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3099 - accuracy: 0.9251\n",
            "Dev Loss: 0.30987685918807983 Dev Acc: 0.9250874519348145\n",
            "Epoch 6 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4191 - accuracy: 0.2495\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3506 - accuracy: 0.9241\n",
            "Dev Loss: 0.35061773657798767 Dev Acc: 0.9240584373474121\n",
            "Epoch 7 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4167 - accuracy: 0.2498\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3736 - accuracy: 0.9239\n",
            "Dev Loss: 0.3736381232738495 Dev Acc: 0.9239212274551392\n",
            "Epoch 8 / 15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4207 - accuracy: 0.2495\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4172 - accuracy: 0.9219\n",
            "Dev Loss: 0.4171793460845947 Dev Acc: 0.921931803226471\n",
            "Epoch 9 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4243 - accuracy: 0.2495\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4480 - accuracy: 0.9215\n",
            "Dev Loss: 0.4480101466178894 Dev Acc: 0.9214859008789062\n",
            "Epoch 10 / 15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 1.4232 - accuracy: 0.2497\n",
            "2002/2002 [==============================] - 7s 4ms/step - loss: 0.4779 - accuracy: 0.9216\n",
            "Dev Loss: 0.47791001200675964 Dev Acc: 0.9215545058250427\n",
            "Epoch 11 / 15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.4251 - accuracy: 0.2524\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4753 - accuracy: 0.9216\n",
            "Dev Loss: 0.47526535391807556 Dev Acc: 0.9216231107711792\n",
            "Epoch 12 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4270 - accuracy: 0.2525\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4720 - accuracy: 0.9209\n",
            "Dev Loss: 0.47197088599205017 Dev Acc: 0.9209027886390686\n",
            "Epoch 13 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4185 - accuracy: 0.2554\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4537 - accuracy: 0.9200\n",
            "Dev Loss: 0.45371633768081665 Dev Acc: 0.9200452566146851\n",
            "Epoch 14 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4224 - accuracy: 0.2539\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4778 - accuracy: 0.9209\n",
            "Dev Loss: 0.47782328724861145 Dev Acc: 0.9208685159683228\n",
            "Epoch 15 / 15\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4314 - accuracy: 0.2492\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.4845 - accuracy: 0.9214\n",
            "Dev Loss: 0.4845442771911621 Dev Acc: 0.9214172959327698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, test_data, test_labels = get_vocabulary_and_data(test_file)\n",
        "epochs = 6\n",
        "embedding_size = 200 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n",
        "\n",
        "loss, acc = pos_tagger.evaluate(batch_generator(test_data, test_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_DpidqK31xD",
        "outputId": "1827b799-0eb5-4f16-eb8c-95a1ee6a7c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_12 (Embedding)    (None, None, 200)         3935200   \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, None, 200)         0         \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, None, 200)        240800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_12 (TimeDi  (None, None, 19)         3819      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,179,819\n",
            "Trainable params: 4,179,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 6\n",
            "250/250 [==============================] - 38s 138ms/step - loss: 1.5097 - accuracy: 0.2208\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2855 - accuracy: 0.9165\n",
            "Dev Loss: 0.28552359342575073 Dev Acc: 0.916546642780304\n",
            "Epoch 2 / 6\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 1.4415 - accuracy: 0.2440\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2659 - accuracy: 0.9230\n",
            "Dev Loss: 0.2658880650997162 Dev Acc: 0.9230294227600098\n",
            "Epoch 3 / 6\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 1.4319 - accuracy: 0.2468\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2799 - accuracy: 0.9244\n",
            "Dev Loss: 0.27985045313835144 Dev Acc: 0.9244014620780945\n",
            "Epoch 4 / 6\n",
            "250/250 [==============================] - 5s 22ms/step - loss: 1.4341 - accuracy: 0.2479\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.2961 - accuracy: 0.9237\n",
            "Dev Loss: 0.2961147427558899 Dev Acc: 0.9237497448921204\n",
            "Epoch 5 / 6\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4205 - accuracy: 0.2491\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3076 - accuracy: 0.9269\n",
            "Dev Loss: 0.3075568675994873 Dev Acc: 0.9269397258758545\n",
            "Epoch 6 / 6\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 1.4253 - accuracy: 0.2490\n",
            "2002/2002 [==============================] - 9s 4ms/step - loss: 0.3414 - accuracy: 0.9257\n",
            "Dev Loss: 0.3413960039615631 Dev Acc: 0.9257391691207886\n",
            "2002/2002 [==============================] - 8s 4ms/step - loss: 0.3315 - accuracy: 0.9257\n",
            "Test Loss: 0.33149614930152893 Test Acc: 0.9257237911224365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KN9d2ce3YGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}