{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yZqRoIHw-PK5A0yo84Mg3xmJAb7vhzzt",
      "authorship_tag": "ABX9TyNqFUDJEjPFRGB8V430bXVH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e2058bb66fd471f8d7b1c29071b1881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_160bfafe67034acd8dc4c63f52c25d50",
              "IPY_MODEL_8b1baafc54c844d280b4700795109892",
              "IPY_MODEL_d68ce17cc379481b868bdf44f75cd7a1"
            ],
            "layout": "IPY_MODEL_16e36c10841a4924b8351dfcab81ee20"
          }
        },
        "160bfafe67034acd8dc4c63f52c25d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b53e82570ca41c282f5cb569204d3c7",
            "placeholder": "​",
            "style": "IPY_MODEL_cfa8fa76a37240849db249655ccb3a4d",
            "value": "100%"
          }
        },
        "8b1baafc54c844d280b4700795109892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbe116630581439592a7a62647d0cdd7",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6441aff4b946442bbbc7ca06c8521f6c",
            "value": 553433881
          }
        },
        "d68ce17cc379481b868bdf44f75cd7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_123ee60f769e47f7a6552bad78a9c008",
            "placeholder": "​",
            "style": "IPY_MODEL_15f77e974f8a4190bdd3720a5bbe06cb",
            "value": " 528M/528M [00:08&lt;00:00, 147MB/s]"
          }
        },
        "16e36c10841a4924b8351dfcab81ee20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b53e82570ca41c282f5cb569204d3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa8fa76a37240849db249655ccb3a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbe116630581439592a7a62647d0cdd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6441aff4b946442bbbc7ca06c8521f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "123ee60f769e47f7a6552bad78a9c008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15f77e974f8a4190bdd3720a5bbe06cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/COSC576_project_early_Xmas__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4epz13JihL1",
        "outputId": "613c651a-6490-4f24-ce3d-2c144b6513d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CW3VP4R2Kxl_"
      },
      "outputs": [],
      "source": [
        "# @title import library\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre-trained model reference: https://debuggercafe.com/transfer-learning-with-pytorch/"
      ],
      "metadata": {
        "id": "Go_BJ3sljNJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMAGE CLASSIFICATION\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, animal_category, image_size = 255, class_size = 100,transform = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sketch_dir (string): Directory to all the sketch images.\n",
        "            realworld_dir (string): Directory to all the real world images.\n",
        "            animal_category: list to fruit catogory\n",
        "            class_size: Num of images in each category\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.animal_category = animal_category\n",
        "        self.class_size = class_size\n",
        "        self.data_dict = dict(np.load(img_dir,allow_pickle=True))\n",
        "        self.image_size = image_size\n",
        "       \n",
        "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
        "                                                transforms.Resize((image_size,image_size)),\n",
        "                                                transforms.ToTensor(),])\n",
        "                                              # transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.class_size * len(self.animal_category)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        class_index = int(idx // self.class_size)\n",
        "        category =  self.animal_category[class_index]\n",
        "        category_idx = int(idx % self.class_size)\n",
        "        label = np.zeros((len(self.animal_category), 1))\n",
        "        label[class_index] = 1\n",
        "        \n",
        "#         label = class_index\n",
        "        image_ary =  self.data_dict[category][category_idx]\n",
        "        sample = {'image': image_ary, 'label': label}\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform_img(sample['image'])\n",
        "            sample['label'] = self.transform_label(sample['label'])\n",
        "        return sample\n",
        "        "
      ],
      "metadata": {
        "id": "idpV7W0Zia3b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load image data\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "train_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "train_realworld = ImageDataset(train_realworld_dir, QURIES, image_size = 224, class_size = 500,transform = True)\n",
        "train_loader = DataLoader(train_realworld, batch_size=128, shuffle=True, pin_memory=True)\n",
        "\n",
        "test_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/test_images.npz\"\n",
        "test_realworld = ImageDataset(test_realworld_dir, QURIES, image_size = 224, class_size = 100,transform = True)\n",
        "test_loader_realworld = DataLoader(test_realworld, batch_size=128, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "h46CpgM0jeb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load image data test\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "train_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "train_realworld = ImageDataset(train_realworld_dir, QURIES, image_size = 224, class_size = 50,transform = True)\n",
        "train_loader = DataLoader(train_realworld, batch_size=128, shuffle=True, pin_memory=True)\n",
        "\n",
        "test_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/test_images.npz\"\n",
        "test_realworld = ImageDataset(test_realworld_dir, QURIES, image_size = 224, class_size = 10,transform = True)\n",
        "test_loader_realworld = DataLoader(test_realworld, batch_size=128, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "fMpspY7huuFt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = models.vgg16(pretrained=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model.classifier[6] = nn.Linear(4096,10)\n",
        "image_model = image_model.to(device)\n",
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962,
          "referenced_widgets": [
            "9e2058bb66fd471f8d7b1c29071b1881",
            "160bfafe67034acd8dc4c63f52c25d50",
            "8b1baafc54c844d280b4700795109892",
            "d68ce17cc379481b868bdf44f75cd7a1",
            "16e36c10841a4924b8351dfcab81ee20",
            "3b53e82570ca41c282f5cb569204d3c7",
            "cfa8fa76a37240849db249655ccb3a4d",
            "fbe116630581439592a7a62647d0cdd7",
            "6441aff4b946442bbbc7ca06c8521f6c",
            "123ee60f769e47f7a6552bad78a9c008",
            "15f77e974f8a4190bdd3720a5bbe06cb"
          ]
        },
        "id": "divQilguj1IS",
        "outputId": "af102fa6-43cd-43b2-8996-c7c36eeb3f98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e2058bb66fd471f8d7b1c29071b1881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model.classifier[6] = nn.Linear(4096,10)"
      ],
      "metadata": {
        "id": "6s8Tapdks2NF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhKDn-R2ulMb",
        "outputId": "5c8b21bf-7a36-4add-803b-65a5405ab611"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = image_model.to(device)"
      ],
      "metadata": {
        "id": "xDI5R4IwwFKX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_model.classifier[6].out_features = 10\n",
        "# ****************************\n",
        "# freeze convolution weights \n",
        "for param in image_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(image_model.eval())"
      ],
      "metadata": {
        "id": "cbkHRz7ZkPNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720e8148-92c0-4ba1-bc56-801c29ae4b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.01 #0.03\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(image_model.classifier.parameters(), lr=learning_rate, weight_decay = 0.007, momentum = 0.9)  \n",
        "LOAD = False\n",
        "start_epoch =0\n",
        "train_realworld_acc = []\n",
        "train_realworld_loss = []\n",
        "val_realworld_acc = []\n",
        "val_realworld_loss = []\n",
        "import gc\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(start_epoch,start_epoch+num_epochs):\n",
        "    for data in train_loader:  \n",
        "        # Move tensors to the configured device\n",
        "        images = data['image'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        correct_t = 0\n",
        "        total_t = 0\n",
        "        # Forward pass\n",
        "        image_model.train()\n",
        "        outputs = image_model(images)\n",
        "        loss = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "        _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "        total_t += labels.size(0)\n",
        "        correct_t +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        train_realworld_loss.append(loss.item())\n",
        "    train_realworld_acc.append(correct_t/total_t)\n",
        "    print ('Epoch [{}/{}], Training Loss: {:.4f}' \n",
        "                   .format(epoch+1,start_epoch+num_epochs, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_model.eval()\n",
        "        correct_v = 0\n",
        "        total_v = 0\n",
        "        for data in test_loader_realworld:\n",
        "            images = data['image'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "        \n",
        "\n",
        "            outputs = image_model(images)\n",
        "            loss_v = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "   \n",
        "            _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "            total_v += labels.size(0)\n",
        "            correct_v +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "            \n",
        "            del images, labels, outputs\n",
        "        val_realworld_loss.append(loss_v.item())\n",
        "        val_realworld_acc.append(correct_v/total_v)\n",
        "        print(f\"Validation accuracy: {(correct_v/total_v):.3f}\")\n",
        "       \n",
        "print(f\"train_acc:{(np.mean([v for v in train_realworld_acc])):.3f},val_acc:{(np.mean([v for v in val_realworld_acc])):.3f}\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jwo605SkgiI",
        "outputId": "1719b5c6-2da5-400f-bc97-c7fbd1613121"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Loss: 0.7211\n",
            "Validation accuracy: 0.830\n",
            "Epoch [2/10], Training Loss: 0.3560\n",
            "Validation accuracy: 0.890\n",
            "Epoch [3/10], Training Loss: 0.1586\n",
            "Validation accuracy: 0.920\n",
            "Epoch [4/10], Training Loss: 0.1521\n",
            "Validation accuracy: 0.900\n",
            "Epoch [5/10], Training Loss: 0.0875\n",
            "Validation accuracy: 0.910\n",
            "Epoch [6/10], Training Loss: 0.0399\n",
            "Validation accuracy: 0.920\n",
            "Epoch [7/10], Training Loss: 0.0369\n",
            "Validation accuracy: 0.910\n",
            "Epoch [8/10], Training Loss: 0.0165\n",
            "Validation accuracy: 0.920\n",
            "Epoch [9/10], Training Loss: 0.0172\n",
            "Validation accuracy: 0.920\n",
            "Epoch [10/10], Training Loss: 0.0083\n",
            "Validation accuracy: 0.910\n",
            "train_acc:0.965,val_acc:0.903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuLt19yDyJE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        image_model.eval()\n",
        "        correct_v = 0\n",
        "        total_v = 0\n",
        "        for data in test_loader_realworld:\n",
        "            images = data['image'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "        \n",
        "\n",
        "            outputs = image_model(images)\n",
        "            print(outputs.shape)\n",
        "            loss_v = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "   \n",
        "            _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "            total_v += labels.size(0)\n",
        "            correct_v +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "            \n",
        "            del images, labels, outputs\n",
        "        val_realworld_loss.append(loss_v.item())\n",
        "        val_realworld_acc.append(correct_v/total_v)\n",
        "        print(f\"Validation accuracy: {(correct_v/total_v):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQZXV7SUpCJY",
        "outputId": "630c05a4-fcaa-4c62-f662-30e51303b80f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 10])\n",
            "Validation accuracy: 0.910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "# image_model.classifier[6]=nn.Linear(4096,10)\n",
        "summary(image_model, (3, 224, 224)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "OD6NwEbmpXSa",
        "outputId": "e820916b-ec2c-4956-c395-d0aafe353008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-62027b31c02a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# image_model.classifier[6]=nn.Linear(4096,10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save image data model\n",
        "image_model_path = \"/content/drive/MyDrive/kaggle/imagenet/vgg16_pretrained_realworld_epoch30_loss_lr_001_1204_lucky.pth\"\n",
        "\n",
        "\n",
        "torch.save(image_model,image_model_path)"
      ],
      "metadata": {
        "id": "Y_9E4pPezYAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SKETCH CLASSIFICATION\n",
        "categories = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]\n",
        "# categories = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "label_dict = {0:'bear',1:'camel',2:'cat', 3:'dog', 4:'elephant',\n",
        "                      5:'frog',6:'lion', 7:'panda', 8:'rabbit', 9:'squirrel'}\n",
        "\n",
        "# load data for each category\n",
        "classes = {}\n",
        "for category in categories:\n",
        "    # ctl's path\n",
        "    data = pd.read_csv(\"./drive/MyDrive/kaggle/sketch/\" + category + \".csv\")\n",
        "    # lzx's path\n",
        "    # data = pd.read_csv(\"./drive/MyDrive/sketch/\" + category + \".csv\")\n",
        "    classes[category] = data"
      ],
      "metadata": {
        "id": "erI_RrcGzohR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title functions\n",
        "# Image manipulation utilities: \n",
        "\n",
        "def convert_to_PIL(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert from drawing to PIL image.\n",
        "    INPUT:\n",
        "        drawing - drawing from 'drawing' column\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        pil_img - (PIL Image) image\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize empty (white) PIL image\n",
        "    pil_img = Image.new('RGB', (width, height), 'white')\n",
        "    pixels = pil_img.load()\n",
        "            \n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    \n",
        "    # draw strokes as lines\n",
        "    for x,y in drawing:\n",
        "        for i in range(1, len(x)):\n",
        "            draw.line((x[i-1], y[i-1], x[i], y[i]), fill=0)\n",
        "        \n",
        "    return pil_img\n",
        "\n",
        "\n",
        "def convert_to_np_raw(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        drawing - drawing in initial format\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        img - drawing converted to the numpy array (28 X 28)\n",
        "    \"\"\"\n",
        "    # initialize empty numpy array\n",
        "    img = np.zeros((28, 28))\n",
        "    \n",
        "    # create a PIL image out of drawing\n",
        "    pil_img = convert_to_PIL(drawing)\n",
        "    \n",
        "    #resize to 28,28\n",
        "    pil_img.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    pil_img = pil_img.convert('RGB')\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    # fill in numpy array with pixel values\n",
        "    for i in range(0, 28):\n",
        "        for j in range(0, 28):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "    \n",
        "    return img\n",
        "\n",
        "def convert_to_np(pil_img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert PIL Image to numpy array.\n",
        "    INPUT:\n",
        "        pil_img - (PIL Image) image to be converted\n",
        "    OUTPUT:\n",
        "        img - (numpy array) converted image with shape (width, height)\n",
        "    \"\"\"\n",
        "    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "    img = np.zeros((width, height))\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    for i in range(0, width):\n",
        "      for j in range(0, height):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def view_image(img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to view numpy image with matplotlib.\n",
        "    The function saves the image as png.\n",
        "    INPUT:\n",
        "        img - (numpy array) image from train dataset with size (1, 784)\n",
        "    OUTPUT:\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,9))\n",
        "    ax.imshow(img.reshape(width, height).squeeze())\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "def crop_image(image):\n",
        "    \"\"\"\n",
        "    Crops image (crops out white spaces).\n",
        "    INPUT:\n",
        "        image - PIL image of original size to be cropped\n",
        "    OUTPUT:\n",
        "        cropped_image - PIL image cropped to the center  and resized to (28, 28)\n",
        "    \"\"\"\n",
        "    cropped_image = image\n",
        "\n",
        "    # get image size\n",
        "    width, height = cropped_image.size\n",
        "\n",
        "    # get image pixels\n",
        "    pixels = cropped_image.load()\n",
        "\n",
        "    image_strokes_rows = []\n",
        "    image_strokes_cols = []\n",
        "\n",
        "    # run through the image\n",
        "    for i in range(0, width):\n",
        "        for j in range(0, height):\n",
        "            # save coordinates of the image\n",
        "            if (pixels[i,j][0] > 0):\n",
        "                image_strokes_cols.append(i)\n",
        "                image_strokes_rows.append(j)\n",
        "\n",
        "    # if image is not empty then crop to contents of the image\n",
        "    if (len(image_strokes_rows)) > 0:\n",
        "        # find the box for image\n",
        "        row_min = np.array(image_strokes_rows).min()\n",
        "        row_max = np.array(image_strokes_rows).max()\n",
        "        col_min = np.array(image_strokes_cols).min()\n",
        "        col_max = np.array(image_strokes_cols).max()\n",
        "\n",
        "        # find the box for cropping\n",
        "        margin = min(row_min, height - row_max, col_min, width - col_max)\n",
        "\n",
        "        # crop image\n",
        "        border = (col_min, row_min, width - col_max, height - row_max)\n",
        "        cropped_image = ImageOps.crop(cropped_image, border)\n",
        "\n",
        "    # get cropped image size\n",
        "    width_cropped, height_cropped = cropped_image.size\n",
        "\n",
        "    # create square resulting image to paste cropped image into the center\n",
        "    dst_im = Image.new(\"RGBA\", (max(width_cropped, height_cropped), max(width_cropped, height_cropped)), \"white\")\n",
        "    offset = ((max(width_cropped, height_cropped) - width_cropped) // 2, (max(width_cropped, height_cropped) - height_cropped) // 2)\n",
        "    # paste to the center of a resulting image\n",
        "    dst_im.paste(cropped_image, offset)\n",
        "\n",
        "    #resize to 28,28\n",
        "    dst_im.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    return dst_im\n",
        "def normalize(arr):\n",
        "    \"\"\"\n",
        "    Function performs the linear normalizarion of the array.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
        "    INPUT:\n",
        "        arr - orginal numpy array\n",
        "    OUTPUT:\n",
        "        arr - normalized numpy array\n",
        "    \"\"\"\n",
        "    arr = arr.astype('float')\n",
        "    # Do not touch the alpha channel\n",
        "    for i in range(3):\n",
        "        minval = arr[...,i].min()\n",
        "        maxval = arr[...,i].max()\n",
        "        if minval != maxval:\n",
        "            arr[...,i] -= minval\n",
        "            arr[...,i] *= (255.0/(maxval-minval))\n",
        "    return arr\n",
        "\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Function performs the normalization of the image.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    INPUT:\n",
        "        image - PIL image to be normalized\n",
        "    OUTPUT:\n",
        "        new_img - PIL image normalized\n",
        "    \"\"\"\n",
        "    arr = np.array(image)\n",
        "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGBA')\n",
        "    return new_img\n",
        "\n",
        "def rotate_image(src_im, angle = 45, size = (28,28)):\n",
        "    \"\"\"\n",
        "    Function to rotate PIL Image file\n",
        "    INPUT:\n",
        "        src_im - (PIL Image) 28x28 image to be rotated\n",
        "        angle - angle to rotate the image\n",
        "        size - (tuple) size of the output image\n",
        "    OUTPUT:\n",
        "    dst_im - (PIL Image) rotated image\n",
        "    \"\"\"\n",
        "    dst_im = Image.new(\"RGBA\", size, \"white\")\n",
        "    src_im = src_im.convert('RGBA')\n",
        "\n",
        "    rot = src_im.rotate(angle)\n",
        "    dst_im.paste(rot, (0, 0), rot)\n",
        "\n",
        "    return dst_im\n",
        "def flip_image(src_im):\n",
        "    \"\"\"\n",
        "    Function to flip a PIL Image file.\n",
        "    INPUT:\n",
        "        scr_im - (PIL Image) image to be flipped\n",
        "    OUTPUT:\n",
        "        dst_im - (PIL Image) flipped image\n",
        "    \"\"\"\n",
        "    dst_im = src_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return dst_im\n"
      ],
      "metadata": {
        "id": "cQmbKP7d7VVn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import library\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "irW_YxbZKasJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load and shrinking the sketch images\n",
        "\n",
        "# create the dictionary containing classes names as keys and images as values\n",
        "values_dict = {}\n",
        "for category in categories:\n",
        "    data = classes[category][:3000]\n",
        "    values = [convert_to_np_raw(ast.literal_eval(img)).reshape(1, 784) for img in data['drawing'].values]\n",
        "    values_dict[category] = values\n",
        "    \n",
        "# concatenate to create X (values) and y (labels) datasets\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for key, value in label_dict.items():\n",
        "    data_i = values_dict[value]\n",
        "    Xi = np.concatenate(data_i, axis = 0)\n",
        "    yi = np.full((len(Xi), 1), key).ravel()\n",
        "    \n",
        "    X.append(Xi)\n",
        "    y.append(yi)\n",
        "X = np.concatenate(X, axis = 0)\n",
        "y = np.concatenate(y, axis = 0)"
      ],
      "metadata": {
        "id": "qkJDAgCy65Ob",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import library\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SvCXyNEv6ovL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sketch based on LeNet \n",
        "def resize(x, kernel_size, dilation, stride, padding):\n",
        "    x = int(1 + (x + 2*padding - dilation * (kernel_size - 1) - 1)/stride)\n",
        "    return x\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, specs, dropout=0.0):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.specs = specs\n",
        "        H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding = specs\n",
        "        pooling = 2\n",
        "        #pooling = 1 # skips pooling\n",
        "        stride = 1\n",
        "        dilation = 1\n",
        "\n",
        "        #self.pool = pool = nn.AvgPool2d(pooling)\n",
        "        self.pool = pool = nn.MaxPool2d(pooling)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(C0, C1, kernel_size, padding=padding)\n",
        "        # in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2\n",
        "        #self.conv1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        H = resize(H, kernel_size, dilation, stride, padding)\n",
        "        W = resize(W, kernel_size, dilation, stride, padding)\n",
        "\n",
        "        H = resize(H, pooling, dilation, pooling, 0)\n",
        "        W = resize(W, pooling, dilation, pooling, 0)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(C1, C2, kernel_size, padding=padding)\n",
        "\n",
        "        H = resize(H, kernel_size, dilation, stride, padding)\n",
        "        W = resize(W, kernel_size, dilation, stride, padding)\n",
        "\n",
        "        H = resize(H, pooling, dilation, pooling, 0)\n",
        "        W = resize(W, pooling, dilation, pooling, 0)\n",
        "\n",
        "        #print(H, W)\n",
        "        size = H * W * C2\n",
        "\n",
        "        self.linear0 = nn.Linear(size, F1)\n",
        "        self.linear1 = nn.Linear(F1, F2)\n",
        "        self.linear2 = nn.Linear(F2, nDigits)\n",
        "\n",
        "        self.non_linear = nn.LeakyReLU(negative_slope=0.01)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        for p in self.parameters(): # optionally apply different randomization\n",
        "            if p.dim() > 1:\n",
        "                nn.init.kaiming_normal_(p)\n",
        "                pass\n",
        "\n",
        "    def forward(self, prev):\n",
        "        nBatch = len(prev)\n",
        "        #print(prev.shape)\n",
        "        prev = self.conv1(prev)#\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "        prev = self.pool(prev)\n",
        "\n",
        "        prev = self.conv2(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "        prev = self.pool(prev)\n",
        "\n",
        "        prev = prev.view(nBatch, -1)\n",
        "        #print(prev.shape)\n",
        "\n",
        "        prev = self.linear0(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "\n",
        "        prev = self.linear1(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "\n",
        "        prev = self.linear2(prev)\n",
        "\n",
        "        return prev\n",
        "##\n",
        "def build_model(input_size, output_size, hidden_sizes, dropout = 0.0):\n",
        "    '''\n",
        "    Function creates deep learning model based on parameters passed.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - layer sizes\n",
        "        dropout - dropout (probability of keeping a node)\n",
        "\n",
        "    OUTPUT:\n",
        "        model - deep learning model\n",
        "    '''\n",
        "\n",
        "    # Build a feed-forward network\n",
        "    #modelCNN = CNNModel()\n",
        "\n",
        "    H=28    # don't change -- actual images are 28x28, not 32x32 -- H = height of image\n",
        "    W=28    # don't change -- actual images are 28x28, not 32x32 -- W = width of image\n",
        "    C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "    C1=6\n",
        "    C2=16\n",
        "    kernel_size=5\n",
        "    F1 = 120\n",
        "    F2 = 84\n",
        "    nDigits=10    # don't change -- # outputs -- 10 digits\n",
        "    padding=2\n",
        "    specs = [H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding]\n",
        "    modelLeNet = LeNet(specs, dropout=0.1)\n",
        "    \n",
        "    return modelLeNet\n",
        "\n",
        "def shuffle(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Function which shuffles training dataset.\n",
        "    INPUT:\n",
        "        X_train - (tensor) training set\n",
        "        y_train - (tensor) labels for training set\n",
        "\n",
        "    OUTPUT:\n",
        "        X_train_shuffled - (tensor) shuffled training set\n",
        "        y_train_shuffled - (tensor) shuffled labels for training set\n",
        "        \"\"\"\n",
        "    X_train_shuffled = X_train.numpy()\n",
        "    y_train_shuffled = y_train.numpy().reshape((X_train.shape[0], 1))\n",
        "\n",
        "    permutation = list(np.random.permutation(X_train.shape[0]))\n",
        "    X_train_shuffled = X_train_shuffled[permutation, :]\n",
        "    y_train_shuffled = y_train_shuffled[permutation, :].reshape((X_train.shape[0], 1))\n",
        "\n",
        "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
        "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
        "\n",
        "    return X_train_shuffled, y_train_shuffled\n",
        "\n",
        "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function which fits the model.\n",
        "    INPUT:\n",
        "        model - pytorch model to fit\n",
        "        X_train - (tensor) train dataset\n",
        "        y_train - (tensor) train dataset labels\n",
        "        epochs - number of epochs\n",
        "        n_chunks - number of chunks to cplit the dataset\n",
        "        learning_rate - learning rate value\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\".format(epochs = epochs, lr = learning_rate))\n",
        "    \n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if (optimizer == 'SGD'):\n",
        "      optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "\n",
        "    print_every = 10\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "        images = torch.chunk(X_train, n_chunks)\n",
        "        labels = torch.chunk(y_train, n_chunks)\n",
        "\n",
        "        for i in range(n_chunks):\n",
        "            steps += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward passes\n",
        "            output = model.forward(images[i])\n",
        "            loss = criterion(output, labels[i].squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        if epochs % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            running_loss = 0\n",
        "                            \n",
        "def view_classify(img, ps):\n",
        "    \"\"\"\n",
        "    Function for viewing an image and it's predicted classes\n",
        "    with matplotlib.\n",
        "\n",
        "    INPUT:\n",
        "        img - (tensor) image file\n",
        "        ps - (tensor) predicted probabilities for each class\n",
        "    \"\"\"\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels([\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_model(model, img):\n",
        "    \"\"\"\n",
        "    Function creates test view of the model's prediction for image.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        img - (tensor) image from the dataset\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert 2D image to 1D vector\n",
        "    img = img.resize_(1, 784)\n",
        "\n",
        "    ps = get_preds(model, img)\n",
        "    view_classify(img.resize_(1, 28, 28), ps)\n",
        "\n",
        "\n",
        "def get_preds(model, input):\n",
        "    \"\"\"\n",
        "    Function to get predicted probabilities from the model for each class.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        input - (tensor) input vector\n",
        "\n",
        "    OUTPUT:\n",
        "        ps - (tensor) vector of predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Turn off gradients to speed up this part\n",
        "    with torch.no_grad():\n",
        "        logits = model.forward(input)\n",
        "    ps = F.softmax(logits, dim=1)\n",
        "    return ps\n",
        "\n",
        "def get_labels(pred):\n",
        "    \"\"\"\n",
        "        Function to get the vector of predicted labels for the images in\n",
        "        the dataset.\n",
        "\n",
        "        INPUT:\n",
        "            pred - (tensor) vector of predictions (probabilities for each class)\n",
        "        OUTPUT:\n",
        "            pred_labels - (numpy) array of predicted classes for each vector\n",
        "    \"\"\"\n",
        "\n",
        "    pred_np = pred.numpy()\n",
        "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
        "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
        "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
        "\n",
        "    return pred_labels\n",
        "def evaluate_model(model, train, y_train, test, y_test):\n",
        "    \"\"\"\n",
        "    Function to print out train and test accuracy of the model.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        train - (tensor) train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "\n",
        "    OUTPUT:\n",
        "        accuracy_train - accuracy on train dataset\n",
        "        accuracy_test - accuracy on test dataset\n",
        "    \"\"\"\n",
        "    train_pred = get_preds(model, train)\n",
        "    train_pred_labels = get_labels(train_pred)\n",
        "\n",
        "    test_pred = get_preds(model, test)\n",
        "    test_pred_labels = get_labels(test_pred)\n",
        "\n",
        "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
        "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
        "\n",
        "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
        "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
        "\n",
        "    return accuracy_train, accuracy_test\n",
        "\n",
        "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function to plot learning curve depending on the number of epochs.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - model parameters\n",
        "        train - (tensor) train dataset\n",
        "        labels - (tensor) labels for train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "        learning_rate - learning rate hyperparameter\n",
        "        weight_decay - weight decay (regularization)\n",
        "        dropout - dropout for hidden layer\n",
        "        n_chunks - the number of minibatches to train the model\n",
        "        optimizer - optimizer to be used for training (SGD or Adam)\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for epochs in np.arange(10, 60, 10):\n",
        "        # create model\n",
        "        sketch_model = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
        "\n",
        "        # fit model\n",
        "        fit_model(sketch_model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = 'SGD')\n",
        "        # get accuracy\n",
        "        accuracy_train, accuracy_test = evaluate_model(sketch_model, train, y_train, test, y_test)\n",
        "\n",
        "        train_acc.append(accuracy_train)\n",
        "        test_acc.append(accuracy_test)\n",
        "\n",
        "    \n",
        "    return train_acc, test_acc, sketch_model"
      ],
      "metadata": {
        "id": "gw-1MAPQ52wE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  My trial:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch import optim,nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "###############################################################################\n",
        "import time, os, sys, random, datetime\n",
        "use_cuda = torch.cuda.is_available()\n",
        "L2_lambda = 0.001\n",
        "global nEpochs\n",
        "nEpochs = 100\n",
        "# nEpochs = 2\n",
        "\n",
        "log_interval = 100\n",
        "#log_interval = 10\n",
        "learning_rate = 0.0005  # default\n",
        "\n",
        "class DatasetFromCSV(Dataset):\n",
        "    def __init__(self,datas,labels,height,width,transforms=None):\n",
        "        #self.data = pd.read_csv(csv_path)\n",
        "        self.data = datas\n",
        "        self.labels = labels\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # 读取所有像素值，并将 1D array ([784]) reshape 成为 2D array ([28,28])\n",
        "        img_as_np = np.asarray(self.data[index][:]).reshape(28, 28).astype(float)\n",
        "        # 把 numpy array 格式的图像转换成灰度 PIL image\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        # img_as_img = img_as_img.convert('L')\n",
        "        # 将图像转换成 tensor\n",
        "        if self.transforms is not None:\n",
        "            img_as_tensor = self.transforms(img_as_img)\n",
        "            # 返回图像及其 label\n",
        "        return (img_as_tensor, single_image_label)\n",
        " \n",
        "    def __len__(self):\n",
        "        #datacopy=self.data.copy().tolist()\n",
        "        #return len(datacopy.index)\n",
        "        return len(self.data)#这里是不是有错？？\n",
        " \n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "      return arr.cuda()\n",
        "    return arr\n",
        "def train_LeNet(model, train_loader, test_loader):\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # define the loss functions\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    # choose an optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "    start = time.time()\n",
        "    w_decay = 0.95 # smoothing factor for reporting results\n",
        "    for e in range(nEpochs):\n",
        "        total_train_images = 0\n",
        "        total_train_loss = 0\n",
        "        train_images = 0\n",
        "        train_loss = 0\n",
        "        w_images = 0\n",
        "        w_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = cuda(data), cuda(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_images += len(data)\n",
        "            train_loss += loss.data.item()\n",
        "\n",
        "            if train_images > log_interval:\n",
        "                total_train_images += train_images\n",
        "                total_train_loss += train_loss\n",
        "                if w_images == 0:\n",
        "                    w_loss = train_loss\n",
        "                    w_images = train_images\n",
        "                else:\n",
        "                    w_images = w_decay * w_images + train_images\n",
        "                    w_loss = w_decay * w_loss + train_loss\n",
        "                #log_message(None, \"%3d %8d %8.3f %8.3f     %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "                # print(\"%3d %8d %8.3f %8.3f     %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "\n",
        "                train_images = 0\n",
        "                train_loss = 0\n",
        "            #     #break\n",
        "\n",
        "        test_images = 0\n",
        "        test_loss = 0\n",
        "        nCorrect = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                data, target = cuda(data), cuda(target)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                test_images += len(data)\n",
        "                test_loss += loss.data.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max value\n",
        "                nCorrect += pred.eq(target.view_as(pred)).sum().item() # count correct items\n",
        "\n",
        "        #log_message(log_file, \"%3d %8d %8.3f %8.3f %8.3f %8.1f%%     %6.1f\" % (e, (e+1)*total_train_images, total_train_loss/total_train_images, w_loss/w_images, test_loss/test_images, 100*nCorrect/test_images, (time.time()-start)))\n",
        "\n",
        "        print(\"%3d %8d %8.1f%% \" % (e, test_loss/test_images, 100*nCorrect/test_images))\n",
        "\n",
        "    return model\n",
        "\n",
        "batch_size = 256\n",
        "transform1 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "transform2 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "# Split dataset into train/test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
        "train_data = DatasetFromCSV(X_train,y_train,28,28,transform1)\n",
        "test_data = DatasetFromCSV(X_test,y_test,28,28,transform2)\n",
        " \n",
        "train_loader = DataLoader(train_data,batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size)\n",
        " \n",
        "#img,lab = next(iter(train_loader))\n",
        "#print(img.shape)\n",
        "\n",
        "H=28    # don't change -- actual images are 28x28, not 32x32 -- H = height of image\n",
        "W=28    # don't change -- actual images are 28x28, not 32x32 -- W = width of image\n",
        "C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "C1=6\n",
        "C2=16\n",
        "kernel_size=5\n",
        "F1 = 120\n",
        "F2 = 84\n",
        "nDigits=10    # don't change -- # outputs -- 10 digits\n",
        "padding=0\n",
        "\n",
        "#C1 = 20\n",
        "#padding = 1\n",
        "\n",
        "specs = [H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding]\n",
        "model = LeNet(specs, dropout=0.1)\n",
        "print(model)\n",
        "model = train_LeNet(model,train_loader,test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "PyEU87M35XHU",
        "outputId": "ff426a4e-82ad-40af-983b-3bb937412edb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8f42a5f3bcea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Split dataset into train/test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3nTtuox9wUw",
        "outputId": "58e76c2f-2bd1-4296-afab-4d83908ae3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 9 9 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load image data model\n",
        "image_model_path = \"/content/drive/MyDrive/kaggle/imagenet/vgg16_pretrained_realworld_epoch30_loss_lr_001_1204_lucky.pth\"\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model=torch.load(image_model_path)\n",
        "image_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKbHzGnIQmFB",
        "outputId": "8b141c01-3d80-4054-8b08-0f579beaf929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "-u1W-0VWJWsJ",
        "outputId": "ea566738-4698-4030-e140-0c8bbdc2515f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-74e8a328dd2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1208\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'VGG' object has no attribute 'summary'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get features\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "\n",
        "search_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "search_realworld = ImageDataset(search_realworld_dir, QURIES, image_size = 224, class_size = 40,transform = True)\n",
        "search_loader_realworld = DataLoader(search_realworld, batch_size=20, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "E9SzjSe6SVrX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model_feature = image_model.features"
      ],
      "metadata": {
        "id": "p2VXOlysRsyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # 如果你想feature的梯度能反向传播，那么去掉 detach（）\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "model.classifier[6].register_forward_hook(get_activation('6'))\n",
        "output = model()\n",
        "print(activation['fc2'])"
      ],
      "metadata": {
        "id": "WOzEYBzCpUbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # 如果你想feature的梯度能反向传播，那么去掉 detach（）\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "outputs_= []\n",
        "def hook(module, input, output):\n",
        "    outputs_.append(output)\n",
        "\n",
        "image_model.classifier[6].register_forward_hook(hook)\n",
        "# output = model()\n",
        "# print(activation['fc2'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in search_loader_realworld:\n",
        "        data['label'] = torch.argmax(data['label'].squeeze(),dim = 1)\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "        outputs = image_model(images)\n",
        "\n",
        "        print(outputs_[0].shape)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcCraF8cfPtS",
        "outputId": "1ffbdc19-fcbb-49e9-de23-e702af57b7fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "features_real = defaultdict(list)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in search_loader_realworld:\n",
        "        data['label'] = torch.argmax(data['label'].squeeze(),dim = 1)\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "        outputs = image_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        labels_sq = labels.tolist()\n",
        "        for i in range(outputs.shape[0]):\n",
        "              features_real[labels_sq[i]].append(outputs[i])\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "fea2 = torch.empty((40*10,10))\n",
        "for i in range(len(features_real)):\n",
        "\n",
        "  fea2[i*40:40*(i+1)] = torch.stack(features_real[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDpA267fhHHW",
        "outputId": "9d4660cc-e2d2-42e2-80ca-263ee982b6e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fea2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0lM_VDTzfKf",
        "outputId": "60125642-d960-4d17-b95c-b243624dc684"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_real[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_irlTB3zjmB",
        "outputId": "c72d60ab-de70-464e-a10a-b64ae94812d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([-1.7640,  7.6825, -0.8944,  0.5999, -0.6326, -3.5277,  1.6245, -3.0279,\n",
              "          1.6631, -0.9435], device='cuda:0'),\n",
              " tensor([-0.1974,  9.3408,  1.9769,  0.0132,  1.4012, -2.6976, -1.0117, -2.0883,\n",
              "         -0.7866, -5.6725], device='cuda:0'),\n",
              " tensor([-3.1950, 16.2866, -3.6793,  3.1858,  2.7311, -6.8855,  0.3871, -1.5081,\n",
              "         -0.4522, -5.0590], device='cuda:0'),\n",
              " tensor([-0.4258, 12.3606, -4.2736, -2.4706,  4.1064, -6.8524,  0.8343, -2.7495,\n",
              "          4.4699, -3.5563], device='cuda:0'),\n",
              " tensor([-1.8822, 11.3686, -2.3376, -1.9689,  2.4231, -4.0952,  3.3906, -0.8676,\n",
              "         -1.8722, -4.2168], device='cuda:0'),\n",
              " tensor([-7.1024, 17.4175, -5.8891,  3.0987,  2.4581, -5.7692,  5.4053, -3.1266,\n",
              "         -2.3910, -1.2160], device='cuda:0'),\n",
              " tensor([-2.0674, 12.8169, -5.0347, -1.4861,  5.1220, -4.1262,  0.0263,  1.2586,\n",
              "         -2.6396, -2.1109], device='cuda:0'),\n",
              " tensor([-4.9356, 15.2406, -1.8889,  0.5792,  4.0911, -3.8433,  2.7159, -6.5469,\n",
              "         -1.6041, -1.6309], device='cuda:0'),\n",
              " tensor([-2.1667, 15.0053, -4.2945, -2.0441,  1.6606, -3.5335,  5.1630, -4.5202,\n",
              "         -0.3867, -2.3856], device='cuda:0'),\n",
              " tensor([-2.9922, 12.0084, -2.3740, -0.1142,  6.1259, -5.5459,  0.8651,  0.1298,\n",
              "         -2.8632, -4.0137], device='cuda:0'),\n",
              " tensor([-4.0273, 20.1333, -5.7881,  3.8987,  3.0885, -5.7687,  1.8485, -6.5774,\n",
              "         -2.0636, -3.1615], device='cuda:0'),\n",
              " tensor([-7.8078, 12.0092, -0.5176, -1.1357,  0.6949, -1.6529,  3.5022, -1.9253,\n",
              "         -0.2587, -3.4147], device='cuda:0'),\n",
              " tensor([-6.0615, 15.5475, -5.8410,  3.4028,  5.1554, -9.4642,  4.6337, -5.8858,\n",
              "          1.4457,  0.2583], device='cuda:0'),\n",
              " tensor([-6.7289, 20.0988, -5.4587, -1.3958, 10.2121, -5.7815,  2.1174, -7.6500,\n",
              "          0.1329, -4.0095], device='cuda:0'),\n",
              " tensor([-6.4420, 14.5797, -5.0170,  0.7637,  2.1865, -5.0289,  2.7409, -5.4837,\n",
              "          3.4046, -0.0627], device='cuda:0'),\n",
              " tensor([-1.1093,  8.4307, -2.3068, -4.6965,  3.0989, -2.7362,  0.9919, -2.0359,\n",
              "          2.7442, -2.3284], device='cuda:0'),\n",
              " tensor([-4.6864, 10.9368, -5.6474, -0.3081,  3.5047, -2.0585,  5.5966, -3.4524,\n",
              "         -2.3054, -0.5284], device='cuda:0'),\n",
              " tensor([-2.7688, 12.7590, -3.0034, -2.8343,  3.7018, -5.3539,  3.4812, -0.4071,\n",
              "         -2.8628, -2.3291], device='cuda:0'),\n",
              " tensor([-4.0990, 19.7218, -6.2873, -2.9989,  7.5739, -8.9270,  3.9501, -7.7411,\n",
              "          0.0914,  0.7034], device='cuda:0'),\n",
              " tensor([-4.6535,  7.1197, -1.4538,  2.2636,  0.2938, -0.8283,  1.0675, -3.4574,\n",
              "          1.1442, -0.4469], device='cuda:0'),\n",
              " tensor([ -1.7194,  14.5837,  -5.8738,   0.1681,   7.5289, -10.2928,   4.0636,\n",
              "          -6.3612,   0.1659,  -1.6058], device='cuda:0'),\n",
              " tensor([-6.1755, 21.5844, -6.4088, -0.1444,  6.7560, -7.8601,  5.1445, -8.7193,\n",
              "          1.7120, -4.4619], device='cuda:0'),\n",
              " tensor([-5.4810e-01,  5.2956e+00, -5.1728e-01,  4.1439e-01,  9.7749e-01,\n",
              "         -2.7327e+00, -2.6962e-02, -2.4951e+00,  3.7610e-03, -4.7330e-01],\n",
              "        device='cuda:0'),\n",
              " tensor([-5.0435, 17.2770, -3.2923, -1.9509,  3.8828, -5.6359,  5.7744, -4.8718,\n",
              "          0.5904, -5.0397], device='cuda:0'),\n",
              " tensor([-6.3573,  8.6216,  0.2325, -0.0445, -0.3520,  0.3977, -1.4918, -2.6142,\n",
              "          2.1190,  0.1824], device='cuda:0'),\n",
              " tensor([-7.0651, 19.0845, -3.2514, -0.3651,  4.5726, -5.3015, -5.7866, -3.0086,\n",
              "          4.1800, -1.0581], device='cuda:0'),\n",
              " tensor([-7.3449, 26.9107, -5.5541, -4.0189,  7.8191, -8.6127,  6.2197, -7.2014,\n",
              "         -0.8524, -5.2457], device='cuda:0'),\n",
              " tensor([-10.0573,  29.7849,  -6.8161,   3.3142,   2.8660,  -9.1889,   4.5737,\n",
              "          -8.2818,   0.9418,  -3.8679], device='cuda:0'),\n",
              " tensor([-1.2696, 10.0506, -0.6471, -3.1237,  1.6073,  0.7869, -0.4902, -2.7832,\n",
              "          0.9329, -4.9913], device='cuda:0'),\n",
              " tensor([-4.0655,  8.5332, -3.8070,  2.2380,  0.9484, -1.7098,  1.2792, -0.2943,\n",
              "          2.2839, -3.9361], device='cuda:0'),\n",
              " tensor([-2.7891, 13.2503, -3.6596, -0.5088,  1.9910, -7.3115,  4.4801, -4.7148,\n",
              "         -1.5450,  1.5765], device='cuda:0'),\n",
              " tensor([-4.6860, 14.7602, -4.7258,  2.4746,  6.9357, -6.0144,  5.2244, -5.0933,\n",
              "         -2.2693, -5.3468], device='cuda:0'),\n",
              " tensor([-3.2803,  8.4756, -3.1411,  2.1211,  2.4733, -3.8288,  0.2226, -3.3887,\n",
              "          0.4557,  0.8075], device='cuda:0'),\n",
              " tensor([-4.6374, 11.0844, -2.7633, -0.3123,  0.4063, -1.1848,  1.3734, -3.1512,\n",
              "          3.5257, -2.4120], device='cuda:0'),\n",
              " tensor([-0.1256, 11.1476, -5.3973,  0.6895,  4.0449, -8.4282, -1.6773, -1.3111,\n",
              "          1.9044, -0.3449], device='cuda:0'),\n",
              " tensor([-4.1879, 10.4281, -0.7238,  2.4777,  1.8105, -2.7122,  0.3155, -0.3492,\n",
              "         -2.2943, -2.4073], device='cuda:0'),\n",
              " tensor([-8.3224, 22.2613, -7.0143,  0.3883,  9.2760, -7.1542,  5.6929, -8.3626,\n",
              "         -0.5337, -4.3899], device='cuda:0'),\n",
              " tensor([-5.3670, 13.4709, -3.0052, -2.6268,  0.9176, -6.0450,  4.5156, -7.0883,\n",
              "          5.7085,  0.2945], device='cuda:0'),\n",
              " tensor([-6.9245, 14.4446, -6.5046,  1.8907,  3.5415, -3.1419,  3.1752, -3.6525,\n",
              "         -0.8943, -0.1456], device='cuda:0'),\n",
              " tensor([-4.2758, 11.0646, -1.3434, -2.7166,  3.3058, -0.7793,  3.0705, -2.1361,\n",
              "         -2.8502, -1.4560], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGDeTXuVj6KY",
        "outputId": "9ffe6683-87c3-4a98-fd60-a65cc3c042ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # 如果你想feature的梯度能反向传播，那么去掉 detach（）\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "outputs_= []\n",
        "def hook(module, input, output):\n",
        "    outputs_.append(output)\n",
        "\n",
        "image_model.classifier[5].register_forward_hook(hook)\n",
        "# output = model()\n",
        "# print(activation['fc2'])\n",
        "features = defaultdict(list)\n",
        "with torch.no_grad():\n",
        "    for data in search_loader_realworld:\n",
        "        \n",
        "        data['label'] = torch.argmax(data['label'].squeeze(),dim = 1)\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "        outputs = image_model(images)\n",
        "        labels_sq = labels.tolist()\n",
        "        for i in range(outputs.shape[0]):\n",
        "          features[labels_sq[i]].append(outputs_[-1][i])\n",
        "          print(outputs_[-1][i].shape)\n",
        "          print(type(outputs_[-1][i]))\n",
        "\n",
        "    features_real = torch.empty((40*10,4096))\n",
        "    for i in range(len(features)):\n",
        "      features_real[i*40:(i+1)*40] = torch.stack(features[i])\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EZxDZdj5UrHO",
        "outputId": "b539f642-0720-49fe-ca49-236dd6a970e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([10])\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-43de622fcc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfeatures_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mfeatures_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4096) must match the existing size (10) at non-singleton dimension 1.  Target sizes: [40, 4096].  Tensor sizes: [40, 10]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs_[0])"
      ],
      "metadata": {
        "id": "6dDFxjxPXY_2",
        "outputId": "d7aec0b8-259f-44a0-f256-defd1123b592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 2.2342, 0.0000,  ..., 0.0000, 1.8571, 0.2678],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0695, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7503, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 2.2349, 0.0000],\n",
            "        [0.0000, 1.3405, 0.0000,  ..., 0.0000, 1.1144, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1802, 0.0000]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PhPqNfkIj9Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}