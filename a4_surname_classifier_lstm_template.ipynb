{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/a4_surname_classifier_lstm_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - Surname Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of the datafile files from **surname-data** directory (available in a4.zip) to your Google Drive in the location **A4/surname-data/surnames.csv**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'surname-data' \n",
        "##### 3. You are all set!\n"
      ],
      "metadata": {
        "id": "ppyxtiCuTt1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TL5j6-cag4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os, random\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "ziQ6pwj0TukR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0d5f0a-90af-4e24-a90d-ac37c7fee18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/drive/My Drive/A4/surname-data/surnames.csv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'"
      ],
      "metadata": {
        "id": "mOppodb0UwAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement this function if you want to transform the input text, e.g. normalizing case"
      ],
      "metadata": {
        "id": "ploK2x6RVgfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ],
      "metadata": {
        "id": "HHOK83jMW4U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)"
      ],
      "metadata": {
        "id": "kQpIf5xhXBNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, split, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cols = line.split(',')\n",
        "            s, surname, label = cols[0].strip(), cols[1].strip(), cols[2].strip()\n",
        "            if s==split:\n",
        "                surname = list(surname)\n",
        "                surname = [START]+surname+[END]\n",
        "                data.append(transform_text_sequence(surname))\n",
        "                labels.append(label)\n",
        "            for tok in surname:\n",
        "                vocab[tok]+=1\n",
        "\n",
        "    vocab = sorted(vocab.keys(), key=lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, set(labels), data, labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for doc, label in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(doc, vocab))\n",
        "            batch_y.append(one_hot_encode_label(label, label_set))\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "POKR921_U9KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "r8AcTwvLW_0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # number of epochs\n",
        "learning_rate = 0.1 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 10 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "XzFMKsiqXs8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "jKG3r0mUdx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "PGj77gPjdxGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e836653-5a6c-40df-d106-118b0252b525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmykwbnhmxsN",
        "outputId": "df02a8a3-5789-4dd0-dbd2-edf69ded84a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "uprkEithqHML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6601803-c78d-4e64-a1c3-7824344b76b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n",
        "\n",
        "##### Use the variables batch_size, hidden_size, embedding_size, dropout, epochs here."
      ],
      "metadata": {
        "id": "h06MbANsXtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "id": "i0a5SqKmW7Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c98216-01a6-47c2-e060-be16300d8eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n",
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 13s 12ms/step - loss: 2.9396 - accuracy: 0.1001\n",
            "3060/3060 [==============================] - 14s 4ms/step - loss: 2.9395 - accuracy: 0.1984\n",
            "Dev Loss: 2.9395248889923096 Dev Acc: 0.19836601614952087\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.9335 - accuracy: 0.2208\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9352 - accuracy: 0.3301\n",
            "Dev Loss: 2.935241460800171 Dev Acc: 0.33006536960601807\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.9268 - accuracy: 0.3663\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 2.9306 - accuracy: 0.4134\n",
            "Dev Loss: 2.9305849075317383 Dev Acc: 0.4133986830711365\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.9196 - accuracy: 0.4473\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9256 - accuracy: 0.4493\n",
            "Dev Loss: 2.925631523132324 Dev Acc: 0.4493463933467865\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.9120 - accuracy: 0.4685\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9204 - accuracy: 0.4637\n",
            "Dev Loss: 2.9204232692718506 Dev Acc: 0.4637254774570465\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.9040 - accuracy: 0.4697\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9149 - accuracy: 0.4650\n",
            "Dev Loss: 2.9149491786956787 Dev Acc: 0.4650326669216156\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.8957 - accuracy: 0.4698\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9092 - accuracy: 0.4650\n",
            "Dev Loss: 2.90923810005188 Dev Acc: 0.4650326669216156\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.8870 - accuracy: 0.4698\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.9033 - accuracy: 0.4650\n",
            "Dev Loss: 2.9032771587371826 Dev Acc: 0.4650326669216156\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 2.8780 - accuracy: 0.4698\n",
            "3060/3060 [==============================] - 11s 4ms/step - loss: 2.8971 - accuracy: 0.4650\n",
            "Dev Loss: 2.897064685821533 Dev Acc: 0.4650326669216156\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 2.8686 - accuracy: 0.4698\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 2.8906 - accuracy: 0.4650\n",
            "Dev Loss: 2.8905932903289795 Dev Acc: 0.4650326669216156\n"
          ]
        }
      ]
    }
  ]
}