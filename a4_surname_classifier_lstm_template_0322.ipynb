{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/a4_surname_classifier_lstm_template_0322.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - Surname Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of the datafile files from **surname-data** directory (available in a4.zip) to your Google Drive in the location **A4/surname-data/surnames.csv**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'surname-data' \n",
        "##### 3. You are all set!\n"
      ],
      "metadata": {
        "id": "ppyxtiCuTt1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TL5j6-cag4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os, random\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "ziQ6pwj0TukR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039bef51-868c-4156-8bf1-caf4f52c70c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/drive/My Drive/A4/surname-data/surnames.csv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'"
      ],
      "metadata": {
        "id": "mOppodb0UwAD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement this function if you want to transform the input text, e.g. normalizing case"
      ],
      "metadata": {
        "id": "ploK2x6RVgfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ],
      "metadata": {
        "id": "HHOK83jMW4U8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)"
      ],
      "metadata": {
        "id": "kQpIf5xhXBNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, split, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cols = line.split(',')\n",
        "            s, surname, label = cols[0].strip(), cols[1].strip(), cols[2].strip()\n",
        "            if s==split:\n",
        "                surname = list(surname)\n",
        "                surname = [START]+surname+[END]\n",
        "                data.append(transform_text_sequence(surname))\n",
        "                labels.append(label)\n",
        "            for tok in surname:\n",
        "                vocab[tok]+=1\n",
        "\n",
        "    vocab = sorted(vocab.keys(), key=lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, set(labels), data, labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for doc, label in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(doc, vocab))\n",
        "            batch_y.append(one_hot_encode_label(label, label_set))\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "POKR921_U9KY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "r8AcTwvLW_0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # number of epochs\n",
        "learning_rate = 0.1 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 10 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "XzFMKsiqXs8e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NLXWwAV6X_n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "jKG3r0mUdx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "PGj77gPjdxGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac806afa-8c5b-4a5f-d25e-9d6fdae92282"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmykwbnhmxsN",
        "outputId": "2a43e8b7-bee6-4a59-f7e6-6fd0ca0f14d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 14\n",
        "output_size = 19"
      ],
      "metadata": {
        "id": "C8IDuxN3newi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "uprkEithqHML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n",
        "\n",
        "##### Use the variables batch_size, hidden_size, embedding_size, dropout, epochs here."
      ],
      "metadata": {
        "id": "h06MbANsXtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "id": "i0a5SqKmW7Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3737ce-2185-49bb-c091-d7ac0b487472"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n",
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 15s 14ms/step - loss: 2.9589 - accuracy: 0.0188\n",
            "3060/3060 [==============================] - 15s 5ms/step - loss: 2.9451 - accuracy: 0.0304\n",
            "Dev Loss: 2.945093870162964 Dev Acc: 0.030392156913876534\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.9535 - accuracy: 0.0221\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9414 - accuracy: 0.0523\n",
            "Dev Loss: 2.9413952827453613 Dev Acc: 0.05228758230805397\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.9476 - accuracy: 0.0383\n",
            "3060/3060 [==============================] - 16s 5ms/step - loss: 2.9374 - accuracy: 0.1690\n",
            "Dev Loss: 2.9373579025268555 Dev Acc: 0.1689542531967163\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 2.9413 - accuracy: 0.0768\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9331 - accuracy: 0.3448\n",
            "Dev Loss: 2.9330618381500244 Dev Acc: 0.34477123618125916\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 2.9346 - accuracy: 0.1339\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9285 - accuracy: 0.4490\n",
            "Dev Loss: 2.9285316467285156 Dev Acc: 0.4490196108818054\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 2.9277 - accuracy: 0.2128\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9238 - accuracy: 0.4696\n",
            "Dev Loss: 2.9237844944000244 Dev Acc: 0.4696078300476074\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 2.9205 - accuracy: 0.3080\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9188 - accuracy: 0.4657\n",
            "Dev Loss: 2.91884708404541 Dev Acc: 0.46568626165390015\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 2.9130 - accuracy: 0.3872\n",
            "3060/3060 [==============================] - 15s 5ms/step - loss: 2.9137 - accuracy: 0.4650\n",
            "Dev Loss: 2.913701295852661 Dev Acc: 0.4650326669216156\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 2.9053 - accuracy: 0.4395\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9084 - accuracy: 0.4650\n",
            "Dev Loss: 2.908355712890625 Dev Acc: 0.4650326669216156\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 2.8973 - accuracy: 0.4665\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 2.9028 - accuracy: 0.4650\n",
            "Dev Loss: 2.9027915000915527 Dev Acc: 0.4650326669216156\n"
          ]
        }
      ]
    }
  ]
}