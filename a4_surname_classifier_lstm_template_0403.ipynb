{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_surname_classifier_lstm_template_0403.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - Surname Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of the datafile files from **surname-data** directory (available in a4.zip) to your Google Drive in the location **A4/surname-data/surnames.csv**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'surname-data' \n",
        "##### 3. You are all set!\n"
      ],
      "metadata": {
        "id": "ppyxtiCuTt1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TL5j6-cag4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os, random\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "ziQ6pwj0TukR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbe6419-b6e7-4304-b2e2-202bc589f9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/drive/My Drive/A4/surname-data/surnames.csv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'"
      ],
      "metadata": {
        "id": "mOppodb0UwAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement this function if you want to transform the input text, e.g. normalizing case"
      ],
      "metadata": {
        "id": "ploK2x6RVgfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ],
      "metadata": {
        "id": "HHOK83jMW4U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)"
      ],
      "metadata": {
        "id": "kQpIf5xhXBNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, split, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cols = line.split(',')\n",
        "            s, surname, label = cols[0].strip(), cols[1].strip(), cols[2].strip()\n",
        "            if s==split:\n",
        "                surname = list(surname)\n",
        "                surname = [START]+surname+[END]\n",
        "                data.append(transform_text_sequence(surname))\n",
        "                labels.append(label)\n",
        "            for tok in surname:\n",
        "                vocab[tok]+=1\n",
        "\n",
        "    vocab = sorted(vocab.keys(), key=lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, set(labels), data, labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for doc, label in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(doc, vocab))\n",
        "            batch_y.append(one_hot_encode_label(label, label_set))\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "POKR921_U9KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "r8AcTwvLW_0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 40 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "XzFMKsiqXs8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "jKG3r0mUdx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "PGj77gPjdxGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2868a3f3-9fd4-4a8a-b99c-359550f5b081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmykwbnhmxsN",
        "outputId": "1e49fe99-43fd-4f24-87e6-d408068a9da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "uprkEithqHML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d01e2c6-e465-41b8-c6f5-96c9dcda23b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n",
        "\n",
        "##### Use the variables batch_size, hidden_size, embedding_size, dropout, epochs here."
      ],
      "metadata": {
        "id": "h06MbANsXtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "id": "i0a5SqKmW7Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52468baa-90fd-4203-9d19-9db66dc555a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n",
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 9s 15ms/step - loss: 1.1918 - accuracy: 0.6531\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0513 - accuracy: 0.6804\n",
            "Dev Loss: 1.0513139963150024 Dev Acc: 0.6803921461105347\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7521 - accuracy: 0.7737\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9270 - accuracy: 0.7121\n",
            "Dev Loss: 0.9270397424697876 Dev Acc: 0.7120915055274963\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6304 - accuracy: 0.8067\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8986 - accuracy: 0.7314\n",
            "Dev Loss: 0.8985798954963684 Dev Acc: 0.7313725352287292\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5663 - accuracy: 0.8221\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8560 - accuracy: 0.7425\n",
            "Dev Loss: 0.8560300469398499 Dev Acc: 0.7424836754798889\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5351 - accuracy: 0.8339\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8485 - accuracy: 0.7490\n",
            "Dev Loss: 0.8485093712806702 Dev Acc: 0.7490196228027344\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5047 - accuracy: 0.8369\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9118 - accuracy: 0.7464\n",
            "Dev Loss: 0.9118237495422363 Dev Acc: 0.7464052438735962\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4914 - accuracy: 0.8425\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8927 - accuracy: 0.7487\n",
            "Dev Loss: 0.8927451372146606 Dev Acc: 0.7486928105354309\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4727 - accuracy: 0.8499\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9033 - accuracy: 0.7464\n",
            "Dev Loss: 0.9032886624336243 Dev Acc: 0.7464052438735962\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.4657 - accuracy: 0.8501\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9668 - accuracy: 0.7356\n",
            "Dev Loss: 0.9667800664901733 Dev Acc: 0.73562091588974\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4518 - accuracy: 0.8549\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9551 - accuracy: 0.7458\n",
            "Dev Loss: 0.9550936818122864 Dev Acc: 0.7457516193389893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_tzImNqn-tz",
        "outputId": "1494f350-635e-45a0-b81f-c03582eed364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 15\n",
            "300/300 [==============================] - 9s 16ms/step - loss: 1.2002 - accuracy: 0.6521\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0541 - accuracy: 0.6807\n",
            "Dev Loss: 1.0541378259658813 Dev Acc: 0.6807189583778381\n",
            "Epoch 2 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7617 - accuracy: 0.7699\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8359 - accuracy: 0.7582\n",
            "Dev Loss: 0.8359115123748779 Dev Acc: 0.758169949054718\n",
            "Epoch 3 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6213 - accuracy: 0.8061\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7668 - accuracy: 0.7739\n",
            "Dev Loss: 0.7668155431747437 Dev Acc: 0.7738562226295471\n",
            "Epoch 4 / 15\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.5526 - accuracy: 0.8276\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7777 - accuracy: 0.7742\n",
            "Dev Loss: 0.7777050733566284 Dev Acc: 0.7741830348968506\n",
            "Epoch 5 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5098 - accuracy: 0.8394\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7815 - accuracy: 0.7781\n",
            "Dev Loss: 0.7815014719963074 Dev Acc: 0.7781046032905579\n",
            "Epoch 6 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4768 - accuracy: 0.8463\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8095 - accuracy: 0.7732\n",
            "Dev Loss: 0.8094638586044312 Dev Acc: 0.7732025980949402\n",
            "Epoch 7 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4663 - accuracy: 0.8520\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8134 - accuracy: 0.7755\n",
            "Dev Loss: 0.8133890628814697 Dev Acc: 0.7754902243614197\n",
            "Epoch 8 / 15\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4434 - accuracy: 0.8549\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8257 - accuracy: 0.7703\n",
            "Dev Loss: 0.825677216053009 Dev Acc: 0.7702614665031433\n",
            "Epoch 9 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4276 - accuracy: 0.8595\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9068 - accuracy: 0.7582\n",
            "Dev Loss: 0.9067528247833252 Dev Acc: 0.758169949054718\n",
            "Epoch 10 / 15\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.4221 - accuracy: 0.8619\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9189 - accuracy: 0.7539\n",
            "Dev Loss: 0.9188896417617798 Dev Acc: 0.7539215683937073\n",
            "Epoch 11 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4050 - accuracy: 0.8692\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9048 - accuracy: 0.7680\n",
            "Dev Loss: 0.9048445224761963 Dev Acc: 0.7679738402366638\n",
            "Epoch 12 / 15\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.3938 - accuracy: 0.8705\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 0.8768 - accuracy: 0.7683\n",
            "Dev Loss: 0.876818835735321 Dev Acc: 0.7683006525039673\n",
            "Epoch 13 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3971 - accuracy: 0.8676\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9378 - accuracy: 0.7601\n",
            "Dev Loss: 0.93780916929245 Dev Acc: 0.7601307034492493\n",
            "Epoch 14 / 15\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3825 - accuracy: 0.8722\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0113 - accuracy: 0.7448\n",
            "Dev Loss: 1.0113099813461304 Dev Acc: 0.7447712421417236\n",
            "Epoch 15 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3803 - accuracy: 0.8747\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9450 - accuracy: 0.7523\n",
            "Dev Loss: 0.9450441598892212 Dev Acc: 0.7522875666618347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test evaluation \n",
        "loss, acc = classifier.evaluate(batch_generator(test_data, test_labels, vocab, labels),\n",
        "                                                  steps=len(test_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrdfNBw-3_yn",
        "outputId": "07f074f9-0998-4cae-fecc-824213b3f289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014/2014 [==============================] - 8s 4ms/step - loss: 0.8795 - accuracy: 0.7736\n",
            "Test Loss: 0.8794943690299988 Test Acc: 0.7735849022865295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 300\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d009c79f-5dba-4e9e-c682-23221495940d",
        "id": "hrOoDEyM0qfq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 11s 22ms/step - loss: 1.1492 - accuracy: 0.6661\n",
            "3060/3060 [==============================] - 14s 4ms/step - loss: 0.9762 - accuracy: 0.7118\n",
            "Dev Loss: 0.9762228727340698 Dev Acc: 0.7117646932601929\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7724 - accuracy: 0.7675\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8610 - accuracy: 0.7503\n",
            "Dev Loss: 0.8609779477119446 Dev Acc: 0.7503268122673035\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6821 - accuracy: 0.7922\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8259 - accuracy: 0.7516\n",
            "Dev Loss: 0.8259142637252808 Dev Acc: 0.7516340017318726\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6492 - accuracy: 0.8013\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8497 - accuracy: 0.7435\n",
            "Dev Loss: 0.8497036695480347 Dev Acc: 0.7434640526771545\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6247 - accuracy: 0.8047\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8202 - accuracy: 0.7647\n",
            "Dev Loss: 0.8201887607574463 Dev Acc: 0.7647058963775635\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6097 - accuracy: 0.8084\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8267 - accuracy: 0.7608\n",
            "Dev Loss: 0.8267378807067871 Dev Acc: 0.7607843279838562\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5928 - accuracy: 0.8164\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9876 - accuracy: 0.7163\n",
            "Dev Loss: 0.9876015782356262 Dev Acc: 0.7163398861885071\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5736 - accuracy: 0.8205\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8885 - accuracy: 0.7549\n",
            "Dev Loss: 0.8884518146514893 Dev Acc: 0.7549019455909729\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5701 - accuracy: 0.8207\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9329 - accuracy: 0.7359\n",
            "Dev Loss: 0.9328904747962952 Dev Acc: 0.7359477281570435\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5696 - accuracy: 0.8173\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9596 - accuracy: 0.7173\n",
            "Dev Loss: 0.9595970511436462 Dev Acc: 0.7173202633857727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 50\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b139a2e9-d6af-48cd-9745-cdcb23d1e594",
        "id": "-GB1a4ql0s0Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 9s 15ms/step - loss: 1.3056 - accuracy: 0.6226\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.1235 - accuracy: 0.6680\n",
            "Dev Loss: 1.1234782934188843 Dev Acc: 0.6679738759994507\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.8223 - accuracy: 0.7499\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8222 - accuracy: 0.7503\n",
            "Dev Loss: 0.8221744894981384 Dev Acc: 0.7503268122673035\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6455 - accuracy: 0.8019\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7469 - accuracy: 0.7833\n",
            "Dev Loss: 0.7468716502189636 Dev Acc: 0.7833333611488342\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.5580 - accuracy: 0.8236\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7562 - accuracy: 0.7761\n",
            "Dev Loss: 0.7561596035957336 Dev Acc: 0.7761437892913818\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.4990 - accuracy: 0.8425\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7775 - accuracy: 0.7778\n",
            "Dev Loss: 0.7774919867515564 Dev Acc: 0.7777777910232544\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4567 - accuracy: 0.8527\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8074 - accuracy: 0.7755\n",
            "Dev Loss: 0.8074012398719788 Dev Acc: 0.7754902243614197\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4266 - accuracy: 0.8636\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7913 - accuracy: 0.7791\n",
            "Dev Loss: 0.791312575340271 Dev Acc: 0.7790849804878235\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3996 - accuracy: 0.8693\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8509 - accuracy: 0.7729\n",
            "Dev Loss: 0.8509447574615479 Dev Acc: 0.7728758454322815\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3711 - accuracy: 0.8770\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8829 - accuracy: 0.7670\n",
            "Dev Loss: 0.8828542828559875 Dev Acc: 0.7669934630393982\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3678 - accuracy: 0.8791\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9410 - accuracy: 0.7650\n",
            "Dev Loss: 0.9409745931625366 Dev Acc: 0.7650327086448669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 30\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85990a5-99bd-44c2-80c3-c7960d2c35f0",
        "id": "vDPKb7Jolg7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 15\n",
            "300/300 [==============================] - 10s 14ms/step - loss: 1.2406 - accuracy: 0.6435\n",
            "3060/3060 [==============================] - 15s 5ms/step - loss: 1.1575 - accuracy: 0.6598\n",
            "Dev Loss: 1.1574530601501465 Dev Acc: 0.6598039269447327\n",
            "Epoch 2 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7988 - accuracy: 0.7602\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9292 - accuracy: 0.7304\n",
            "Dev Loss: 0.9291821718215942 Dev Acc: 0.7303921580314636\n",
            "Epoch 3 / 15\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6696 - accuracy: 0.7963\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8616 - accuracy: 0.7503\n",
            "Dev Loss: 0.8615626096725464 Dev Acc: 0.7503268122673035\n",
            "Epoch 4 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5977 - accuracy: 0.8153\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8531 - accuracy: 0.7588\n",
            "Dev Loss: 0.8531039953231812 Dev Acc: 0.7588235139846802\n",
            "Epoch 5 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5628 - accuracy: 0.8227\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8435 - accuracy: 0.7595\n",
            "Dev Loss: 0.8435284495353699 Dev Acc: 0.7594771385192871\n",
            "Epoch 6 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5286 - accuracy: 0.8345\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8440 - accuracy: 0.7676\n",
            "Dev Loss: 0.8440449833869934 Dev Acc: 0.7676470875740051\n",
            "Epoch 7 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5038 - accuracy: 0.8449\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8511 - accuracy: 0.7608\n",
            "Dev Loss: 0.8510923385620117 Dev Acc: 0.7607843279838562\n",
            "Epoch 8 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4905 - accuracy: 0.8435\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8396 - accuracy: 0.7739\n",
            "Dev Loss: 0.8395876288414001 Dev Acc: 0.7738562226295471\n",
            "Epoch 9 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4663 - accuracy: 0.8531\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8502 - accuracy: 0.7748\n",
            "Dev Loss: 0.8502305150032043 Dev Acc: 0.7748365998268127\n",
            "Epoch 10 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4530 - accuracy: 0.8531\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8694 - accuracy: 0.7647\n",
            "Dev Loss: 0.8694010376930237 Dev Acc: 0.7647058963775635\n",
            "Epoch 11 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4412 - accuracy: 0.8591\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9133 - accuracy: 0.7621\n",
            "Dev Loss: 0.9132871031761169 Dev Acc: 0.7620915174484253\n",
            "Epoch 12 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4354 - accuracy: 0.8609\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9042 - accuracy: 0.7654\n",
            "Dev Loss: 0.9041827321052551 Dev Acc: 0.7653594613075256\n",
            "Epoch 13 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4270 - accuracy: 0.8629\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9186 - accuracy: 0.7601\n",
            "Dev Loss: 0.9185845851898193 Dev Acc: 0.7601307034492493\n",
            "Epoch 14 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4369 - accuracy: 0.8584\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8840 - accuracy: 0.7722\n",
            "Dev Loss: 0.8839916586875916 Dev Acc: 0.7722222208976746\n",
            "Epoch 15 / 15\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4171 - accuracy: 0.8607\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9375 - accuracy: 0.7588\n",
            "Dev Loss: 0.937543511390686 Dev Acc: 0.7588235139846802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 20\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fTeSkGqrc5F",
        "outputId": "f7a8dfd7-fc6d-4bd2-a9d9-9f7a6fbed25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 48s 13ms/step - loss: 1.2937 - accuracy: 0.6281\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.1121 - accuracy: 0.6905\n",
            "Dev Loss: 1.1121164560317993 Dev Acc: 0.6905228495597839\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.8428 - accuracy: 0.7476\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9851 - accuracy: 0.7137\n",
            "Dev Loss: 0.9851279258728027 Dev Acc: 0.7137255072593689\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7014 - accuracy: 0.7885\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8797 - accuracy: 0.7310\n",
            "Dev Loss: 0.8797189593315125 Dev Acc: 0.7310457229614258\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6415 - accuracy: 0.8021\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8448 - accuracy: 0.7418\n",
            "Dev Loss: 0.8448327779769897 Dev Acc: 0.741830050945282\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.5987 - accuracy: 0.8127\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8373 - accuracy: 0.7471\n",
            "Dev Loss: 0.8373358845710754 Dev Acc: 0.7470588088035583\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5742 - accuracy: 0.8215\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8354 - accuracy: 0.7490\n",
            "Dev Loss: 0.8354207873344421 Dev Acc: 0.7490196228027344\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.5533 - accuracy: 0.8259\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8483 - accuracy: 0.7474\n",
            "Dev Loss: 0.8483107089996338 Dev Acc: 0.7473856210708618\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.5427 - accuracy: 0.8307\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8585 - accuracy: 0.7369\n",
            "Dev Loss: 0.8585145473480225 Dev Acc: 0.7369281053543091\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5268 - accuracy: 0.8330\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8604 - accuracy: 0.7513\n",
            "Dev Loss: 0.8604316115379333 Dev Acc: 0.7513071894645691\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5066 - accuracy: 0.8405\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8955 - accuracy: 0.7356\n",
            "Dev Loss: 0.8955186605453491 Dev Acc: 0.73562091588974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxwyLIzltLxe",
        "outputId": "81dd99d5-b340-4104-d235-f4d79ca6224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 7s 12ms/step - loss: 1.2098 - accuracy: 0.6473\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9868 - accuracy: 0.7160\n",
            "Dev Loss: 0.9867905378341675 Dev Acc: 0.7160130739212036\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7600 - accuracy: 0.7667\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8663 - accuracy: 0.7405\n",
            "Dev Loss: 0.8662500381469727 Dev Acc: 0.7405228614807129\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6225 - accuracy: 0.8065\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7619 - accuracy: 0.7807\n",
            "Dev Loss: 0.7619409561157227 Dev Acc: 0.780718982219696\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.5440 - accuracy: 0.8275\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7714 - accuracy: 0.7843\n",
            "Dev Loss: 0.7713531255722046 Dev Acc: 0.7843137383460999\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.4971 - accuracy: 0.8396\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7657 - accuracy: 0.7869\n",
            "Dev Loss: 0.7657081484794617 Dev Acc: 0.786928117275238\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.4622 - accuracy: 0.8496\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8102 - accuracy: 0.7742\n",
            "Dev Loss: 0.8102098107337952 Dev Acc: 0.7741830348968506\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4381 - accuracy: 0.8547\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7914 - accuracy: 0.7843\n",
            "Dev Loss: 0.7914313077926636 Dev Acc: 0.7843137383460999\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.4230 - accuracy: 0.8623\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7983 - accuracy: 0.7814\n",
            "Dev Loss: 0.7982662320137024 Dev Acc: 0.7813725471496582\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.4104 - accuracy: 0.8645\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8369 - accuracy: 0.7856\n",
            "Dev Loss: 0.836898922920227 Dev Acc: 0.785620927810669\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3899 - accuracy: 0.8703\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8652 - accuracy: 0.7840\n",
            "Dev Loss: 0.8652098178863525 Dev Acc: 0.7839869260787964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 10\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7GX_951tNmX",
        "outputId": "8a617750-738f-49ab-e2c3-ffd02eb1dbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 8s 12ms/step - loss: 1.3780 - accuracy: 0.6006\n",
            "3060/3060 [==============================] - 14s 4ms/step - loss: 1.2415 - accuracy: 0.6552\n",
            "Dev Loss: 1.2415069341659546 Dev Acc: 0.6552287340164185\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.9948 - accuracy: 0.7128\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 1.1990 - accuracy: 0.6503\n",
            "Dev Loss: 1.1990325450897217 Dev Acc: 0.6503267884254456\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.8772 - accuracy: 0.7392\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 1.1019 - accuracy: 0.6784\n",
            "Dev Loss: 1.1019408702850342 Dev Acc: 0.6784313917160034\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.8049 - accuracy: 0.7620\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 1.0540 - accuracy: 0.7000\n",
            "Dev Loss: 1.0539747476577759 Dev Acc: 0.699999988079071\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.7679 - accuracy: 0.7724\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0112 - accuracy: 0.7193\n",
            "Dev Loss: 1.011244535446167 Dev Acc: 0.719281017780304\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7430 - accuracy: 0.7769\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 1.0511 - accuracy: 0.7042\n",
            "Dev Loss: 1.0510516166687012 Dev Acc: 0.7042483687400818\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.7200 - accuracy: 0.7843\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.1069 - accuracy: 0.6971\n",
            "Dev Loss: 1.1069016456604004 Dev Acc: 0.6970587968826294\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6935 - accuracy: 0.7911\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.1270 - accuracy: 0.7062\n",
            "Dev Loss: 1.126950740814209 Dev Acc: 0.706209123134613\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6836 - accuracy: 0.7930\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 1.0814 - accuracy: 0.7059\n",
            "Dev Loss: 1.0814073085784912 Dev Acc: 0.7058823704719543\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6783 - accuracy: 0.7952\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0512 - accuracy: 0.7062\n",
            "Dev Loss: 1.0511503219604492 Dev Acc: 0.706209123134613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 60\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7960b5a2-685e-4572-8e58-47e9f4442255",
        "id": "ryluw_VtwhLf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "300/300 [==============================] - 8s 14ms/step - loss: 1.1983 - accuracy: 0.6509\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0103 - accuracy: 0.6984\n",
            "Dev Loss: 1.0102802515029907 Dev Acc: 0.6983659863471985\n",
            "Epoch 2 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.7450 - accuracy: 0.7713\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7424 - accuracy: 0.7765\n",
            "Dev Loss: 0.7423787713050842 Dev Acc: 0.7764706015586853\n",
            "Epoch 3 / 10\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.5971 - accuracy: 0.8113\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7140 - accuracy: 0.7833\n",
            "Dev Loss: 0.7139737010002136 Dev Acc: 0.7833333611488342\n",
            "Epoch 4 / 10\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.5155 - accuracy: 0.8371\n",
            "3060/3060 [==============================] - 11s 4ms/step - loss: 0.7314 - accuracy: 0.7886\n",
            "Dev Loss: 0.7313960790634155 Dev Acc: 0.7885621190071106\n",
            "Epoch 5 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4719 - accuracy: 0.8479\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7375 - accuracy: 0.7902\n",
            "Dev Loss: 0.7374964356422424 Dev Acc: 0.7901960611343384\n",
            "Epoch 6 / 10\n",
            "300/300 [==============================] - 3s 8ms/step - loss: 0.4431 - accuracy: 0.8565\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8051 - accuracy: 0.7797\n",
            "Dev Loss: 0.8051390647888184 Dev Acc: 0.7797385454177856\n",
            "Epoch 7 / 10\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.4128 - accuracy: 0.8673\n",
            "3060/3060 [==============================] - 11s 4ms/step - loss: 0.8218 - accuracy: 0.7810\n",
            "Dev Loss: 0.8218370079994202 Dev Acc: 0.7810457348823547\n",
            "Epoch 8 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3936 - accuracy: 0.8727\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8659 - accuracy: 0.7709\n",
            "Dev Loss: 0.865903913974762 Dev Acc: 0.7709150314331055\n",
            "Epoch 9 / 10\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3872 - accuracy: 0.8716\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8694 - accuracy: 0.7797\n",
            "Dev Loss: 0.8693708181381226 Dev Acc: 0.7797385454177856\n",
            "Epoch 10 / 10\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.3693 - accuracy: 0.8771\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8779 - accuracy: 0.7722\n",
            "Dev Loss: 0.8779428601264954 Dev Acc: 0.7722222208976746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxZMzTuB653Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "embedding_size = 300\n",
        "epochs = 8\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981e6af2-d974-4882-eea6-795f065f0b54",
        "id": "E4TtWY787C0E"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 8\n",
            "300/300 [==============================] - 9s 13ms/step - loss: 1.1493 - accuracy: 0.6713\n",
            "3060/3060 [==============================] - 16s 5ms/step - loss: 1.0146 - accuracy: 0.6840\n",
            "Dev Loss: 1.0146058797836304 Dev Acc: 0.6839869022369385\n",
            "Epoch 2 / 8\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.7529 - accuracy: 0.7735\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.9735 - accuracy: 0.6990\n",
            "Dev Loss: 0.9734741449356079 Dev Acc: 0.6990196108818054\n",
            "Epoch 3 / 8\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.6625 - accuracy: 0.7948\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 0.8952 - accuracy: 0.7314\n",
            "Dev Loss: 0.895188570022583 Dev Acc: 0.7313725352287292\n",
            "Epoch 4 / 8\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.6276 - accuracy: 0.8056\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8396 - accuracy: 0.7526\n",
            "Dev Loss: 0.8395711779594421 Dev Acc: 0.7526143789291382\n",
            "Epoch 5 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6010 - accuracy: 0.8110\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8439 - accuracy: 0.7503\n",
            "Dev Loss: 0.8439297676086426 Dev Acc: 0.7503268122673035\n",
            "Epoch 6 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5962 - accuracy: 0.8121\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8606 - accuracy: 0.7487\n",
            "Dev Loss: 0.860573947429657 Dev Acc: 0.7486928105354309\n",
            "Epoch 7 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5730 - accuracy: 0.8176\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8529 - accuracy: 0.7408\n",
            "Dev Loss: 0.8529453873634338 Dev Acc: 0.7408496737480164\n",
            "Epoch 8 / 8\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.5920 - accuracy: 0.8125\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 0.8676 - accuracy: 0.7422\n",
            "Dev Loss: 0.867561936378479 Dev Acc: 0.7421568632125854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "embedding_size = 150\n",
        "epochs = 8\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454488d8-1f75-4bbf-e933-94f5c433c226",
        "id": "ifsmp2kK7S20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 8\n",
            "300/300 [==============================] - 9s 16ms/step - loss: 1.1637 - accuracy: 0.6658\n",
            "3060/3060 [==============================] - 15s 5ms/step - loss: 0.9532 - accuracy: 0.7069\n",
            "Dev Loss: 0.9532268643379211 Dev Acc: 0.70686274766922\n",
            "Epoch 2 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7392 - accuracy: 0.7772\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7967 - accuracy: 0.7699\n",
            "Dev Loss: 0.7966799139976501 Dev Acc: 0.7699346542358398\n",
            "Epoch 3 / 8\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.6238 - accuracy: 0.8073\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7742 - accuracy: 0.7725\n",
            "Dev Loss: 0.7741916179656982 Dev Acc: 0.772549033164978\n",
            "Epoch 4 / 8\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.5547 - accuracy: 0.8275\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7495 - accuracy: 0.7775\n",
            "Dev Loss: 0.7494708895683289 Dev Acc: 0.7774509787559509\n",
            "Epoch 5 / 8\n",
            "300/300 [==============================] - 2s 8ms/step - loss: 0.5164 - accuracy: 0.8355\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7675 - accuracy: 0.7846\n",
            "Dev Loss: 0.7674776911735535 Dev Acc: 0.7846405506134033\n",
            "Epoch 6 / 8\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.4895 - accuracy: 0.8426\n",
            "3060/3060 [==============================] - 11s 4ms/step - loss: 0.7811 - accuracy: 0.7791\n",
            "Dev Loss: 0.7811238765716553 Dev Acc: 0.7790849804878235\n",
            "Epoch 7 / 8\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.4708 - accuracy: 0.8503\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8075 - accuracy: 0.7810\n",
            "Dev Loss: 0.8074911236763 Dev Acc: 0.7810457348823547\n",
            "Epoch 8 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4603 - accuracy: 0.8543\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.8152 - accuracy: 0.7873\n",
            "Dev Loss: 0.8151692748069763 Dev Acc: 0.7872549295425415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "embedding_size = 50\n",
        "epochs = 8\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ee7588-063b-4a7e-ce52-cd83b16b3d74",
        "id": "zvw4SlwE7brZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 8\n",
            "300/300 [==============================] - 7s 12ms/step - loss: 1.2412 - accuracy: 0.6376\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0005 - accuracy: 0.7052\n",
            "Dev Loss: 1.000499963760376 Dev Acc: 0.7052287459373474\n",
            "Epoch 2 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7849 - accuracy: 0.7636\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8405 - accuracy: 0.7487\n",
            "Dev Loss: 0.8405497074127197 Dev Acc: 0.7486928105354309\n",
            "Epoch 3 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6222 - accuracy: 0.8063\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7820 - accuracy: 0.7719\n",
            "Dev Loss: 0.7820004820823669 Dev Acc: 0.7718954086303711\n",
            "Epoch 4 / 8\n",
            "300/300 [==============================] - 3s 12ms/step - loss: 0.5354 - accuracy: 0.8327\n",
            "3060/3060 [==============================] - 17s 6ms/step - loss: 0.7696 - accuracy: 0.7690\n",
            "Dev Loss: 0.769624650478363 Dev Acc: 0.7689542770385742\n",
            "Epoch 5 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4764 - accuracy: 0.8483\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7729 - accuracy: 0.7797\n",
            "Dev Loss: 0.7728553414344788 Dev Acc: 0.7797385454177856\n",
            "Epoch 6 / 8\n",
            "300/300 [==============================] - 3s 9ms/step - loss: 0.4288 - accuracy: 0.8621\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7980 - accuracy: 0.7761\n",
            "Dev Loss: 0.7979559302330017 Dev Acc: 0.7761437892913818\n",
            "Epoch 7 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3965 - accuracy: 0.8717\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7971 - accuracy: 0.7846\n",
            "Dev Loss: 0.7971059083938599 Dev Acc: 0.7846405506134033\n",
            "Epoch 8 / 8\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.3720 - accuracy: 0.8791\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8424 - accuracy: 0.7846\n",
            "Dev Loss: 0.8424361944198608 Dev Acc: 0.7846405506134033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKab4xmR7edD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "embedding_size = 100\n",
        "epochs = 5\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565caed5-0cc9-4778-faaa-803d48ece1c6",
        "id": "33U2q9qH86bN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 5\n",
            "300/300 [==============================] - 7s 13ms/step - loss: 1.2088 - accuracy: 0.6522\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 1.0044 - accuracy: 0.7085\n",
            "Dev Loss: 1.004434585571289 Dev Acc: 0.7084967494010925\n",
            "Epoch 2 / 5\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.7601 - accuracy: 0.7698\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8303 - accuracy: 0.7539\n",
            "Dev Loss: 0.8302643299102783 Dev Acc: 0.7539215683937073\n",
            "Epoch 3 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6101 - accuracy: 0.8091\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7628 - accuracy: 0.7752\n",
            "Dev Loss: 0.7627842426300049 Dev Acc: 0.7751634120941162\n",
            "Epoch 4 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5320 - accuracy: 0.8330\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7540 - accuracy: 0.7866\n",
            "Dev Loss: 0.7540210485458374 Dev Acc: 0.7866013050079346\n",
            "Epoch 5 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.4947 - accuracy: 0.8425\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7622 - accuracy: 0.7820\n",
            "Dev Loss: 0.7621514797210693 Dev Acc: 0.7820261716842651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = classifier.evaluate(batch_generator(test_data, test_labels, vocab, labels),\n",
        "                                                  steps=len(test_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0rQL37j-cEM",
        "outputId": "7b2cb036-ab23-45b7-b05e-b417137e91cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014/2014 [==============================] - 9s 4ms/step - loss: 0.7450 - accuracy: 0.7835\n",
            "Test Loss: 0.7449893355369568 Test Acc: 0.7835153937339783\n"
          ]
        }
      ]
    }
  ]
}