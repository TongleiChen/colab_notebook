{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1yZqRoIHw-PK5A0yo84Mg3xmJAb7vhzzt",
      "authorship_tag": "ABX9TyO9eF+hEx3ZTNFIYL2urwe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/COSC576_project_early_Xmas_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4epz13JihL1",
        "outputId": "613c651a-6490-4f24-ce3d-2c144b6513d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW3VP4R2Kxl_"
      },
      "outputs": [],
      "source": [
        "# @title import library\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre-trained model reference: https://debuggercafe.com/transfer-learning-with-pytorch/"
      ],
      "metadata": {
        "id": "Go_BJ3sljNJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMAGE CLASSIFICATION\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, animal_category, image_size = 255, class_size = 100,transform = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sketch_dir (string): Directory to all the sketch images.\n",
        "            realworld_dir (string): Directory to all the real world images.\n",
        "            animal_category: list to fruit catogory\n",
        "            class_size: Num of images in each category\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.animal_category = animal_category\n",
        "        self.class_size = class_size\n",
        "        self.data_dict = dict(np.load(img_dir,allow_pickle=True))\n",
        "        self.image_size = image_size\n",
        "       \n",
        "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
        "                                                transforms.Resize((image_size,image_size)),\n",
        "                                                transforms.ToTensor(),])\n",
        "                                              # transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.class_size * len(self.animal_category)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        class_index = int(idx // self.class_size)\n",
        "        category =  self.animal_category[class_index]\n",
        "        category_idx = int(idx % self.class_size)\n",
        "        label = np.zeros((len(self.animal_category), 1))\n",
        "        label[class_index] = 1\n",
        "        \n",
        "#         label = class_index\n",
        "        image_ary =  self.data_dict[category][category_idx]\n",
        "        sample = {'image': image_ary, 'label': label}\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform_img(sample['image'])\n",
        "            sample['label'] = self.transform_label(sample['label'])\n",
        "        return sample\n",
        "        "
      ],
      "metadata": {
        "id": "idpV7W0Zia3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load image data\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "train_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "train_realworld = ImageDataset(train_realworld_dir, QURIES, image_size = 64, class_size = 500,transform = True)\n",
        "train_loader = DataLoader(train_realworld, batch_size=128, shuffle=True, pin_memory=True)\n",
        "\n",
        "test_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/test_images.npz\"\n",
        "test_realworld = ImageDataset(test_realworld_dir, QURIES, image_size = 64, class_size = 10,transform = True)\n",
        "test_loader_realworld = DataLoader(test_realworld, batch_size=128, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "h46CpgM0jeb8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = models.vgg16(pretrained=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model.to(device)\n",
        "print(image_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "divQilguj1IS",
        "outputId": "ba3851c6-4036-4e58-9eac-b56800d2449c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model.classifier[6].out_features = 10\n",
        "# ****************************\n",
        "# freeze convolution weights \n",
        "for param in image_model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "cbkHRz7ZkPNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "num_classes = 10\n",
        "num_epochs = 30\n",
        "batch_size = 64\n",
        "learning_rate = 0.01 #0.03\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(image_model.classifier.parameters(), lr=learning_rate, weight_decay = 0.007, momentum = 0.9)  \n",
        "LOAD = False\n",
        "start_epoch =0\n",
        "train_realworld_acc = []\n",
        "train_realworld_loss = []\n",
        "val_realworld_acc = []\n",
        "val_realworld_loss = []\n",
        "import gc\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(start_epoch,start_epoch+num_epochs):\n",
        "    for data in train_loader:  \n",
        "        # Move tensors to the configured device\n",
        "        images = data['image'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        correct_t = 0\n",
        "        total_t = 0\n",
        "        # Forward pass\n",
        "        image_model.train()\n",
        "        outputs = image_model(images)\n",
        "        loss = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "        _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "        total_t += labels.size(0)\n",
        "        correct_t +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        train_realworld_loss.append(loss.item())\n",
        "    train_realworld_acc.append(correct_t/total_t)\n",
        "    print ('Epoch [{}/{}], Training Loss: {:.4f}' \n",
        "                   .format(epoch+1,start_epoch+num_epochs, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_model.eval()\n",
        "        correct_v = 0\n",
        "        total_v = 0\n",
        "        for data in test_loader_realworld:\n",
        "            images = data['image'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "        \n",
        "\n",
        "            outputs = image_model(images)\n",
        "            loss_v = criterion(outputs, torch.argmax(labels.squeeze(),dim = 1))\n",
        "   \n",
        "            _, predicted = outputs.squeeze().topk(1, dim=1, largest=True, sorted=True)\n",
        "        \n",
        "            total_v += labels.size(0)\n",
        "            correct_v +=  (predicted.squeeze() == torch.argmax(labels.squeeze(),dim = 1)).sum().item()\n",
        "            \n",
        "            del images, labels, outputs\n",
        "        val_realworld_loss.append(loss_v.item())\n",
        "        val_realworld_acc.append(correct_v/total_v)\n",
        "        print(f\"Validation accuracy: {(correct_v/total_v):.3f}\")\n",
        "       \n",
        "print(f\"train_acc:{(np.mean([v for v in train_realworld_acc])):.3f},val_acc:{(np.mean([v for v in val_realworld_acc])):.3f}\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jwo605SkgiI",
        "outputId": "cc83f9f3-c306-4c0b-d4f6-ff1e6d09a50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Training Loss: 0.7527\n",
            "Validation accuracy: 0.580\n",
            "Epoch [2/30], Training Loss: 0.8536\n",
            "Validation accuracy: 0.630\n",
            "Epoch [3/30], Training Loss: 0.9809\n",
            "Validation accuracy: 0.680\n",
            "Epoch [4/30], Training Loss: 1.2078\n",
            "Validation accuracy: 0.630\n",
            "Epoch [5/30], Training Loss: 0.4609\n",
            "Validation accuracy: 0.620\n",
            "Epoch [6/30], Training Loss: 0.7744\n",
            "Validation accuracy: 0.680\n",
            "Epoch [7/30], Training Loss: 0.6792\n",
            "Validation accuracy: 0.630\n",
            "Epoch [8/30], Training Loss: 1.7265\n",
            "Validation accuracy: 0.700\n",
            "Epoch [9/30], Training Loss: 0.1999\n",
            "Validation accuracy: 0.700\n",
            "Epoch [10/30], Training Loss: 0.1726\n",
            "Validation accuracy: 0.640\n",
            "Epoch [11/30], Training Loss: 0.2447\n",
            "Validation accuracy: 0.690\n",
            "Epoch [12/30], Training Loss: 0.6001\n",
            "Validation accuracy: 0.680\n",
            "Epoch [13/30], Training Loss: 0.3548\n",
            "Validation accuracy: 0.660\n",
            "Epoch [14/30], Training Loss: 0.2136\n",
            "Validation accuracy: 0.690\n",
            "Epoch [15/30], Training Loss: 0.5625\n",
            "Validation accuracy: 0.690\n",
            "Epoch [16/30], Training Loss: 0.2110\n",
            "Validation accuracy: 0.680\n",
            "Epoch [17/30], Training Loss: 0.1855\n",
            "Validation accuracy: 0.670\n",
            "Epoch [18/30], Training Loss: 0.2786\n",
            "Validation accuracy: 0.670\n",
            "Epoch [19/30], Training Loss: 0.0618\n",
            "Validation accuracy: 0.680\n",
            "Epoch [20/30], Training Loss: 0.0433\n",
            "Validation accuracy: 0.720\n",
            "Epoch [21/30], Training Loss: 0.0449\n",
            "Validation accuracy: 0.690\n",
            "Epoch [22/30], Training Loss: 0.1241\n",
            "Validation accuracy: 0.700\n",
            "Epoch [23/30], Training Loss: 0.3089\n",
            "Validation accuracy: 0.710\n",
            "Epoch [24/30], Training Loss: 0.0947\n",
            "Validation accuracy: 0.720\n",
            "Epoch [25/30], Training Loss: 0.0916\n",
            "Validation accuracy: 0.660\n",
            "Epoch [26/30], Training Loss: 0.1627\n",
            "Validation accuracy: 0.670\n",
            "Epoch [27/30], Training Loss: 0.1806\n",
            "Validation accuracy: 0.660\n",
            "Epoch [28/30], Training Loss: 0.2636\n",
            "Validation accuracy: 0.670\n",
            "Epoch [29/30], Training Loss: 0.1474\n",
            "Validation accuracy: 0.660\n",
            "Epoch [30/30], Training Loss: 0.0062\n",
            "Validation accuracy: 0.720\n",
            "train_acc:0.863,val_acc:0.673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save image data model\n",
        "image_model_path = \"/content/drive/MyDrive/kaggle/imagenet/vgg16_pretrained_realworld_epoch30_loss_lr_001.pth\"\n",
        "\n",
        "\n",
        "torch.save(image_model,image_model_path)"
      ],
      "metadata": {
        "id": "Y_9E4pPezYAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SKETCH CLASSIFICATION\n",
        "categories = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]\n",
        "# categories = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "label_dict = {0:'bear',1:'camel',2:'cat', 3:'dog', 4:'elephant',\n",
        "                      5:'frog',6:'lion', 7:'panda', 8:'rabbit', 9:'squirrel'}\n",
        "\n",
        "# load data for each category\n",
        "classes = {}\n",
        "for category in categories:\n",
        "    # ctl's path\n",
        "    data = pd.read_csv(\"./drive/MyDrive/kaggle/sketch/\" + category + \".csv\")\n",
        "    # lzx's path\n",
        "    # data = pd.read_csv(\"./drive/MyDrive/sketch/\" + category + \".csv\")\n",
        "    classes[category] = data"
      ],
      "metadata": {
        "id": "erI_RrcGzohR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image manipulation utilities: \n",
        "\n",
        "def convert_to_PIL(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert from drawing to PIL image.\n",
        "    INPUT:\n",
        "        drawing - drawing from 'drawing' column\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        pil_img - (PIL Image) image\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize empty (white) PIL image\n",
        "    pil_img = Image.new('RGB', (width, height), 'white')\n",
        "    pixels = pil_img.load()\n",
        "            \n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    \n",
        "    # draw strokes as lines\n",
        "    for x,y in drawing:\n",
        "        for i in range(1, len(x)):\n",
        "            draw.line((x[i-1], y[i-1], x[i], y[i]), fill=0)\n",
        "        \n",
        "    return pil_img\n",
        "\n",
        "\n",
        "def convert_to_np_raw(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        drawing - drawing in initial format\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        img - drawing converted to the numpy array (28 X 28)\n",
        "    \"\"\"\n",
        "    # initialize empty numpy array\n",
        "    img = np.zeros((28, 28))\n",
        "    \n",
        "    # create a PIL image out of drawing\n",
        "    pil_img = convert_to_PIL(drawing)\n",
        "    \n",
        "    #resize to 28,28\n",
        "    pil_img.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    pil_img = pil_img.convert('RGB')\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    # fill in numpy array with pixel values\n",
        "    for i in range(0, 28):\n",
        "        for j in range(0, 28):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "    \n",
        "    return img\n",
        "\n",
        "def convert_to_np(pil_img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert PIL Image to numpy array.\n",
        "    INPUT:\n",
        "        pil_img - (PIL Image) image to be converted\n",
        "    OUTPUT:\n",
        "        img - (numpy array) converted image with shape (width, height)\n",
        "    \"\"\"\n",
        "    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "    img = np.zeros((width, height))\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    for i in range(0, width):\n",
        "      for j in range(0, height):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def view_image(img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to view numpy image with matplotlib.\n",
        "    The function saves the image as png.\n",
        "    INPUT:\n",
        "        img - (numpy array) image from train dataset with size (1, 784)\n",
        "    OUTPUT:\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,9))\n",
        "    ax.imshow(img.reshape(width, height).squeeze())\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "def crop_image(image):\n",
        "    \"\"\"\n",
        "    Crops image (crops out white spaces).\n",
        "    INPUT:\n",
        "        image - PIL image of original size to be cropped\n",
        "    OUTPUT:\n",
        "        cropped_image - PIL image cropped to the center  and resized to (28, 28)\n",
        "    \"\"\"\n",
        "    cropped_image = image\n",
        "\n",
        "    # get image size\n",
        "    width, height = cropped_image.size\n",
        "\n",
        "    # get image pixels\n",
        "    pixels = cropped_image.load()\n",
        "\n",
        "    image_strokes_rows = []\n",
        "    image_strokes_cols = []\n",
        "\n",
        "    # run through the image\n",
        "    for i in range(0, width):\n",
        "        for j in range(0, height):\n",
        "            # save coordinates of the image\n",
        "            if (pixels[i,j][0] > 0):\n",
        "                image_strokes_cols.append(i)\n",
        "                image_strokes_rows.append(j)\n",
        "\n",
        "    # if image is not empty then crop to contents of the image\n",
        "    if (len(image_strokes_rows)) > 0:\n",
        "        # find the box for image\n",
        "        row_min = np.array(image_strokes_rows).min()\n",
        "        row_max = np.array(image_strokes_rows).max()\n",
        "        col_min = np.array(image_strokes_cols).min()\n",
        "        col_max = np.array(image_strokes_cols).max()\n",
        "\n",
        "        # find the box for cropping\n",
        "        margin = min(row_min, height - row_max, col_min, width - col_max)\n",
        "\n",
        "        # crop image\n",
        "        border = (col_min, row_min, width - col_max, height - row_max)\n",
        "        cropped_image = ImageOps.crop(cropped_image, border)\n",
        "\n",
        "    # get cropped image size\n",
        "    width_cropped, height_cropped = cropped_image.size\n",
        "\n",
        "    # create square resulting image to paste cropped image into the center\n",
        "    dst_im = Image.new(\"RGBA\", (max(width_cropped, height_cropped), max(width_cropped, height_cropped)), \"white\")\n",
        "    offset = ((max(width_cropped, height_cropped) - width_cropped) // 2, (max(width_cropped, height_cropped) - height_cropped) // 2)\n",
        "    # paste to the center of a resulting image\n",
        "    dst_im.paste(cropped_image, offset)\n",
        "\n",
        "    #resize to 28,28\n",
        "    dst_im.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    return dst_im\n",
        "def normalize(arr):\n",
        "    \"\"\"\n",
        "    Function performs the linear normalizarion of the array.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
        "    INPUT:\n",
        "        arr - orginal numpy array\n",
        "    OUTPUT:\n",
        "        arr - normalized numpy array\n",
        "    \"\"\"\n",
        "    arr = arr.astype('float')\n",
        "    # Do not touch the alpha channel\n",
        "    for i in range(3):\n",
        "        minval = arr[...,i].min()\n",
        "        maxval = arr[...,i].max()\n",
        "        if minval != maxval:\n",
        "            arr[...,i] -= minval\n",
        "            arr[...,i] *= (255.0/(maxval-minval))\n",
        "    return arr\n",
        "\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Function performs the normalization of the image.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    INPUT:\n",
        "        image - PIL image to be normalized\n",
        "    OUTPUT:\n",
        "        new_img - PIL image normalized\n",
        "    \"\"\"\n",
        "    arr = np.array(image)\n",
        "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGBA')\n",
        "    return new_img\n",
        "\n",
        "def rotate_image(src_im, angle = 45, size = (28,28)):\n",
        "    \"\"\"\n",
        "    Function to rotate PIL Image file\n",
        "    INPUT:\n",
        "        src_im - (PIL Image) 28x28 image to be rotated\n",
        "        angle - angle to rotate the image\n",
        "        size - (tuple) size of the output image\n",
        "    OUTPUT:\n",
        "    dst_im - (PIL Image) rotated image\n",
        "    \"\"\"\n",
        "    dst_im = Image.new(\"RGBA\", size, \"white\")\n",
        "    src_im = src_im.convert('RGBA')\n",
        "\n",
        "    rot = src_im.rotate(angle)\n",
        "    dst_im.paste(rot, (0, 0), rot)\n",
        "\n",
        "    return dst_im\n",
        "def flip_image(src_im):\n",
        "    \"\"\"\n",
        "    Function to flip a PIL Image file.\n",
        "    INPUT:\n",
        "        scr_im - (PIL Image) image to be flipped\n",
        "    OUTPUT:\n",
        "        dst_im - (PIL Image) flipped image\n",
        "    \"\"\"\n",
        "    dst_im = src_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return dst_im\n"
      ],
      "metadata": {
        "id": "cQmbKP7d7VVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import library\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "irW_YxbZKasJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shrinking the images\n",
        "\n",
        "# create the dictionary containing classes names as keys and images as values\n",
        "values_dict = {}\n",
        "for category in categories:\n",
        "    data = classes[category][:3000]\n",
        "    values = [convert_to_np_raw(ast.literal_eval(img)).reshape(1, 784) for img in data['drawing'].values]\n",
        "    values_dict[category] = values\n",
        "    \n",
        "# concatenate to create X (values) and y (labels) datasets\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for key, value in label_dict.items():\n",
        "    data_i = values_dict[value]\n",
        "    Xi = np.concatenate(data_i, axis = 0)\n",
        "    yi = np.full((len(Xi), 1), key).ravel()\n",
        "    \n",
        "    X.append(Xi)\n",
        "    y.append(yi)\n",
        "X = np.concatenate(X, axis = 0)\n",
        "y = np.concatenate(y, axis = 0)"
      ],
      "metadata": {
        "id": "qkJDAgCy65Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import library\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "SvCXyNEv6ovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## sketch based on LeNet \n",
        "def resize(x, kernel_size, dilation, stride, padding):\n",
        "    x = int(1 + (x + 2*padding - dilation * (kernel_size - 1) - 1)/stride)\n",
        "    return x\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, specs, dropout=0.0):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.specs = specs\n",
        "        H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding = specs\n",
        "        pooling = 2\n",
        "        #pooling = 1 # skips pooling\n",
        "        stride = 1\n",
        "        dilation = 1\n",
        "\n",
        "        #self.pool = pool = nn.AvgPool2d(pooling)\n",
        "        self.pool = pool = nn.MaxPool2d(pooling)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(C0, C1, kernel_size, padding=padding)\n",
        "        # in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2\n",
        "        #self.conv1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        H = resize(H, kernel_size, dilation, stride, padding)\n",
        "        W = resize(W, kernel_size, dilation, stride, padding)\n",
        "\n",
        "        H = resize(H, pooling, dilation, pooling, 0)\n",
        "        W = resize(W, pooling, dilation, pooling, 0)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(C1, C2, kernel_size, padding=padding)\n",
        "\n",
        "        H = resize(H, kernel_size, dilation, stride, padding)\n",
        "        W = resize(W, kernel_size, dilation, stride, padding)\n",
        "\n",
        "        H = resize(H, pooling, dilation, pooling, 0)\n",
        "        W = resize(W, pooling, dilation, pooling, 0)\n",
        "\n",
        "        #print(H, W)\n",
        "        size = H * W * C2\n",
        "\n",
        "        self.linear0 = nn.Linear(size, F1)\n",
        "        self.linear1 = nn.Linear(F1, F2)\n",
        "        self.linear2 = nn.Linear(F2, nDigits)\n",
        "\n",
        "        self.non_linear = nn.LeakyReLU(negative_slope=0.01)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        for p in self.parameters(): # optionally apply different randomization\n",
        "            if p.dim() > 1:\n",
        "                nn.init.kaiming_normal_(p)\n",
        "                pass\n",
        "\n",
        "    def forward(self, prev):\n",
        "        nBatch = len(prev)\n",
        "        #print(prev.shape)\n",
        "        prev = self.conv1(prev)#\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "        prev = self.pool(prev)\n",
        "\n",
        "        prev = self.conv2(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "        prev = self.pool(prev)\n",
        "\n",
        "        prev = prev.view(nBatch, -1)\n",
        "        #print(prev.shape)\n",
        "\n",
        "        prev = self.linear0(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "\n",
        "        prev = self.linear1(prev)\n",
        "        prev = self.non_linear(prev)\n",
        "        prev = self.dropout(prev)\n",
        "\n",
        "        prev = self.linear2(prev)\n",
        "\n",
        "        return prev\n",
        "##\n",
        "def build_model(input_size, output_size, hidden_sizes, dropout = 0.0):\n",
        "    '''\n",
        "    Function creates deep learning model based on parameters passed.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - layer sizes\n",
        "        dropout - dropout (probability of keeping a node)\n",
        "\n",
        "    OUTPUT:\n",
        "        model - deep learning model\n",
        "    '''\n",
        "\n",
        "    # Build a feed-forward network\n",
        "    #modelCNN = CNNModel()\n",
        "\n",
        "    H=28    # don't change -- actual images are 28x28, not 32x32 -- H = height of image\n",
        "    W=28    # don't change -- actual images are 28x28, not 32x32 -- W = width of image\n",
        "    C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "    C1=6\n",
        "    C2=16\n",
        "    kernel_size=5\n",
        "    F1 = 120\n",
        "    F2 = 84\n",
        "    nDigits=10    # don't change -- # outputs -- 10 digits\n",
        "    padding=2\n",
        "    specs = [H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding]\n",
        "    modelLeNet = LeNet(specs, dropout=0.1)\n",
        "    \n",
        "    return modelLeNet\n",
        "\n",
        "def shuffle(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Function which shuffles training dataset.\n",
        "    INPUT:\n",
        "        X_train - (tensor) training set\n",
        "        y_train - (tensor) labels for training set\n",
        "\n",
        "    OUTPUT:\n",
        "        X_train_shuffled - (tensor) shuffled training set\n",
        "        y_train_shuffled - (tensor) shuffled labels for training set\n",
        "        \"\"\"\n",
        "    X_train_shuffled = X_train.numpy()\n",
        "    y_train_shuffled = y_train.numpy().reshape((X_train.shape[0], 1))\n",
        "\n",
        "    permutation = list(np.random.permutation(X_train.shape[0]))\n",
        "    X_train_shuffled = X_train_shuffled[permutation, :]\n",
        "    y_train_shuffled = y_train_shuffled[permutation, :].reshape((X_train.shape[0], 1))\n",
        "\n",
        "    X_train_shuffled = torch.from_numpy(X_train_shuffled).float()\n",
        "    y_train_shuffled = torch.from_numpy(y_train_shuffled).long()\n",
        "\n",
        "    return X_train_shuffled, y_train_shuffled\n",
        "\n",
        "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function which fits the model.\n",
        "    INPUT:\n",
        "        model - pytorch model to fit\n",
        "        X_train - (tensor) train dataset\n",
        "        y_train - (tensor) train dataset labels\n",
        "        epochs - number of epochs\n",
        "        n_chunks - number of chunks to cplit the dataset\n",
        "        learning_rate - learning rate value\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\".format(epochs = epochs, lr = learning_rate))\n",
        "    \n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if (optimizer == 'SGD'):\n",
        "      optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "\n",
        "    print_every = 10\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "        images = torch.chunk(X_train, n_chunks)\n",
        "        labels = torch.chunk(y_train, n_chunks)\n",
        "\n",
        "        for i in range(n_chunks):\n",
        "            steps += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward passes\n",
        "            output = model.forward(images[i])\n",
        "            loss = criterion(output, labels[i].squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        if epochs % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            running_loss = 0\n",
        "                            \n",
        "def view_classify(img, ps):\n",
        "    \"\"\"\n",
        "    Function for viewing an image and it's predicted classes\n",
        "    with matplotlib.\n",
        "\n",
        "    INPUT:\n",
        "        img - (tensor) image file\n",
        "        ps - (tensor) predicted probabilities for each class\n",
        "    \"\"\"\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels([\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_model(model, img):\n",
        "    \"\"\"\n",
        "    Function creates test view of the model's prediction for image.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        img - (tensor) image from the dataset\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert 2D image to 1D vector\n",
        "    img = img.resize_(1, 784)\n",
        "\n",
        "    ps = get_preds(model, img)\n",
        "    view_classify(img.resize_(1, 28, 28), ps)\n",
        "\n",
        "\n",
        "def get_preds(model, input):\n",
        "    \"\"\"\n",
        "    Function to get predicted probabilities from the model for each class.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        input - (tensor) input vector\n",
        "\n",
        "    OUTPUT:\n",
        "        ps - (tensor) vector of predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Turn off gradients to speed up this part\n",
        "    with torch.no_grad():\n",
        "        logits = model.forward(input)\n",
        "    ps = F.softmax(logits, dim=1)\n",
        "    return ps\n",
        "\n",
        "def get_labels(pred):\n",
        "    \"\"\"\n",
        "        Function to get the vector of predicted labels for the images in\n",
        "        the dataset.\n",
        "\n",
        "        INPUT:\n",
        "            pred - (tensor) vector of predictions (probabilities for each class)\n",
        "        OUTPUT:\n",
        "            pred_labels - (numpy) array of predicted classes for each vector\n",
        "    \"\"\"\n",
        "\n",
        "    pred_np = pred.numpy()\n",
        "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
        "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
        "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
        "\n",
        "    return pred_labels\n",
        "def evaluate_model(model, train, y_train, test, y_test):\n",
        "    \"\"\"\n",
        "    Function to print out train and test accuracy of the model.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        train - (tensor) train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "\n",
        "    OUTPUT:\n",
        "        accuracy_train - accuracy on train dataset\n",
        "        accuracy_test - accuracy on test dataset\n",
        "    \"\"\"\n",
        "    train_pred = get_preds(model, train)\n",
        "    train_pred_labels = get_labels(train_pred)\n",
        "\n",
        "    test_pred = get_preds(model, test)\n",
        "    test_pred_labels = get_labels(test_pred)\n",
        "\n",
        "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
        "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
        "\n",
        "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
        "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
        "\n",
        "    return accuracy_train, accuracy_test\n",
        "\n",
        "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function to plot learning curve depending on the number of epochs.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - model parameters\n",
        "        train - (tensor) train dataset\n",
        "        labels - (tensor) labels for train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "        learning_rate - learning rate hyperparameter\n",
        "        weight_decay - weight decay (regularization)\n",
        "        dropout - dropout for hidden layer\n",
        "        n_chunks - the number of minibatches to train the model\n",
        "        optimizer - optimizer to be used for training (SGD or Adam)\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for epochs in np.arange(10, 60, 10):\n",
        "        # create model\n",
        "        sketch_model = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
        "\n",
        "        # fit model\n",
        "        fit_model(sketch_model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = 'SGD')\n",
        "        # get accuracy\n",
        "        accuracy_train, accuracy_test = evaluate_model(sketch_model, train, y_train, test, y_test)\n",
        "\n",
        "        train_acc.append(accuracy_train)\n",
        "        test_acc.append(accuracy_test)\n",
        "\n",
        "    \n",
        "    return train_acc, test_acc, sketch_model"
      ],
      "metadata": {
        "id": "gw-1MAPQ52wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## My trial:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch import optim,nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "###############################################################################\n",
        "import time, os, sys, random, datetime\n",
        "use_cuda = torch.cuda.is_available()\n",
        "L2_lambda = 0.001\n",
        "global nEpochs\n",
        "nEpochs = 100\n",
        "# nEpochs = 2\n",
        "\n",
        "log_interval = 100\n",
        "#log_interval = 10\n",
        "learning_rate = 0.0005  # default\n",
        "\n",
        "class DatasetFromCSV(Dataset):\n",
        "    def __init__(self,datas,labels,height,width,transforms=None):\n",
        "        #self.data = pd.read_csv(csv_path)\n",
        "        self.data = datas\n",
        "        self.labels = labels\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # 读取所有像素值，并将 1D array ([784]) reshape 成为 2D array ([28,28])\n",
        "        img_as_np = np.asarray(self.data[index][:]).reshape(28, 28).astype(float)\n",
        "        # 把 numpy array 格式的图像转换成灰度 PIL image\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        # img_as_img = img_as_img.convert('L')\n",
        "        # 将图像转换成 tensor\n",
        "        if self.transforms is not None:\n",
        "            img_as_tensor = self.transforms(img_as_img)\n",
        "            # 返回图像及其 label\n",
        "        return (img_as_tensor, single_image_label)\n",
        " \n",
        "    def __len__(self):\n",
        "        #datacopy=self.data.copy().tolist()\n",
        "        #return len(datacopy.index)\n",
        "        return len(self.data)#这里是不是有错？？\n",
        " \n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "      return arr.cuda()\n",
        "    return arr\n",
        "def train_LeNet(model, train_loader, test_loader):\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # define the loss functions\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    # choose an optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "    start = time.time()\n",
        "    w_decay = 0.95 # smoothing factor for reporting results\n",
        "    for e in range(nEpochs):\n",
        "        total_train_images = 0\n",
        "        total_train_loss = 0\n",
        "        train_images = 0\n",
        "        train_loss = 0\n",
        "        w_images = 0\n",
        "        w_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = cuda(data), cuda(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_images += len(data)\n",
        "            train_loss += loss.data.item()\n",
        "\n",
        "            if train_images > log_interval:\n",
        "                total_train_images += train_images\n",
        "                total_train_loss += train_loss\n",
        "                if w_images == 0:\n",
        "                    w_loss = train_loss\n",
        "                    w_images = train_images\n",
        "                else:\n",
        "                    w_images = w_decay * w_images + train_images\n",
        "                    w_loss = w_decay * w_loss + train_loss\n",
        "                #log_message(None, \"%3d %8d %8.3f %8.3f     %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "                # print(\"%3d %8d %8.3f %8.3f     %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "\n",
        "                train_images = 0\n",
        "                train_loss = 0\n",
        "            #     #break\n",
        "\n",
        "        test_images = 0\n",
        "        test_loss = 0\n",
        "        nCorrect = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                data, target = cuda(data), cuda(target)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                test_images += len(data)\n",
        "                test_loss += loss.data.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max value\n",
        "                nCorrect += pred.eq(target.view_as(pred)).sum().item() # count correct items\n",
        "\n",
        "        #log_message(log_file, \"%3d %8d %8.3f %8.3f %8.3f %8.1f%%     %6.1f\" % (e, (e+1)*total_train_images, total_train_loss/total_train_images, w_loss/w_images, test_loss/test_images, 100*nCorrect/test_images, (time.time()-start)))\n",
        "\n",
        "        print(\"%3d %8d %8.1f%% \" % (e, test_loss/test_images, 100*nCorrect/test_images))\n",
        "\n",
        "    return model\n",
        "\n",
        "batch_size = 256\n",
        "transform1 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "transform2 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "# Split dataset into train/test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
        "train_data = DatasetFromCSV(X_train,y_train,28,28,transform1)\n",
        "test_data = DatasetFromCSV(X_test,y_test,28,28,transform2)\n",
        " \n",
        "train_loader = DataLoader(train_data,batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size)\n",
        " \n",
        "#img,lab = next(iter(train_loader))\n",
        "#print(img.shape)\n",
        "\n",
        "H=28    # don't change -- actual images are 28x28, not 32x32 -- H = height of image\n",
        "W=28    # don't change -- actual images are 28x28, not 32x32 -- W = width of image\n",
        "C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "C1=6\n",
        "C2=16\n",
        "kernel_size=5\n",
        "F1 = 120\n",
        "F2 = 84\n",
        "nDigits=10    # don't change -- # outputs -- 10 digits\n",
        "padding=0\n",
        "\n",
        "#C1 = 20\n",
        "#padding = 1\n",
        "\n",
        "specs = [H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding]\n",
        "model = LeNet(specs, dropout=0.1)\n",
        "print(model)\n",
        "model = train_LeNet(model,train_loader,test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "PyEU87M35XHU",
        "outputId": "ff426a4e-82ad-40af-983b-3bb937412edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8f42a5f3bcea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Split dataset into train/test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetFromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3nTtuox9wUw",
        "outputId": "58e76c2f-2bd1-4296-afab-4d83908ae3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 9 9 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load image data model\n",
        "image_model_path = \"/content/drive/MyDrive/kaggle/imagenet/vgg16_pretrained_realworld_epoch30_loss_lr_001.pth\"\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model=torch.load(image_model_path)\n",
        "image_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKbHzGnIQmFB",
        "outputId": "af099e26-d9c0-41f9-aaf2-05d070ae1514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get features\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "\n",
        "search_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "search_realworld = ImageDataset(search_realworld_dir, QURIES, image_size = 64, class_size = 100,transform = True)\n",
        "search_loader_realworld = DataLoader(search_realworld, batch_size=20, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "E9SzjSe6SVrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model_feature = image_model.features"
      ],
      "metadata": {
        "id": "p2VXOlysRsyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # 如果你想feature的梯度能反向传播，那么去掉 detach（）\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "model.classifier[6].register_forward_hook(get_activation('6'))\n",
        "output = model()\n",
        "print(activation['fc2'])"
      ],
      "metadata": {
        "id": "WOzEYBzCpUbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # 如果你想feature的梯度能反向传播，那么去掉 detach（）\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "outputs_= []\n",
        "def hook(module, input, output):\n",
        "    outputs_.append(output)\n",
        "\n",
        "image_model.classifier[6].register_forward_hook(hook)\n",
        "# output = model()\n",
        "# print(activation['fc2'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in search_loader_realworld:\n",
        "        data['label'] = torch.argmax(data['label'].squeeze(),dim = 1)\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "        outputs = image_model(images)\n",
        "\n",
        "        print(outputs_[0].shape)\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZxDZdj5UrHO",
        "outputId": "0c2f3b26-2261-4f8d-c7e4-f66ae5b673d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs_[0])"
      ],
      "metadata": {
        "id": "6dDFxjxPXY_2",
        "outputId": "d7aec0b8-259f-44a0-f256-defd1123b592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 2.2342, 0.0000,  ..., 0.0000, 1.8571, 0.2678],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0695, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7503, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 2.2349, 0.0000],\n",
            "        [0.0000, 1.3405, 0.0000,  ..., 0.0000, 1.1144, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1802, 0.0000]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PhPqNfkIj9Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}