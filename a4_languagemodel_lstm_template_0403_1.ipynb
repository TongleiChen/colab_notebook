{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_languagemodel_lstm_template_0403_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXV9QlMN0DQ3"
      },
      "source": [
        "## A4 - Language Model LSTM \n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "### Follow the steps to use this notebook for your A4.\n",
        "\n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of all 3 txt files from **lm-data** directory (available in a4.zip) to your Google Drive in the folder location **A4/lm-data/**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'lm-data'\n",
        "##### 3. You are all set!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkySNiz1WH6"
      },
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0qWyxlwG5h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba03b792-9c2d-4e65-9b2c-8d20a68157b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow-2.12.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.32.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (16.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (67.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.53.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.17.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "Successfully installed keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JZ5dIx40GKR",
        "outputId": "f3731b2d-f04d-40d1-9260-a20f6f22ef78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers\n",
        "import os, random\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from keras.activations import softmax\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from transformers import BertTokenizer, TFBertLMHeadModel, BertConfig, TFBertModel\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDPzX4aj1aXm"
      },
      "outputs": [],
      "source": [
        "train_file = '/content/drive/My Drive/A4/lm-data/little-prince-train.txt'\n",
        "dev_file = '/content/drive/My Drive/A4/lm-data/little-prince-dev.txt'\n",
        "test_file = '/content/drive/My Drive/A4/lm-data/little-prince-test.txt'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkKlYFrn5db8"
      },
      "source": [
        "###Change these arguments as needed for your experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnk3SXot5dlE"
      },
      "outputs": [],
      "source": [
        "epochs = 7 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "batch_size = 50 # batch size\n",
        "use_bert = False # to use the BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-aYI_pV3yYA"
      },
      "source": [
        "### Implement this function if you want to transform the input text, e.g. normalizing case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGGvg8_K3ULB"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVfmWmC-51d8"
      },
      "source": [
        "### Implement this function to generate the next-word labels for a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6PFtYXx51n9"
      },
      "outputs": [],
      "source": [
        "def shift_by_one(seq):\n",
        "    '''\n",
        "    input: ['<s>', 'The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>']\n",
        "    output: ['The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>', '[PAD]']\n",
        "    '''\n",
        "    output = []\n",
        "    for i in range(1,len(seq)):\n",
        "      output.append(seq[i])\n",
        "      \n",
        "    output.append(PAD)\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOiLZLYQ9gjc"
      },
      "source": [
        "### Download the GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXiKSjGw9g9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d860ea98-8941-44fa-f78d-daa1890b59b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-02 17:03:21--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-04-02 17:03:21--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-02 17:06:00 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -o glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NadIw1Ve51wm"
      },
      "source": [
        "### Implement this function to load the pre-trained GloVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHVU5gz1514x"
      },
      "outputs": [],
      "source": [
        "glove_file = 'glove.6B.100d.txt' # Change as necessary\n",
        "\n",
        "def load_pretrained_embeddings(glove_file, vocab):\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    embeddings_index = {}\n",
        "    word_index = dict(zip(vocab, range(len(vocab))))\n",
        "    with open(glove_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            # Each line will be a word and a list of floats, separated by spaces.\n",
        "            # If the word is in your vocabulary, create a numpy array from the list of floats.\n",
        "            # Assign the array to the correct row of embedding_matrix.\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "    num_tokens = len(vocab)\n",
        "\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "            \n",
        "    embedding_matrix[vocab[UNK]] = np.random.randn(embedding_size)\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82oUhFQb4__1"
      },
      "source": [
        "###Helper Functions (no need to implement)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZwD6Xkf30aM"
      },
      "outputs": [],
      "source": [
        "def get_vocabulary_and_data_with_bert_tokenization(data_file):\n",
        "    data = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            sent = [START]\n",
        "            sent.extend(tokenizer.tokenize(line))\n",
        "            sent.append(END)\n",
        "            data.append(sent)\n",
        "    vocab = {k:v for k,v in tokenizer.vocab.items()}\n",
        "    vocab['<s>'] = 101 # alias for [CLS]\n",
        "    vocab['</s>'] = 102 # alias for [SEP]\n",
        "    return vocab, data\n",
        "\n",
        "\n",
        "def get_vocabulary_and_data(data_file, max_vocab_size=None, use_bert=False):\n",
        "    if use_bert:\n",
        "        return get_vocabulary_and_data_with_bert_tokenization(data_file)\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            sent = [START]\n",
        "            tokens = transform_text_sequence(line.split())\n",
        "            for tok in tokens:\n",
        "                sent.append(tok)\n",
        "                vocab[tok]+=1\n",
        "            sent.append(END)\n",
        "            data.append(sent)\n",
        "            vocab[START]+=1\n",
        "            vocab[END]+=1\n",
        "    vocab = [w for w in sorted(vocab, key=lambda x:vocab[x], reverse=True)]\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, data\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, vocab):\n",
        "    vec = [1.0 if l==label else 0.0 for l in vocab]\n",
        "    return vec\n",
        "\n",
        "\n",
        "def batch_generator_lm(data, vocab, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent in data:\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(token, vocab) for token in shift_by_one(sent)])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, vocab))\n",
        "                batch_x, batch_y = np.array(batch_x), np.array(batch_y)\n",
        "                yield batch_x, batch_y.astype('float32')\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[49])\n",
        "    print('Data size',len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x\n",
        "\n",
        "\n",
        "def generate_text(language_model, vocab):\n",
        "    prediction = [START]\n",
        "    while not (prediction[-1] == END or len(prediction)>=50):\n",
        "        next_token_one_hot = language_model.predict(np.array([[vocab[p] for p in prediction]]), batch_size=1)[0][-1]\n",
        "        threshold = random.random()\n",
        "        sum = 0\n",
        "        next_token = 0\n",
        "        for i,p in enumerate(next_token_one_hot):\n",
        "            sum += p\n",
        "            if sum>threshold:\n",
        "                next_token = i\n",
        "                break\n",
        "        for w, i in vocab.items():\n",
        "            if i==next_token:\n",
        "                prediction.append(w)\n",
        "                break\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "    # https://stackoverflow.com/questions/41881308/how-to-calculate-perplexity-of-rnn-in-tensorflow\n",
        "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
        "    perp = K.exp(cross_entropy)\n",
        "    return perp\n",
        "\n",
        "\n",
        "class BERT_Wrapper(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BERT_Wrapper, self).__init__()\n",
        "    self.encoder = TFBertModel.from_pretrained(\"bert-base-uncased\", trainable=False)\n",
        "    self.dense = Dense(hidden_size)\n",
        "\n",
        "  def call(self, inputs, **kwargs):\n",
        "      outputs = self.encoder(inputs)\n",
        "      last_hidden_states = outputs[0] # The last hidden-state is the first element of the output tuple\n",
        "      output = self.dense(last_hidden_states)\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfWOhCQYmkXF"
      },
      "outputs": [],
      "source": [
        "# BERT_Wrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsxavcjCApA"
      },
      "source": [
        "###Check the GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWhEuSeB5TPy",
        "outputId": "f6bd6382-de90-4b52-8a2d-d223b5de92fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gia3C2Onp5I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wxm5waVCMPc"
      },
      "source": [
        "###Main procedure call: Implement the keras model here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXomgwnACH3P",
        "outputId": "20b9ecf8-ffe7-4b82-8d22-a5accf58bacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'next', ',', 'the', 'lamp', '##light', '##ers', 'of', 'china', 'and', 'siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 33)\n",
            "Batch output shape: (50, 33, 30524)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 3\n",
            "27/27 [==============================] - 216s 7s/step - loss: 7.2509 - accuracy: 0.4930 - perplexity: 2575198.2500\n",
            "100/100 [==============================] - 15s 96ms/step - loss: 7.2329 - accuracy: 0.0612 - perplexity: 684407.5625\n",
            "Dev Loss: 7.232909679412842 Dev Acc: 0.06123698875308037 Dev Peprlexity: 684407.5625\n",
            "Epoch 2 / 3\n",
            "27/27 [==============================] - 189s 7s/step - loss: 5.7131 - accuracy: 0.5239 - perplexity: 2532605.2500\n",
            "100/100 [==============================] - 11s 114ms/step - loss: 5.6400 - accuracy: 0.0637 - perplexity: 41809.3125\n",
            "Dev Loss: 5.639987945556641 Dev Acc: 0.06368646770715714 Dev Peprlexity: 41809.3125\n",
            "Epoch 3 / 3\n",
            "27/27 [==============================] - 190s 7s/step - loss: 5.5486 - accuracy: 0.5253 - perplexity: 2515631.5000\n",
            "100/100 [==============================] - 10s 103ms/step - loss: 5.4347 - accuracy: 0.0961 - perplexity: 52570.8516\n",
            "Dev Loss: 5.4346923828125 Dev Acc: 0.09614206850528717 Dev Peprlexity: 52570.8515625\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "['<s>', 'bobbie', 'bmw', 'now', 'prince', '\"', 'dude', 'prince', 'sunrise', '[PAD]', 'it', 'sad', 'high', 'bo', 'million', '\"', 'they', 'i', '##nova', 'the', 'and', 'aziz', '[PAD]', 'know', '[PAD]', 'me', 'heads', 'the', 'introducing', 'bucharest', 'i', '[PAD]', 'was', '##baldi', 'with', 'prince', 'to', 'flower', ',', 'the', '[PAD]', 'is', 'said', 'a', 'the', 'with', 'thousand', 'little', ',', '[PAD]']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "['<s>', 'track', '##ע', 'my', '[PAD]', 'me', 'was', 'it', 'little', 'out', 'much', 'prince', ',', 'the', 'little', 'ts', 'was', 'had', 'lamp', 'yes', 'for', '.', 'such', 'stars', 'preparations', '\"', 'bad', '##iere', '[PAD]', ',', 'i', 'only', 'westminster', 'the', 'ears', '上', 'denver', 'just', 'fuscous', 'lonely', '[PAD]', 'it', 'my', '[PAD]', 'prince', 'lamp', 'she', 'it', 'the', 'by']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "['<s>', 'towards', '[PAD]', 'that', '?', 'a', 'was', 'been', 'little', '\"', '\"', 'himself', 'upon', 'of', ':', 'see', 'am', 'only', '325', 'conquest', 'lie', ',', 'they', 'is', 'was', 'always', 'was', '[PAD]', 't', 'so', 'ideas', 'neighborhood', '-', 'active', 'if', 'prince', 'with', 'prince', 'not', ',', 'the', 'his', 'spot', 'of', 'then', 'the', 'prince', ',', 'sheep', 'explorer']\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "['<s>', ':', '##ᵀ', 'businessman', 'good', 'hugh', 'and', 'of', '[PAD]', 'a', '!', 'time', 'man', '[PAD]', 'fox', '[PAD]', 'they', 'said', 'had', '[PAD]', 'for', 'უ', ',', 'on', 'was', 'the', 'a', 'the', 'place', 'succeeded', 'and', 'heart', 'goodbye', 'so', '-', 'prince', 'does', 'eureka', 'the', 'little', 'himself', 'i', 'only', 'was', 'who', 'it', 'i', 'was', 'she', 'france']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "['<s>', 'raft', 'no', 'of', 'it', '[PAD]', ';', 'the', 'is', 'me', 'mann', 'said', '[PAD]', 'little', 'cry', 'four', 'thursday', 'it', 'geo', 'were', 'much', '[PAD]', 'they', 'did', 'said', 'sheep', 'prince', 'is', 'the', 'prince', 'prince', 'i', 'the', 'what', 'the', 'you', 'with', 'was', 'excuse', 'a', 'he', 'i', 'been', 'yet', 'a', '[PAD]', 'i', '[PAD]', 'geo', 'little']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "['<s>', 'no', '[PAD]', 'the', 'it', 'had', 'fox', '[PAD]', 'four', 'volcanoes', ',', ',', 'the', 'or', 'bo', 'little', 'i', 'wheat', 'the', 'little', ',', '[PAD]', 'i', ',', 'that', '[PAD]', '\"', 'and', '[PAD]', 'the', 'concentrated', 'am', '[unused206]', '!', 'a', 'that', 'a', 'not', ':', 'i', 'the', '[PAD]', 'said', '[PAD]', 'is', 'one', '?', 'come', 'fox', 'plane']\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "['<s>', 'rifle', 'is', 'having', 'ought', 'gave', 'scotland', 'am', 'him', '[PAD]', 'he', 'a', 'talk', 'reform', 'being', 'little', 'five', 'has', 'time', 'annoying', 'the', 'day', 'any', 'little', ',', 'then', 'that', 'my', 'the', 'a', '[PAD]', 'any', 'said', 'its', 'the', 'was', 'but', 'the', 'beforehand', 'with', 'but', 'words', 'have', 'prince', 'was', 'by', 'at', 'little', 'said', 'should']\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "['<s>', 'ah', 'change', '[PAD]', 'planet', 'weed', 'from', '[PAD]', 'are', 'natives', 'the', 'one', 'course', 'have', '.', 'of', 'prince', '##stock', 'periodically', 'now', 'have', 'laughter', 'a', 'it', 'is', '[PAD]', 'another', 'who', '2005', 'rusty', '-', 'answered', '\"', 'is', 'no', '.', 'i', 'a', 'little', 'she', 'a', 'was', ',', 'are', 'if', 'thorns', '[PAD]', 'got', 'nothing', 'who']\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "['<s>', 'sleep', 'prince', 'what', 'too', 'a', ':', 'is', 'there', 'stars', 'prince', 'good', 'sharpened', ':', 'well', 'i', 'practiced', 'have', 'prince', 'matters', ',', 'about', 'the', 'parameters', 'that', 'can', '.', ',', 'never', 'and', 'it', 'friend', 'him', 'he', '1852', 'is', 'flower', 'not', '-', 'a', 'the', '[PAD]', 'on', '[PAD]', 'water', '[PAD]', 'n', 'little', 'it', ',']\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "['<s>', 'one', 'i', ':', 'friend', '.', '\"', 'one', 'i', 'your', '##ric', 'remember', 'a', 'so', 'he', 'as', 'some', 'weed', 'track', 'i', 'was', 'not', 'night', 'yours', '[PAD]', '16', 'absurd', 'details', 'and', '-', 'color', 'only', 'me', 'about', '[PAD]', 'draw', 'they', '[PAD]', 'you', '##ited', '1928', 'i', 'was', ',', '[PAD]', 'am', 'fox', 'my', '[PAD]', 'the']\n"
          ]
        }
      ],
      "source": [
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oufrMFytC0B",
        "outputId": "a352cdc9-e1cc-4a60-a025-ab77e956e37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 5\n",
            "27/27 [==============================] - 210s 7s/step - loss: 10.1660 - accuracy: 0.4478 - perplexity: 2529373.0000\n",
            "100/100 [==============================] - 14s 98ms/step - loss: 6.6980 - accuracy: 0.0612 - perplexity: 4561.8823\n",
            "Dev Loss: 6.697998523712158 Dev Acc: 0.06123698875308037 Dev Peprlexity: 4561.88232421875\n",
            "Epoch 2 / 5\n",
            "27/27 [==============================] - 188s 7s/step - loss: 6.9378 - accuracy: 0.5240 - perplexity: 2507764.7500\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 5.7876 - accuracy: 0.0612 - perplexity: 10325.8076\n",
            "Dev Loss: 5.78758430480957 Dev Acc: 0.06123698875308037 Dev Peprlexity: 10325.8076171875\n",
            "Epoch 3 / 5\n",
            "27/27 [==============================] - 190s 7s/step - loss: 5.8301 - accuracy: 0.5237 - perplexity: 2519071.2500\n",
            "100/100 [==============================] - 13s 125ms/step - loss: 6.2371 - accuracy: 0.0612 - perplexity: 38288.0820\n",
            "Dev Loss: 6.237147808074951 Dev Acc: 0.06123698875308037 Dev Peprlexity: 38288.08203125\n",
            "Epoch 4 / 5\n",
            "27/27 [==============================] - 192s 7s/step - loss: 5.7804 - accuracy: 0.5230 - perplexity: 2531774.2500\n",
            "100/100 [==============================] - 10s 103ms/step - loss: 6.0472 - accuracy: 0.0612 - perplexity: 59609.7148\n",
            "Dev Loss: 6.047185897827148 Dev Acc: 0.06123698875308037 Dev Peprlexity: 59609.71484375\n",
            "Epoch 5 / 5\n",
            "27/27 [==============================] - 196s 7s/step - loss: 5.7653 - accuracy: 0.5222 - perplexity: 2531185.7500\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 6.0735 - accuracy: 0.0612 - perplexity: 79158.9844\n",
            "Dev Loss: 6.073492050170898 Dev Acc: 0.06123698875308037 Dev Peprlexity: 79158.984375\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "['<s>', '[PAD]', '[PAD]', '[PAD]', ',', 'other', '[PAD]', '[PAD]', '\"', 'of', '\"', '[PAD]', 'and', '[PAD]', 'began', '[PAD]', '[PAD]', 'little', '[PAD]', '\"', 'morning', '[PAD]', '[PAD]', \"'\", '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'thing', '\"', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '\"', 'three', 'his', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "['<s>', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'was', '[PAD]', 'only', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'switch', 'little', '[PAD]', '.', 'real', '[PAD]', '[PAD]', '[PAD]', ',', '[PAD]', 'this', '[PAD]', '[PAD]', '[PAD]', 'speaking', '[PAD]', '[PAD]', '[PAD]', 'in', '[PAD]', '[PAD]', '[PAD]', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'then', '[PAD]', '[PAD]', 'flower']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "['<s>', '\"', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'that', '[PAD]', '[PAD]', 'the', '?', '[PAD]', 'delicious', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '.', '[PAD]', '[PAD]', 'urgent', '[PAD]', '[PAD]', '[PAD]', 'but', '[PAD]', '[PAD]', 'all', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'little', '[PAD]', 'of', '[PAD]', '[PAD]', '[PAD]', 'somewhere', '[PAD]', 'some', 'a', '[PAD]', '[PAD]', '[PAD]', 'not']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "['<s>', 'long', '[PAD]', '[PAD]', 'will', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', ',', 'hundred', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'i', 'and', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '-', '[PAD]', '[PAD]', '[PAD]', 'at', '[PAD]', 'myself', '[PAD]', '[PAD]', '[PAD]', 'true', '[PAD]', '[PAD]', 'is', '[PAD]', '##s', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'they', 'when']\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "['<s>', '302', 'come', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'there', 'will', ',', 'the', '[PAD]', 'felt', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'you', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '.', '-', '[PAD]', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'of', 'i', '[PAD]', '[PAD]', '[PAD]', 'out', '[PAD]']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "['<s>', 'beauty', '[PAD]', 'drawing', '[PAD]', 'time', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'on', 'now', '[PAD]', '[PAD]', '[PAD]', '.', 'plants', '[PAD]', '[PAD]', 'drop', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'yes', 'eat', 'little', 'wells', '[PAD]', 'men', '-', '.', 'heaven', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'drinking', 'the', '[PAD]', '[PAD]', '[PAD]', '-', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "['<s>', 'kind', ',', '[PAD]', '[PAD]', 'do', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'off', '[PAD]', '[PAD]', '[PAD]', 'i', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'planet', '[PAD]', 'you', '[PAD]', 'protruding', '[PAD]', '[PAD]', ',', 'ball', 'all', '[PAD]', 'smooth', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'to', '[PAD]', 'and', '[PAD]', '[PAD]', '[PAD]', 'the', 'of']\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "['<s>', '[PAD]', 'last', 'fields', '[PAD]', '[PAD]', 'to', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', ',', '[PAD]', '\"', '[PAD]', '[PAD]', '[PAD]', '[PAD]', ',', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'if', 'one', 'the', 'is', 'his', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'or', '[PAD]', '[PAD]', \"'\", '[PAD]', '[PAD]', '[PAD]', 'it']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "['<s>', 'momentum', '[PAD]', 'walk', '[PAD]', 'not', '[PAD]', '[PAD]', ',', '[PAD]', '[PAD]', 'some', '[PAD]', 'a', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', ',', '[PAD]', '[PAD]', 'from', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'little', '[PAD]', '[PAD]', 'him', '.', '[PAD]', '[PAD]', 'electricity', '[PAD]', 'loaf', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'of', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'nothing']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "['<s>', 'disturbed', '[PAD]', ',', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'that', '[PAD]', '[PAD]', '[PAD]', 'happy', 'there', '[PAD]', 'if', '[PAD]', '[PAD]', '[PAD]', 'wander', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'ins', '[PAD]', 'this', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'looked', '[PAD]', '[PAD]', 'the', '[PAD]', 'exercise', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 15\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpt0jN4j6emO",
        "outputId": "1379ad61-cbae-4870-cb70-c11f28cc10dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 15\n",
            "27/27 [==============================] - 14s 452ms/step - loss: 6.3350 - accuracy: 0.4470 - perplexity: 2541708.2500\n",
            "100/100 [==============================] - 2s 11ms/step - loss: 4.8476 - accuracy: 0.0936 - perplexity: 116598.4844\n",
            "Dev Loss: 4.8476176261901855 Dev Acc: 0.09361163526773453 Dev Peprlexity: 116598.484375\n",
            "Epoch 2 / 15\n",
            "27/27 [==============================] - 11s 404ms/step - loss: 5.4141 - accuracy: 0.5455 - perplexity: 2509409.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.3492 - accuracy: 0.1746 - perplexity: 16956.7598\n",
            "Dev Loss: 4.349239349365234 Dev Acc: 0.17457304894924164 Dev Peprlexity: 16956.759765625\n",
            "Epoch 3 / 15\n",
            "27/27 [==============================] - 12s 444ms/step - loss: 5.3381 - accuracy: 0.5504 - perplexity: 2514737.7500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.0823 - accuracy: 0.1999 - perplexity: 3358.7937\n",
            "Dev Loss: 4.08231782913208 Dev Acc: 0.19987349212169647 Dev Peprlexity: 3358.793701171875\n",
            "Epoch 4 / 15\n",
            "27/27 [==============================] - 14s 548ms/step - loss: 5.2596 - accuracy: 0.5552 - perplexity: 2495654.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9803 - accuracy: 0.2075 - perplexity: 3350.0547\n",
            "Dev Loss: 3.980283737182617 Dev Acc: 0.20746363699436188 Dev Peprlexity: 3350.0546875\n",
            "Epoch 5 / 15\n",
            "27/27 [==============================] - 14s 549ms/step - loss: 5.2505 - accuracy: 0.5561 - perplexity: 2501638.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9742 - accuracy: 0.2125 - perplexity: 5353.0601\n",
            "Dev Loss: 3.974179267883301 Dev Acc: 0.2125237137079239 Dev Peprlexity: 5353.06005859375\n",
            "Epoch 6 / 15\n",
            "27/27 [==============================] - 13s 509ms/step - loss: 5.2440 - accuracy: 0.5560 - perplexity: 2511434.0000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9919 - accuracy: 0.2106 - perplexity: 7149.3975\n",
            "Dev Loss: 3.9918999671936035 Dev Acc: 0.21062618494033813 Dev Peprlexity: 7149.3974609375\n",
            "Epoch 7 / 15\n",
            "27/27 [==============================] - 13s 506ms/step - loss: 5.1937 - accuracy: 0.5571 - perplexity: 2487617.0000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.9745 - accuracy: 0.2087 - perplexity: 6133.6045\n",
            "Dev Loss: 3.974499225616455 Dev Acc: 0.20872865617275238 Dev Peprlexity: 6133.6044921875\n",
            "Epoch 8 / 15\n",
            "27/27 [==============================] - 13s 481ms/step - loss: 5.1809 - accuracy: 0.5580 - perplexity: 2481396.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.9575 - accuracy: 0.2113 - perplexity: 4759.4639\n",
            "Dev Loss: 3.9574642181396484 Dev Acc: 0.2112586945295334 Dev Peprlexity: 4759.4638671875\n",
            "Epoch 9 / 15\n",
            "27/27 [==============================] - 13s 500ms/step - loss: 5.1807 - accuracy: 0.5584 - perplexity: 2499116.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9255 - accuracy: 0.2062 - perplexity: 3856.7461\n",
            "Dev Loss: 3.9254965782165527 Dev Acc: 0.20619860291481018 Dev Peprlexity: 3856.74609375\n",
            "Epoch 10 / 15\n",
            "27/27 [==============================] - 14s 515ms/step - loss: 5.1707 - accuracy: 0.5583 - perplexity: 2501824.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.9070 - accuracy: 0.2119 - perplexity: 6709.0957\n",
            "Dev Loss: 3.9069602489471436 Dev Acc: 0.21189120411872864 Dev Peprlexity: 6709.095703125\n",
            "Epoch 11 / 15\n",
            "27/27 [==============================] - 13s 480ms/step - loss: 5.1941 - accuracy: 0.5574 - perplexity: 2513117.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.9049 - accuracy: 0.2119 - perplexity: 6012.2866\n",
            "Dev Loss: 3.904923915863037 Dev Acc: 0.21189120411872864 Dev Peprlexity: 6012.28662109375\n",
            "Epoch 12 / 15\n",
            "27/27 [==============================] - 13s 508ms/step - loss: 5.1972 - accuracy: 0.5583 - perplexity: 2521743.2500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8522 - accuracy: 0.2315 - perplexity: 8129.4751\n",
            "Dev Loss: 3.852186918258667 Dev Acc: 0.23149904608726501 Dev Peprlexity: 8129.47509765625\n",
            "Epoch 13 / 15\n",
            "27/27 [==============================] - 11s 412ms/step - loss: 5.1664 - accuracy: 0.5604 - perplexity: 2506428.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8344 - accuracy: 0.2353 - perplexity: 9579.7354\n",
            "Dev Loss: 3.8344178199768066 Dev Acc: 0.23529411852359772 Dev Peprlexity: 9579.7353515625\n",
            "Epoch 14 / 15\n",
            "27/27 [==============================] - 13s 474ms/step - loss: 5.1993 - accuracy: 0.5590 - perplexity: 2532167.0000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.8071 - accuracy: 0.2366 - perplexity: 16977.5000\n",
            "Dev Loss: 3.807121992111206 Dev Acc: 0.23655913770198822 Dev Peprlexity: 16977.5\n",
            "Epoch 15 / 15\n",
            "27/27 [==============================] - 13s 506ms/step - loss: 5.1398 - accuracy: 0.5620 - perplexity: 2509368.7500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.7932 - accuracy: 0.2347 - perplexity: 17069.9434\n",
            "Dev Loss: 3.7932190895080566 Dev Acc: 0.23466160893440247 Dev Peprlexity: 17069.943359375\n",
            "1/1 [==============================] - 0s 361ms/step\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', 'There', 'is', 'took', 'not', 'all', '.', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['<s>', 'He', ',', '\"', 'little', 'by', 'you', 'day', ',', 'said', 'the', 'Drawing', 'is', 'said', 'far', 'been', 'himself', 'be', 'my', 'Every', 'could', 'anything', '!', '</s>']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "['<s>', '\"', 'It', ',', 'elephants', ',', 'because', '?', '</s>']\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'The', '.', '</s>']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "['<s>', 'On', 'it', 'the', 'Boa', 'did', 'hesitated', ',', '\"', 'I', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', '\"', 'You', '.', '\"', '\"', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', 'He', 'so', 'it', 'I', 'found', 'plants', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', '\"', 'Here', 'am', 'to', 'I', 'shall', 'a', 'possible', 'asked', '.', '</s>']\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "['<s>', 'It', 'my', 'explorers', '!', 'and', '\"', 'preparations', 'the', '326', 'had', 'make', 'beautiful', ',', 'the', '-', 'thousand', 'out', 'to', 'respected', 'in', '</s>']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['<s>', 'The', 'little', 'is', 'But', 'is', 'the', 'detail', ',', 'the', 'planet', 'shell', 'it', 'that', 'and', '\"', 'this', 'fox', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "hidden_size = 5\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7def4f06-af25-43a9-f2ae-7e24f1ef18e0",
        "id": "C1YaYTDaAIbE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 10\n",
            "27/27 [==============================] - 43s 495ms/step - loss: 6.2459 - accuracy: 0.4923 - perplexity: 2504097.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.8108 - accuracy: 0.0633 - perplexity: 17685.8105\n",
            "Dev Loss: 4.810844898223877 Dev Acc: 0.06325110793113708 Dev Peprlexity: 17685.810546875\n",
            "Epoch 2 / 10\n",
            "27/27 [==============================] - 13s 480ms/step - loss: 5.4493 - accuracy: 0.5251 - perplexity: 2504651.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.5948 - accuracy: 0.1347 - perplexity: 8143.6191\n",
            "Dev Loss: 4.594799041748047 Dev Acc: 0.13472485542297363 Dev Peprlexity: 8143.619140625\n",
            "Epoch 3 / 10\n",
            "27/27 [==============================] - 13s 504ms/step - loss: 5.4007 - accuracy: 0.5381 - perplexity: 2505874.5000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.5037 - accuracy: 0.1543 - perplexity: 4783.2236\n",
            "Dev Loss: 4.503693103790283 Dev Acc: 0.15433269739151 Dev Peprlexity: 4783.2236328125\n",
            "Epoch 4 / 10\n",
            "27/27 [==============================] - 11s 425ms/step - loss: 5.3460 - accuracy: 0.5438 - perplexity: 2465141.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.5213 - accuracy: 0.1720 - perplexity: 2762.1313\n",
            "Dev Loss: 4.5212578773498535 Dev Acc: 0.17204301059246063 Dev Peprlexity: 2762.13134765625\n",
            "Epoch 5 / 10\n",
            "27/27 [==============================] - 13s 495ms/step - loss: 5.3443 - accuracy: 0.5454 - perplexity: 2502284.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.3600 - accuracy: 0.1708 - perplexity: 2160.1523\n",
            "Dev Loss: 4.359964370727539 Dev Acc: 0.17077799141407013 Dev Peprlexity: 2160.15234375\n",
            "Epoch 6 / 10\n",
            "27/27 [==============================] - 13s 482ms/step - loss: 5.3228 - accuracy: 0.5508 - perplexity: 2514378.7500\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 4.1956 - accuracy: 0.1872 - perplexity: 1991.9490\n",
            "Dev Loss: 4.195607662200928 Dev Acc: 0.18722327053546906 Dev Peprlexity: 1991.948974609375\n",
            "Epoch 7 / 10\n",
            "27/27 [==============================] - 11s 406ms/step - loss: 5.2720 - accuracy: 0.5537 - perplexity: 2488121.7500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.1236 - accuracy: 0.1872 - perplexity: 2257.8892\n",
            "Dev Loss: 4.123592376708984 Dev Acc: 0.18722327053546906 Dev Peprlexity: 2257.88916015625\n",
            "Epoch 8 / 10\n",
            "27/27 [==============================] - 13s 505ms/step - loss: 5.2766 - accuracy: 0.5540 - perplexity: 2500520.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.0889 - accuracy: 0.1917 - perplexity: 2809.1018\n",
            "Dev Loss: 4.088930606842041 Dev Acc: 0.191650852560997 Dev Peprlexity: 2809.101806640625\n",
            "Epoch 9 / 10\n",
            "27/27 [==============================] - 13s 510ms/step - loss: 5.3280 - accuracy: 0.5528 - perplexity: 2543612.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9934 - accuracy: 0.2005 - perplexity: 2354.8044\n",
            "Dev Loss: 3.9934279918670654 Dev Acc: 0.20050600171089172 Dev Peprlexity: 2354.804443359375\n",
            "Epoch 10 / 10\n",
            "27/27 [==============================] - 12s 464ms/step - loss: 5.2255 - accuracy: 0.5579 - perplexity: 2495694.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.9799 - accuracy: 0.2024 - perplexity: 2474.7224\n",
            "Dev Loss: 3.9799437522888184 Dev Acc: 0.20240354537963867 Dev Peprlexity: 2474.722412109375\n",
            "1/1 [==============================] - 0s 465ms/step\n",
            "1/1 [==============================] - 0s 337ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "['<s>', '\"', 'The', 'know', 'my', 'me', 'Yes', '\"', 'of', 'string', '.', 'he', 'desert', '!', '</s>']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'He', 'must', 'how', 'worried', '!', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', '\"', 'I', 'man', 'of', '\"', '-', 'when', 'of', 'see', '...', '</s>']\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', 'I', 'owned', 'it', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', '\"', 'It', 'he', 'my', 'which', 'that', 'go', 'a', 'on', 'be', 'of', 'or', 'little', 'four', 'understand', 'that', 'nursing', 'from', 'finger', 'to', 'quiet', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "['<s>', 'He', 'I', 'is', '.', '</s>']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "['<s>', 'I', 'that', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', 'The', 'Bit', '8', '</s>']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', '\"', 'Yes', 'made', 'say', '.', '</s>']\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', 'The', 'you', 'days', 'tames', 'learned', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "hidden_size = 50\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c536154-8052-450f-eb75-d6f2016f3dfb",
        "id": "CoGRRgOrHTiZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 10\n",
            "27/27 [==============================] - 21s 651ms/step - loss: 5.9832 - accuracy: 0.5182 - perplexity: 2684598.0000\n",
            "100/100 [==============================] - 2s 12ms/step - loss: 4.4638 - accuracy: 0.1455 - perplexity: 24139.1758\n",
            "Dev Loss: 4.463803291320801 Dev Acc: 0.1454775482416153 Dev Peprlexity: 24139.17578125\n",
            "Epoch 2 / 10\n",
            "27/27 [==============================] - 14s 537ms/step - loss: 5.3493 - accuracy: 0.5493 - perplexity: 2521418.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.0982 - accuracy: 0.1866 - perplexity: 4176.0991\n",
            "Dev Loss: 4.098181247711182 Dev Acc: 0.1865907609462738 Dev Peprlexity: 4176.09912109375\n",
            "Epoch 3 / 10\n",
            "27/27 [==============================] - 11s 431ms/step - loss: 5.2198 - accuracy: 0.5589 - perplexity: 2491401.0000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9728 - accuracy: 0.2081 - perplexity: 2530.6182\n",
            "Dev Loss: 3.9728448390960693 Dev Acc: 0.20809614658355713 Dev Peprlexity: 2530.6181640625\n",
            "Epoch 4 / 10\n",
            "27/27 [==============================] - 13s 509ms/step - loss: 5.1595 - accuracy: 0.5626 - perplexity: 2479328.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.8616 - accuracy: 0.2188 - perplexity: 3977.0254\n",
            "Dev Loss: 3.861614942550659 Dev Acc: 0.2188488245010376 Dev Peprlexity: 3977.025390625\n",
            "Epoch 5 / 10\n",
            "27/27 [==============================] - 13s 510ms/step - loss: 5.1656 - accuracy: 0.5650 - perplexity: 2500348.2500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.7783 - accuracy: 0.2423 - perplexity: 12743.1572\n",
            "Dev Loss: 3.7782821655273438 Dev Acc: 0.24225173890590668 Dev Peprlexity: 12743.1572265625\n",
            "Epoch 6 / 10\n",
            "27/27 [==============================] - 10s 397ms/step - loss: 5.0901 - accuracy: 0.5666 - perplexity: 2493768.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8377 - accuracy: 0.2378 - perplexity: 18387.0586\n",
            "Dev Loss: 3.837714433670044 Dev Acc: 0.23782415688037872 Dev Peprlexity: 18387.05859375\n",
            "Epoch 7 / 10\n",
            "27/27 [==============================] - 12s 467ms/step - loss: 5.0593 - accuracy: 0.5687 - perplexity: 2484791.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.7897 - accuracy: 0.2378 - perplexity: 19399.5508\n",
            "Dev Loss: 3.789682626724243 Dev Acc: 0.23782415688037872 Dev Peprlexity: 19399.55078125\n",
            "Epoch 8 / 10\n",
            "27/27 [==============================] - 15s 567ms/step - loss: 5.0908 - accuracy: 0.5661 - perplexity: 2517901.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.7099 - accuracy: 0.2391 - perplexity: 10442.3213\n",
            "Dev Loss: 3.7099006175994873 Dev Acc: 0.23908919095993042 Dev Peprlexity: 10442.3212890625\n",
            "Epoch 9 / 10\n",
            "27/27 [==============================] - 14s 518ms/step - loss: 4.9929 - accuracy: 0.5720 - perplexity: 2476657.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.7028 - accuracy: 0.2454 - perplexity: 36678.6641\n",
            "Dev Loss: 3.7028403282165527 Dev Acc: 0.24541430175304413 Dev Peprlexity: 36678.6640625\n",
            "Epoch 10 / 10\n",
            "27/27 [==============================] - 13s 509ms/step - loss: 4.9860 - accuracy: 0.5723 - perplexity: 2472662.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.7962 - accuracy: 0.2372 - perplexity: 37102.5156\n",
            "Dev Loss: 3.7962465286254883 Dev Acc: 0.23719164729118347 Dev Peprlexity: 37102.515625\n",
            "1/1 [==============================] - 0s 344ms/step\n",
            "1/1 [==============================] - 0s 352ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "['<s>', 'Unfortunately', 'is', 'discouraged', 'me', 'said', ',', 'favorable', ',', 'the', 'little', 'prince', 'climbed', 'of', 'modest', 'than', 'speaking', 'with', 'the', 'earth', '.', '</s>']\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "['<s>', '\"', 'Who', 'orders', 'One', 'have', 'I', 'wanted', 'all', 'come', 'about', '-', '-', 'ups', 'of', 'white', 'that', 'would', 'the', 'sheep', ',', 'too', '!', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "['<s>', '\"', 'Exactly', 'morning', 'away', '?', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "['<s>', 'The', 'took', 'me', 'in', 'thirsty', 'are', 'the', 'time', 'in', 'the', 'tall', '.', '</s>']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', 'It', 'one', 'room', '-', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "['<s>', 'For', 'I', 'had', 'get', 'our', 'your', 'way', '.', '</s>']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', '\"', 'Leave', 'am', 'that', 'have', 'able', 'the', 'businessman', '.', '</s>']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "['<s>', 'You', 'he', 'have', 'outside', '.', '</s>']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', 'The', 'does', 'waiting', 'the', 'tone', '.', '</s>']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "['<s>', 'No', 'idea', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "hidden_size = 100\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e18044-e5a5-405b-a3ed-6a1917311850",
        "id": "80tUeaQ2Hi7B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 10\n",
            "27/27 [==============================] - 16s 474ms/step - loss: 5.6660 - accuracy: 0.5266 - perplexity: 2515363.2500\n",
            "100/100 [==============================] - 2s 10ms/step - loss: 4.3084 - accuracy: 0.1841 - perplexity: 30388.1484\n",
            "Dev Loss: 4.308379650115967 Dev Acc: 0.1840607225894928 Dev Peprlexity: 30388.1484375\n",
            "Epoch 2 / 10\n",
            "27/27 [==============================] - 11s 405ms/step - loss: 5.2636 - accuracy: 0.5567 - perplexity: 2520844.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8305 - accuracy: 0.2302 - perplexity: 27271.1270\n",
            "Dev Loss: 3.830456018447876 Dev Acc: 0.2302340269088745 Dev Peprlexity: 27271.126953125\n",
            "Epoch 3 / 10\n",
            "27/27 [==============================] - 12s 449ms/step - loss: 5.1690 - accuracy: 0.5668 - perplexity: 2509747.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.7491 - accuracy: 0.2505 - perplexity: 42611.1211\n",
            "Dev Loss: 3.7490835189819336 Dev Acc: 0.25047439336776733 Dev Peprlexity: 42611.12109375\n",
            "Epoch 4 / 10\n",
            "27/27 [==============================] - 13s 490ms/step - loss: 5.1149 - accuracy: 0.5667 - perplexity: 2517298.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.6980 - accuracy: 0.2498 - perplexity: 50408.3906\n",
            "Dev Loss: 3.6980056762695312 Dev Acc: 0.2498418688774109 Dev Peprlexity: 50408.390625\n",
            "Epoch 5 / 10\n",
            "27/27 [==============================] - 11s 418ms/step - loss: 5.0654 - accuracy: 0.5688 - perplexity: 2503941.7500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.6864 - accuracy: 0.2524 - perplexity: 46442.0430\n",
            "Dev Loss: 3.6864380836486816 Dev Acc: 0.2523719072341919 Dev Peprlexity: 46442.04296875\n",
            "Epoch 6 / 10\n",
            "27/27 [==============================] - 13s 501ms/step - loss: 5.0369 - accuracy: 0.5702 - perplexity: 2501078.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.7289 - accuracy: 0.2530 - perplexity: 53533.8203\n",
            "Dev Loss: 3.7288501262664795 Dev Acc: 0.25300443172454834 Dev Peprlexity: 53533.8203125\n",
            "Epoch 7 / 10\n",
            "27/27 [==============================] - 13s 510ms/step - loss: 4.9980 - accuracy: 0.5705 - perplexity: 2486392.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.8018 - accuracy: 0.2600 - perplexity: 73512.9219\n",
            "Dev Loss: 3.8018298149108887 Dev Acc: 0.2599620521068573 Dev Peprlexity: 73512.921875\n",
            "Epoch 8 / 10\n",
            "27/27 [==============================] - 14s 524ms/step - loss: 4.9924 - accuracy: 0.5708 - perplexity: 2494021.0000\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 3.8177 - accuracy: 0.2505 - perplexity: 129030.4062\n",
            "Dev Loss: 3.8177175521850586 Dev Acc: 0.25047439336776733 Dev Peprlexity: 129030.40625\n",
            "Epoch 9 / 10\n",
            "27/27 [==============================] - 12s 456ms/step - loss: 5.0017 - accuracy: 0.5730 - perplexity: 2502667.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8138 - accuracy: 0.2511 - perplexity: 136950.4219\n",
            "Dev Loss: 3.813826084136963 Dev Acc: 0.2511068880558014 Dev Peprlexity: 136950.421875\n",
            "Epoch 10 / 10\n",
            "27/27 [==============================] - 12s 454ms/step - loss: 4.9693 - accuracy: 0.5724 - perplexity: 2506839.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.7804 - accuracy: 0.2574 - perplexity: 159491.5781\n",
            "Dev Loss: 3.7803518772125244 Dev Acc: 0.2574320137500763 Dev Peprlexity: 159491.578125\n",
            "1/1 [==============================] - 0s 355ms/step\n",
            "1/1 [==============================] - 1s 512ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "['<s>', '\"', 'Those', 'is', 'on', 'my', '.', '</s>']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "['<s>', '\"', 'Over', 'does', 'are', 'possible', 'for', 'here', '...', 'he', 'learned', 'from', 'her', 'Drawing', 'Number', 'One', ',', 'and', 'a', 'heart', 'whole', ',', 'and', 'set', 'his', 'they', 'would', 'be', 'be', 'tie', 'with', 'this', 'little', 'prince', 'answered', 'could', 'on', 'on', 'for', 'hunger', 'constrictors', 'and', 'danger', 'China', ';', 'would', 'enter', 'so', 'cumbersome', 'departure']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'And', 'have', 'gold', '.', 'once', 'loved', ',', '\"', 'said', 'the', 'little', 'prince', 'is', 'from', 'consequence', ':', '\"', 'What', 'thought', '?', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'And', 'from', 'his', 'escape', ',', 'and', 'sand', 'you', 'see', 'you', 'from', 'our', 'others', 'to', 'me', 'as', 'come', 'on', 'another', 'neck', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "['<s>', 'Only', 'the', 'to', 'take', 'other', 'as', 'steadily', 'by', 'an', 'color', '...', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "['<s>', 'This', 'I', 'tossed', 'off', 'all', 'hear', 'that', 'only', 'wasted', 'for', 'to', 'eat', 'at', 'I', 'demanded', '.', '</s>']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "['<s>', '\"', 'It', 'am', ',', 'have', 'six', 'back', 'through', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "['<s>', 'I', 'must', 'find', '.', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "['<s>', 'And', '...', '\"', 'said', 'the', 'little', '-', 'last', 'of', 'consequence', '.', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "['<s>', '\"', 'They', 'morning', 'you', 'happier', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "hidden_size = 200\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd845fd-f9be-46a2-c4f0-f63cf4c62d55",
        "id": "t8-1op6NHtF6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 10\n",
            "27/27 [==============================] - 15s 423ms/step - loss: 6.7111 - accuracy: 0.4930 - perplexity: 3279052.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 7.2942 - accuracy: 0.1025 - perplexity: 2940504.7500\n",
            "Dev Loss: 7.294198989868164 Dev Acc: 0.10246679186820984 Dev Peprlexity: 2940504.75\n",
            "Epoch 2 / 10\n",
            "27/27 [==============================] - 14s 523ms/step - loss: 6.2203 - accuracy: 0.5351 - perplexity: 3350332.7500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 6.6733 - accuracy: 0.1581 - perplexity: 2883330.7500\n",
            "Dev Loss: 6.673266410827637 Dev Acc: 0.1581277698278427 Dev Peprlexity: 2883330.75\n",
            "Epoch 3 / 10\n",
            "27/27 [==============================] - 14s 515ms/step - loss: 6.1255 - accuracy: 0.5410 - perplexity: 3348122.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 6.5695 - accuracy: 0.1777 - perplexity: 2866486.0000\n",
            "Dev Loss: 6.569479942321777 Dev Acc: 0.1777356117963791 Dev Peprlexity: 2866486.0\n",
            "Epoch 4 / 10\n",
            "27/27 [==============================] - 14s 540ms/step - loss: 6.0591 - accuracy: 0.5476 - perplexity: 3329730.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 6.4645 - accuracy: 0.1828 - perplexity: 2900080.7500\n",
            "Dev Loss: 6.464451789855957 Dev Acc: 0.1827957034111023 Dev Peprlexity: 2900080.75\n",
            "Epoch 5 / 10\n",
            "27/27 [==============================] - 13s 503ms/step - loss: 6.0793 - accuracy: 0.5476 - perplexity: 3357720.0000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 6.4270 - accuracy: 0.1929 - perplexity: 2883349.2500\n",
            "Dev Loss: 6.4269795417785645 Dev Acc: 0.1929158717393875 Dev Peprlexity: 2883349.25\n",
            "Epoch 6 / 10\n",
            "27/27 [==============================] - 12s 453ms/step - loss: 6.0633 - accuracy: 0.5498 - perplexity: 3353162.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 6.4675 - accuracy: 0.1872 - perplexity: 2901357.0000\n",
            "Dev Loss: 6.467472553253174 Dev Acc: 0.18722327053546906 Dev Peprlexity: 2901357.0\n",
            "Epoch 7 / 10\n",
            "27/27 [==============================] - 14s 534ms/step - loss: 6.0664 - accuracy: 0.5502 - perplexity: 3357960.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 6.3640 - accuracy: 0.1942 - perplexity: 2890973.5000\n",
            "Dev Loss: 6.364020347595215 Dev Acc: 0.19418089091777802 Dev Peprlexity: 2890973.5\n",
            "Epoch 8 / 10\n",
            "27/27 [==============================] - 13s 507ms/step - loss: 6.0504 - accuracy: 0.5502 - perplexity: 3347696.7500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 6.4668 - accuracy: 0.1752 - perplexity: 2909994.5000\n",
            "Dev Loss: 6.466760635375977 Dev Acc: 0.17520557343959808 Dev Peprlexity: 2909994.5\n",
            "Epoch 9 / 10\n",
            "27/27 [==============================] - 15s 560ms/step - loss: 6.0549 - accuracy: 0.5513 - perplexity: 3359176.7500\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 6.4502 - accuracy: 0.1904 - perplexity: 2950790.0000\n",
            "Dev Loss: 6.4501566886901855 Dev Acc: 0.1903858333826065 Dev Peprlexity: 2950790.0\n",
            "Epoch 10 / 10\n",
            "27/27 [==============================] - 11s 422ms/step - loss: 6.0400 - accuracy: 0.5530 - perplexity: 3351559.5000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 6.3617 - accuracy: 0.2043 - perplexity: 2929286.5000\n",
            "Dev Loss: 6.361748695373535 Dev Acc: 0.20430107414722443 Dev Peprlexity: 2929286.5\n",
            "1/1 [==============================] - 1s 569ms/step\n",
            "1/1 [==============================] - 1s 564ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'We', 'is', 'a', 'they', '</s>']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "['<s>', '\"', 'What', 'they', ',', 'little', ',', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "['<s>', 'I', 'is', 'to', 'that', 'that', 'is', '</s>']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['<s>', 'It', 'will', 'not', 'much', 'I', ',', 'you', '\"', '.', '\"', 'What', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "['<s>', 'My', 'is', ',', 'it', 'a', '.', '</s>']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'That', 'first', 'me', ',', '\"', 'the', 'a', '-', ',', 'the', 'a', 'knows', ',', 'and', '</s>']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'said', 'I', '[PAD]', 'am', '</s>']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "['<s>', 'The', 'little', 'never', 'the', '.', '\"', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'She', 'never', ',', 'and', ',', ',', 'that', 'planet', 'had', 'old', '.', '</s>']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "['<s>', 'They', 'little', 'not', 'to', '.', 'the', 'little', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "hidden_size = 10\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76248a9d-62dc-4fb4-a963-116c7eead512",
        "id": "nf93BofsHyL7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 10\n",
            "27/27 [==============================] - 14s 414ms/step - loss: 6.0988 - accuracy: 0.5074 - perplexity: 2530199.5000\n",
            "100/100 [==============================] - 1s 9ms/step - loss: 4.8221 - accuracy: 0.0734 - perplexity: 83939.5938\n",
            "Dev Loss: 4.822131156921387 Dev Acc: 0.0733712837100029 Dev Peprlexity: 83939.59375\n",
            "Epoch 2 / 10\n",
            "27/27 [==============================] - 13s 476ms/step - loss: 5.4023 - accuracy: 0.5369 - perplexity: 2491954.0000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.5212 - accuracy: 0.1467 - perplexity: 18941.6973\n",
            "Dev Loss: 4.521227836608887 Dev Acc: 0.1467425674200058 Dev Peprlexity: 18941.697265625\n",
            "Epoch 3 / 10\n",
            "27/27 [==============================] - 12s 452ms/step - loss: 5.3021 - accuracy: 0.5458 - perplexity: 2471888.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.2787 - accuracy: 0.1784 - perplexity: 2150.3076\n",
            "Dev Loss: 4.278672218322754 Dev Acc: 0.17836812138557434 Dev Peprlexity: 2150.3076171875\n",
            "Epoch 4 / 10\n",
            "27/27 [==============================] - 13s 508ms/step - loss: 5.2587 - accuracy: 0.5520 - perplexity: 2484074.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.0788 - accuracy: 0.1891 - perplexity: 2132.1995\n",
            "Dev Loss: 4.078842639923096 Dev Acc: 0.189120814204216 Dev Peprlexity: 2132.199462890625\n",
            "Epoch 5 / 10\n",
            "27/27 [==============================] - 12s 438ms/step - loss: 5.2564 - accuracy: 0.5536 - perplexity: 2498059.0000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.9948 - accuracy: 0.2075 - perplexity: 2103.3103\n",
            "Dev Loss: 3.9948058128356934 Dev Acc: 0.20746363699436188 Dev Peprlexity: 2103.310302734375\n",
            "Epoch 6 / 10\n",
            "27/27 [==============================] - 13s 508ms/step - loss: 5.2340 - accuracy: 0.5574 - perplexity: 2507814.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.9065 - accuracy: 0.2087 - perplexity: 2491.3555\n",
            "Dev Loss: 3.9064865112304688 Dev Acc: 0.20872865617275238 Dev Peprlexity: 2491.35546875\n",
            "Epoch 7 / 10\n",
            "27/27 [==============================] - 13s 505ms/step - loss: 5.1935 - accuracy: 0.5599 - perplexity: 2497422.5000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8483 - accuracy: 0.2201 - perplexity: 3802.8750\n",
            "Dev Loss: 3.848334312438965 Dev Acc: 0.2201138585805893 Dev Peprlexity: 3802.875\n",
            "Epoch 8 / 10\n",
            "27/27 [==============================] - 12s 453ms/step - loss: 5.1772 - accuracy: 0.5627 - perplexity: 2501594.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.7540 - accuracy: 0.2397 - perplexity: 4135.8560\n",
            "Dev Loss: 3.753964424133301 Dev Acc: 0.23972170054912567 Dev Peprlexity: 4135.85595703125\n",
            "Epoch 9 / 10\n",
            "27/27 [==============================] - 13s 512ms/step - loss: 5.1571 - accuracy: 0.5626 - perplexity: 2506115.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.7110 - accuracy: 0.2441 - perplexity: 5201.2681\n",
            "Dev Loss: 3.7109808921813965 Dev Acc: 0.24414926767349243 Dev Peprlexity: 5201.26806640625\n",
            "Epoch 10 / 10\n",
            "27/27 [==============================] - 16s 617ms/step - loss: 5.1009 - accuracy: 0.5680 - perplexity: 2476120.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 3.6858 - accuracy: 0.2467 - perplexity: 5977.8008\n",
            "Dev Loss: 3.685849905014038 Dev Acc: 0.24667932093143463 Dev Peprlexity: 5977.80078125\n",
            "1/1 [==============================] - 0s 347ms/step\n",
            "1/1 [==============================] - 0s 334ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['<s>', 'He', 'is', 'knew', ',', 'the', 'Africa', '.', '</s>']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "['<s>', 'He', 'it', 'to', 'dare', 'bring', 'drinking', 'to', 'two', 'sank', 'prince', 'themselves', 'and', 'was', 'I', 'am', 'like', 'like', 'one', 'large', 'are', 'whatever', '-', 'the', 'richest', 'prince', 'in', 'across', 'ring', 'who', ',', 'she', 'pity', 'the', 'stars', ',', 'nothing', ',', 'made', 'that', 'the', 'moment', '.', '</s>']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', '\"', 'They', 'you', 'that', 'tame', 'prince', 'into', 'replied', 'be', 'not', 'you', 'rush', 'days', 'much', 'often', 'he', 'to', 'himself', 'naïve', '.', '...', '\"', '\"', 'asked', 'you', 'by', 'do', ',', '\"', 'the', 'altogether', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'But', ',', '\"', ':', '?', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', 'Just', 'two', 'regard', ',', 'all', '.', '</s>']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', '\"', 'story', 'frightened', '-', 'too', ',', 'the', 'planet', 'blows', 'of', 'Not', '[PAD]', 'proud', 'in', 'clean', 'king', ',', 'or', 'rosy', '.', '</s>']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'Nothing', 'he', 'about', 'what', 'an', 'harm', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'Thirty', 'the', 'explorer', '.', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "['<s>', 'I', 'do', 'Ah', 'grown', 'from', 'seeds', '.', '</s>']\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "['<s>', 'You', 'to', 'sat', ',', '\"', 'went', 'this', 'drawing', 'echoed', 'planet', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_bert = False # to use the BERT embeddings\n",
        "learning_rate = 0.1\n",
        "epochs = 15\n",
        "hidden_size = 100\n",
        "embedding_size = 100\n",
        "vocab, train_data = get_vocabulary_and_data(train_file, use_bert=use_bert)\n",
        "_, dev_data = get_vocabulary_and_data(dev_file, use_bert=use_bert)\n",
        "\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, batch_size))\n",
        "\n",
        "# from keras.initializers import Constant\n",
        "\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)\n",
        "\n",
        "    for i in range(10):\n",
        "        text = generate_text(language_model, vocab)\n",
        "        print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bf13544-0d54-4f5b-a879-9e25f3c5de35",
        "id": "8FkoSF1IHlQN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Next', ',', 'the', 'lamplighters', 'of', 'China', 'and', 'Siberia', 'would', 'enter', 'for', 'their', 'steps', 'in', 'the', 'dance', ',', 'and', 'then', 'they', 'too', 'would', 'be', 'waved', 'back', 'into', 'the', 'wings', '.', '</s>']\n",
            "Data size 1362\n",
            "Batch input shape: (50, 32)\n",
            "Batch output shape: (50, 32, 2158)\n",
            "Epoch 1 / 15\n",
            "27/27 [==============================] - 16s 511ms/step - loss: 5.7540 - accuracy: 0.5283 - perplexity: 2604151.0000\n",
            "100/100 [==============================] - 2s 10ms/step - loss: 4.9786 - accuracy: 0.1746 - perplexity: 602839.8125\n",
            "Dev Loss: 4.978583812713623 Dev Acc: 0.17457304894924164 Dev Peprlexity: 602839.8125\n",
            "Epoch 2 / 15\n",
            "27/27 [==============================] - 13s 501ms/step - loss: 5.4146 - accuracy: 0.5553 - perplexity: 2630069.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.4947 - accuracy: 0.2170 - perplexity: 472138.1562\n",
            "Dev Loss: 4.494687557220459 Dev Acc: 0.21695129573345184 Dev Peprlexity: 472138.15625\n",
            "Epoch 3 / 15\n",
            "27/27 [==============================] - 12s 454ms/step - loss: 5.3520 - accuracy: 0.5613 - perplexity: 2621941.5000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.2962 - accuracy: 0.2188 - perplexity: 338101.5000\n",
            "Dev Loss: 4.296202182769775 Dev Acc: 0.2188488245010376 Dev Peprlexity: 338101.5\n",
            "Epoch 4 / 15\n",
            "27/27 [==============================] - 16s 597ms/step - loss: 5.2691 - accuracy: 0.5626 - perplexity: 2579055.0000\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 4.1706 - accuracy: 0.2163 - perplexity: 257532.7344\n",
            "Dev Loss: 4.170568466186523 Dev Acc: 0.2163187861442566 Dev Peprlexity: 257532.734375\n",
            "Epoch 5 / 15\n",
            "27/27 [==============================] - 12s 473ms/step - loss: 5.2211 - accuracy: 0.5638 - perplexity: 2570359.2500\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 4.0945 - accuracy: 0.2334 - perplexity: 235596.3125\n",
            "Dev Loss: 4.094464302062988 Dev Acc: 0.23339658975601196 Dev Peprlexity: 235596.3125\n",
            "Epoch 6 / 15\n",
            "27/27 [==============================] - 11s 424ms/step - loss: 5.2031 - accuracy: 0.5642 - perplexity: 2571285.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.0955 - accuracy: 0.2302 - perplexity: 235983.8906\n",
            "Dev Loss: 4.0954909324646 Dev Acc: 0.2302340269088745 Dev Peprlexity: 235983.890625\n",
            "Epoch 7 / 15\n",
            "27/27 [==============================] - 13s 490ms/step - loss: 5.1642 - accuracy: 0.5652 - perplexity: 2557070.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.1443 - accuracy: 0.2277 - perplexity: 250282.9844\n",
            "Dev Loss: 4.144336223602295 Dev Acc: 0.2277039885520935 Dev Peprlexity: 250282.984375\n",
            "Epoch 8 / 15\n",
            "27/27 [==============================] - 13s 476ms/step - loss: 5.1230 - accuracy: 0.5685 - perplexity: 2540982.7500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.0993 - accuracy: 0.2404 - perplexity: 272673.7500\n",
            "Dev Loss: 4.099301338195801 Dev Acc: 0.24035421013832092 Dev Peprlexity: 272673.75\n",
            "Epoch 9 / 15\n",
            "27/27 [==============================] - 11s 408ms/step - loss: 5.1076 - accuracy: 0.5693 - perplexity: 2542739.0000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.0891 - accuracy: 0.2378 - perplexity: 288198.9062\n",
            "Dev Loss: 4.089107036590576 Dev Acc: 0.23782415688037872 Dev Peprlexity: 288198.90625\n",
            "Epoch 10 / 15\n",
            "27/27 [==============================] - 14s 521ms/step - loss: 5.1490 - accuracy: 0.5651 - perplexity: 2579055.7500\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 3.9854 - accuracy: 0.2423 - perplexity: 278564.2812\n",
            "Dev Loss: 3.9854462146759033 Dev Acc: 0.24225173890590668 Dev Peprlexity: 278564.28125\n",
            "Epoch 11 / 15\n",
            "27/27 [==============================] - 13s 500ms/step - loss: 5.0271 - accuracy: 0.5726 - perplexity: 2502490.0000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.2046 - accuracy: 0.2366 - perplexity: 322374.0938\n",
            "Dev Loss: 4.204603672027588 Dev Acc: 0.23655913770198822 Dev Peprlexity: 322374.09375\n",
            "Epoch 12 / 15\n",
            "27/27 [==============================] - 14s 513ms/step - loss: 5.0293 - accuracy: 0.5705 - perplexity: 2531864.5000\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.1520 - accuracy: 0.2435 - perplexity: 324895.3438\n",
            "Dev Loss: 4.151997089385986 Dev Acc: 0.24351675808429718 Dev Peprlexity: 324895.34375\n",
            "Epoch 13 / 15\n",
            "27/27 [==============================] - 14s 520ms/step - loss: 5.0339 - accuracy: 0.5720 - perplexity: 2546111.5000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 4.1452 - accuracy: 0.2435 - perplexity: 305062.2812\n",
            "Dev Loss: 4.145206451416016 Dev Acc: 0.24351675808429718 Dev Peprlexity: 305062.28125\n",
            "Epoch 14 / 15\n",
            "27/27 [==============================] - 12s 462ms/step - loss: 4.9986 - accuracy: 0.5731 - perplexity: 2526139.7500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.2136 - accuracy: 0.2467 - perplexity: 372293.8750\n",
            "Dev Loss: 4.213637828826904 Dev Acc: 0.24667932093143463 Dev Peprlexity: 372293.875\n",
            "Epoch 15 / 15\n",
            "27/27 [==============================] - 16s 624ms/step - loss: 4.9790 - accuracy: 0.5742 - perplexity: 2520791.2500\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 4.3166 - accuracy: 0.2505 - perplexity: 496854.6250\n",
            "Dev Loss: 4.316585540771484 Dev Acc: 0.25047439336776733 Dev Peprlexity: 496854.625\n",
            "1/1 [==============================] - 1s 625ms/step\n",
            "1/1 [==============================] - 1s 517ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "['<s>', '\"', 'I', 'is', 'true', '!', 'in', 'the', 'rest', 'would', 'be', 'too', 'for', 'seek', 'have', 'see', 'makes', 'big', 'to', 'say', 'to', 'work', 'bridge', ',', 'you', 'dangling', '.', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "['<s>', '\"', 'One', 'is', 'essential', 'it', 'travel', '.', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "['<s>', '\"', 'As', 'we', 'naïve', '.', '</s>']\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "['<s>', 'To', 'will', 'not', 'space', '.', '</s>']\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', '\"', 'It', 'is', 'no', 'speak', 'me', ':', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['<s>', 'could', 'am', 'to', 'clean', 'a', 'so', 'homesick', ',', ',', '\"', '</s>']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "['<s>', 'I', 'it', 'heart', '.', '</s>']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "['<s>', 'He', 'much', ',', 'said', 'anything', 'not', 'know', 'it', '.', '</s>']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['<s>', 'But', 'little', 'prince', ',', 'had', 'so', ',', 'for', 'chickens', '.', '</s>']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['<s>', 'It', 'year', 'three', '.', '\"', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 7\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    \n",
        "    if use_bert:\n",
        "        embedding_layer = BERT_Wrapper()\n",
        "    else:\n",
        "        embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "        embedding_layer = Embedding(\n",
        "            len(vocab),\n",
        "            embedding_size,\n",
        "            embeddings_initializer=Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "        )\n",
        "    language_model = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(vocab)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    language_model.add(embedding_layer)\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    language_model.add(LSTM(hidden_size, return_sequences=True))# dropout\n",
        "\n",
        "    language_model.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    language_model.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    language_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',perplexity])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        language_model.fit(batch_generator_lm(train_data, vocab, batch_size),\n",
        "                                      epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc, perp = language_model.evaluate(batch_generator_lm(dev_data, vocab),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc, 'Dev Peprlexity:', perp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QDOV3A_Njl-",
        "outputId": "b8619e84-2035-4e58-b28d-4672fb32ed33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 7\n",
            "27/27 [==============================] - 16s 528ms/step - loss: 5.6318 - accuracy: 0.5306 - perplexity: 2504673.2500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 4.1659 - accuracy: 0.2106 - perplexity: 119609.9922\n",
            "Dev Loss: 4.1659040451049805 Dev Acc: 0.21062618494033813 Dev Peprlexity: 119609.9921875\n",
            "Epoch 2 / 7\n",
            "27/27 [==============================] - 14s 526ms/step - loss: 5.1796 - accuracy: 0.5631 - perplexity: 2493405.5000\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.9683 - accuracy: 0.2359 - perplexity: 157780.4219\n",
            "Dev Loss: 3.968287706375122 Dev Acc: 0.23592662811279297 Dev Peprlexity: 157780.421875\n",
            "Epoch 3 / 7\n",
            "27/27 [==============================] - 14s 539ms/step - loss: 5.1450 - accuracy: 0.5642 - perplexity: 2510422.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8649 - accuracy: 0.2416 - perplexity: 151367.0000\n",
            "Dev Loss: 3.8649444580078125 Dev Acc: 0.24161922931671143 Dev Peprlexity: 151367.0\n",
            "Epoch 4 / 7\n",
            "27/27 [==============================] - 12s 472ms/step - loss: 5.0863 - accuracy: 0.5684 - perplexity: 2493888.2500\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 3.9053 - accuracy: 0.2467 - perplexity: 157699.4219\n",
            "Dev Loss: 3.9053258895874023 Dev Acc: 0.24667932093143463 Dev Peprlexity: 157699.421875\n",
            "Epoch 5 / 7\n",
            "27/27 [==============================] - 13s 511ms/step - loss: 5.0401 - accuracy: 0.5695 - perplexity: 2479664.7500\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 3.8589 - accuracy: 0.2467 - perplexity: 156436.2031\n",
            "Dev Loss: 3.8588902950286865 Dev Acc: 0.24667932093143463 Dev Peprlexity: 156436.203125\n",
            "Epoch 6 / 7\n",
            "27/27 [==============================] - 13s 493ms/step - loss: 5.0868 - accuracy: 0.5682 - perplexity: 2537497.0000\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8626 - accuracy: 0.2498 - perplexity: 154233.1719\n",
            "Dev Loss: 3.8626465797424316 Dev Acc: 0.2498418688774109 Dev Peprlexity: 154233.171875\n",
            "Epoch 7 / 7\n",
            "27/27 [==============================] - 11s 408ms/step - loss: 4.9584 - accuracy: 0.5739 - perplexity: 2462331.2500\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 3.8527 - accuracy: 0.2568 - perplexity: 192382.9531\n",
            "Dev Loss: 3.852668523788452 Dev Acc: 0.25679948925971985 Dev Peprlexity: 192382.953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, test_data = get_vocabulary_and_data(test_file, use_bert=use_bert)\n",
        "loss, acc, perp = language_model.evaluate(batch_generator_lm(test_data, vocab),\n",
        "                                          steps=len(test_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc, 'Test Peprlexity:', perp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w86HXFiTNH-x",
        "outputId": "a4340285-dde3-4525-deaa-d302bf599eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 1s 13ms/step - loss: 3.8480 - accuracy: 0.2702 - perplexity: 165402.7188\n",
            "Test Loss: 3.8479981422424316 Test Acc: 0.2702205777168274 Test Peprlexity: 165402.71875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}