{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_surname_classifier_lstm_template_0403_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - Surname Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of the datafile files from **surname-data** directory (available in a4.zip) to your Google Drive in the location **A4/surname-data/surnames.csv**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'surname-data' \n",
        "##### 3. You are all set!\n"
      ],
      "metadata": {
        "id": "ppyxtiCuTt1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TL5j6-cag4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import os, random\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "ziQ6pwj0TukR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330b48b5-22af-48e6-f580-f0df9b7d147c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = '/content/drive/My Drive/A4/surname-data/surnames.csv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'"
      ],
      "metadata": {
        "id": "mOppodb0UwAD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement this function if you want to transform the input text, e.g. normalizing case"
      ],
      "metadata": {
        "id": "ploK2x6RVgfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq"
      ],
      "metadata": {
        "id": "HHOK83jMW4U8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)"
      ],
      "metadata": {
        "id": "kQpIf5xhXBNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, split, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cols = line.split(',')\n",
        "            s, surname, label = cols[0].strip(), cols[1].strip(), cols[2].strip()\n",
        "            if s==split:\n",
        "                surname = list(surname)\n",
        "                surname = [START]+surname+[END]\n",
        "                data.append(transform_text_sequence(surname))\n",
        "                labels.append(label)\n",
        "            for tok in surname:\n",
        "                vocab[tok]+=1\n",
        "\n",
        "    vocab = sorted(vocab.keys(), key=lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, set(labels), data, labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for doc, label in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(doc, vocab))\n",
        "            batch_y.append(one_hot_encode_label(label, label_set))\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "POKR921_U9KY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "r8AcTwvLW_0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "dropout = 0.3 # dropout rate not used\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 100 # embedding dimension size\n",
        "hidden_size = 50 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "XzFMKsiqXs8e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "jKG3r0mUdx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "PGj77gPjdxGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc18375-3196-4d1d-cf79-46712eed400d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n",
        "\n",
        "##### Use the variables batch_size, hidden_size, embedding_size, dropout, epochs here."
      ],
      "metadata": {
        "id": "h06MbANsXtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(data_file, 'train')\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(data_file, 'dev')\n",
        "_, _, test_data, test_labels = get_vocabulary_and_data(data_file, 'test')\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "                  batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "    classifier = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "    classifier.add(tf.keras.layers.Embedding(input_size, embedding_size, input_length=batch_size))\n",
        "    classifier.add(Bidirectional(LSTM(hidden_size, return_sequences=False)))\n",
        "    classifier.add(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        classifier.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size=batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = classifier.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)\n"
      ],
      "metadata": {
        "id": "i0a5SqKmW7Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f38596-aafc-4385-face-b6343ac8aae9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'H', 'a', 'd', 'a', 'd', '</s>']\n",
            "Label: arabic\n",
            "Label count: 19\n",
            "Data size 15000\n",
            "Batch input shape: (50, 14)\n",
            "Batch output shape: (50, 19)\n",
            "Epoch 1 / 5\n",
            "300/300 [==============================] - 12s 17ms/step - loss: 1.1910 - accuracy: 0.6501\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.9725 - accuracy: 0.7003\n",
            "Dev Loss: 0.9725028276443481 Dev Acc: 0.7003268003463745\n",
            "Epoch 2 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.7567 - accuracy: 0.7705\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.8136 - accuracy: 0.7484\n",
            "Dev Loss: 0.813614010810852 Dev Acc: 0.7483659982681274\n",
            "Epoch 3 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.6144 - accuracy: 0.8095\n",
            "3060/3060 [==============================] - 12s 4ms/step - loss: 0.7570 - accuracy: 0.7758\n",
            "Dev Loss: 0.7570313215255737 Dev Acc: 0.7758169770240784\n",
            "Epoch 4 / 5\n",
            "300/300 [==============================] - 2s 7ms/step - loss: 0.5427 - accuracy: 0.8257\n",
            "3060/3060 [==============================] - 14s 5ms/step - loss: 0.7422 - accuracy: 0.7778\n",
            "Dev Loss: 0.7422214150428772 Dev Acc: 0.7777777910232544\n",
            "Epoch 5 / 5\n",
            "300/300 [==============================] - 3s 10ms/step - loss: 0.4935 - accuracy: 0.8415\n",
            "3060/3060 [==============================] - 13s 4ms/step - loss: 0.7294 - accuracy: 0.7837\n",
            "Dev Loss: 0.7293575406074524 Dev Acc: 0.7836601138114929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = classifier.evaluate(batch_generator(test_data, test_labels, vocab, labels),\n",
        "                                                  steps=len(test_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0rQL37j-cEM",
        "outputId": "d7a5aba2-9a84-4b17-f935-330646834fa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014/2014 [==============================] - 12s 6ms/step - loss: 0.7073 - accuracy: 0.7910\n",
            "Test Loss: 0.707269549369812 Test Acc: 0.7909632325172424\n"
          ]
        }
      ]
    }
  ]
}