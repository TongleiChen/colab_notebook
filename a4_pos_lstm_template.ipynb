{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/colab_notebook/blob/main/a4_pos_lstm_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4 - POS LSTM Classifier\n",
        "\n",
        "Author: Austin Blodgett\n",
        "\n",
        "Adaptation to colab: Nitin Venkateswaran\n",
        "\n",
        "\n",
        "### Follow the steps to use this notebook for your A4. \n",
        "**NOTE**: It is best to use your Georgetown Google accounts.\n",
        "##### 1. Save a copy of this notebook starter template in your Google Drive (File -> Save a copy in drive)\n",
        "##### 2. Upload a copy of all 3 tsv files in **pos-data** directory (available in a4.zip) to your Google Drive in the folder location **A4/pos-data/**; you will need to create the folder 'A4' at the root location in your Drive, followed by the subfolder 'pos-data'\n",
        "##### 3. You are all set!"
      ],
      "metadata": {
        "id": "naWvCo-lnKX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Import libraries and mount Google Drive\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tCImn0GnNC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed,Input,Dropout\n",
        "from keras.activations import softmax\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "-BoD2K5jnNeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffd32fc-750d-4f3b-c699-a323ea252255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/My Drive/A4/pos-data/en-ud-train.upos.tsv'\n",
        "dev_file = '/content/drive/My Drive/A4/pos-data/en-ud-dev.upos.tsv'\n",
        "test_file = '/content/drive/My Drive/A4/pos-data/en-ud-test.upos.tsv'\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n"
      ],
      "metadata": {
        "id": "QzNDuv4kqG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement this function if you want to transform the input text, e.g. normalizing case\n"
      ],
      "metadata": {
        "id": "A1nXpu8Is33Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "def transform_text_sequence(seq):\n",
        "    '''\n",
        "    Implement this function if you want to transform the input text,\n",
        "    for example normalizing case.\n",
        "    '''\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "ckEzn6vCs2KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helper Functions (no need to implement)\n",
        "\n"
      ],
      "metadata": {
        "id": "KKRtY7VwsVJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary_and_data(data_file, max_vocab_size=None):\n",
        "    vocab = Counter()\n",
        "    pos_vocab = {'<s>','</s>'}\n",
        "    vocab[UNK] = 1\n",
        "    vocab[PAD] = 1\n",
        "    data = []\n",
        "    gold_labels = []\n",
        "    with open(data_file, 'r', encoding='utf8') as f:\n",
        "        sent = ['<s>']\n",
        "        sent_pos = ['<s>']\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                tok, pos = line.strip().split('\\t')[0], line.strip().split('\\t')[1]\n",
        "                sent.append(tok)\n",
        "                sent_pos.append(pos)\n",
        "                vocab[tok]+=1\n",
        "                vocab['<s>'] += 1\n",
        "                vocab['</s>'] += 1\n",
        "                pos_vocab.add(pos)\n",
        "            elif sent:\n",
        "                sent.append('</s>')\n",
        "                sent_pos.append('</s>')\n",
        "                sent = transform_text_sequence(sent)\n",
        "                data.append(sent)\n",
        "                gold_labels.append(sent_pos)\n",
        "                sent = ['<s>']\n",
        "                sent_pos = ['<s>']\n",
        "    vocab = sorted(vocab.keys(), key = lambda k: vocab[k], reverse=True)\n",
        "    if max_vocab_size:\n",
        "        vocab = vocab[:max_vocab_size-2]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "    return {k:v for v,k in enumerate(vocab)}, list(pos_vocab), data, gold_labels\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, label_set):\n",
        "    vec = [1.0 if l==label else 0.0 for l in label_set]\n",
        "    return np.array(vec)\n",
        "\n",
        "def clean(seqs, vocab, unk):\n",
        "    for i,seq in enumerate(seqs):\n",
        "        for j,tok in enumerate(seq):\n",
        "            if tok>=len(vocab):\n",
        "                seq[j] = unk\n",
        "\n",
        "def batch_generator(data, labels, vocab, label_set, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent, sent_pos in zip(data,labels):\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(label, label_set) for label in sent_pos])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                clean(batch_x, vocab, vocab[UNK])\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, label_set))\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, gold_labels, label_set, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Label:',gold_labels[0])\n",
        "    print('Label count:', len(label_set))\n",
        "    print('Data size', len(data))\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x"
      ],
      "metadata": {
        "id": "EozeUctXrxin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Change these arguments for the main procedure call as needed for your experiments"
      ],
      "metadata": {
        "id": "E6y3DqEJtKgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6 # number of epochs\n",
        "learning_rate = 0.01 # learning rate\n",
        "dropout = 0.3 # dropout rate\n",
        "early_stopping = -1 # early stopping criteria\n",
        "embedding_size = 200 # embedding dimension size\n",
        "hidden_size = 100 # hidden layer size\n",
        "batch_size = 50 # batch size"
      ],
      "metadata": {
        "id": "xgih5BIas_UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the GPU is available"
      ],
      "metadata": {
        "id": "w2LUBOl_tMzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  device_name = '/cpu:0'\n",
        "  print(\n",
        "      '\\n\\n This notebook is not '\n",
        "      'configured to use a GPU.  You can change this in Notebook Settings. Defaulting to:' + device_name)\n",
        "else:\n",
        "  print ('GPU Device found: ' + device_name)"
      ],
      "metadata": {
        "id": "nHsrmiMItM8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494e4e72-cf52-4d2b-a57c-d2638618a38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device found: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main procedure call: Implement the keras model here\n"
      ],
      "metadata": {
        "id": "ogLr4rVev-5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, labels, train_data, train_labels = get_vocabulary_and_data(train_file)\n",
        "_, _, dev_data, dev_labels = get_vocabulary_and_data(dev_file)\n",
        "\n",
        "describe_data(train_data, train_labels, labels,\n",
        "              batch_generator(train_data, train_labels, vocab, labels, batch_size))\n",
        "\n",
        "with tf.device(device_name):\n",
        "\n",
        "    # Implement your model here! ----------------------------------------------------------------------\n",
        "    # Use the variables batch_size, hidden_size, embedding_size, dropout, epochs\n",
        "\n",
        "    pos_tagger = tf.keras.Sequential()\n",
        "    input_size = len(vocab)\n",
        "    output_size = len(labels)\n",
        "\n",
        "    drop_out_e = 0.25\n",
        "    drop_out_lstm = 0.25\n",
        "    drop_out_d = 0.25\n",
        "    pos_tagger.add(Embedding(input_dim = input_size, output_dim = embedding_size))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_e))\n",
        "    pos_tagger.add(Bidirectional(LSTM(hidden_size, return_sequences=True,dropout = drop_out_lstm)))# dropout\n",
        "\n",
        "    pos_tagger.add(TimeDistributed(Dense(output_size, activation='softmax')))\n",
        "    pos_tagger.add(tf.keras.layers.Dropout(drop_out_d))\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    pos_tagger.summary()\n",
        "    optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "    pos_tagger.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print('Epoch',i+1,'/',epochs)\n",
        "        # Training\n",
        "        pos_tagger.fit(batch_generator(train_data, train_labels, vocab, labels, batch_size),\n",
        "                                  epochs=1, steps_per_epoch=len(train_data)/batch_size)\n",
        "        # Evaluation\n",
        "        loss, acc = pos_tagger.evaluate(batch_generator(dev_data, dev_labels, vocab, labels),\n",
        "                                                  steps=len(dev_data))\n",
        "        print('Dev Loss:', loss, 'Dev Acc:', acc)"
      ],
      "metadata": {
        "id": "MR7RBP6atSUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031fb269-5f54-4872-ea6a-bf994416f5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data example: ['<s>', 'Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.', '</s>']\n",
            "Label: ['<s>', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', '</s>']\n",
            "Label count: 19\n",
            "Data size 12543\n",
            "Batch input shape: (50, 62)\n",
            "Batch output shape: (50, 62, 19)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 200)         3935200   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 200)         0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 200)        240800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 19)         3819      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 19)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,179,819\n",
            "Trainable params: 4,179,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 / 6\n",
            "250/250 [==============================] - 59s 192ms/step - loss: 1.5187 - accuracy: 0.2181\n",
            "2002/2002 [==============================] - 10s 4ms/step - loss: 0.2872 - accuracy: 0.9168\n",
            "Dev Loss: 0.28718045353889465 Dev Acc: 0.9168210029602051\n",
            "Epoch 2 / 6\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4470 - accuracy: 0.2436\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.2752 - accuracy: 0.9220\n",
            "Dev Loss: 0.27521800994873047 Dev Acc: 0.922034740447998\n",
            "Epoch 3 / 6\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4296 - accuracy: 0.2468\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.2709 - accuracy: 0.9256\n",
            "Dev Loss: 0.27094700932502747 Dev Acc: 0.9256019592285156\n",
            "Epoch 4 / 6\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 1.4213 - accuracy: 0.2507\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.2899 - accuracy: 0.9258\n",
            "Dev Loss: 0.2898913025856018 Dev Acc: 0.9258421063423157\n",
            "Epoch 5 / 6\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4319 - accuracy: 0.2572\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.3057 - accuracy: 0.9253\n",
            "Dev Loss: 0.3056658208370209 Dev Acc: 0.925258994102478\n",
            "Epoch 6 / 6\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 1.4261 - accuracy: 0.2767\n",
            "2002/2002 [==============================] - 10s 5ms/step - loss: 0.3157 - accuracy: 0.9276\n",
            "Dev Loss: 0.315678209066391 Dev Acc: 0.9275913834571838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, test_data, test_labels = get_vocabulary_and_data(test_file)\n",
        "\n",
        "loss, acc = pos_tagger.evaluate(batch_generator(test_data, test_labels, vocab, labels),\n",
        "                                                  steps=len(test_data))\n",
        "print('Test Loss:', loss, 'Test Acc:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_DpidqK31xD",
        "outputId": "cf8f07f2-edcd-4171-e559-37c63fcc24c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2077/2077 [==============================] - 10s 5ms/step - loss: 0.3249 - accuracy: 0.9258\n",
            "Test Loss: 0.3248816728591919 Test Acc: 0.925848662853241\n"
          ]
        }
      ]
    }
  ]
}