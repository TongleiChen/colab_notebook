{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CMmmnmeURxvMjiYGUKIdk-DtpghSg1cR",
      "authorship_tag": "ABX9TyPMDCnU4iLKjUOdS8bdzVMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongleiChen/sketch_to_image/blob/main/1_layer_neural_network___.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7gEOAZdk3vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TZUcHGtEc_7t"
      },
      "outputs": [],
      "source": [
        "#@title Import library\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title load image data model\n",
        "image_model_path = \"/content/drive/MyDrive/kaggle/imagenet/vgg16_pretrained_realworld_epoch8_loss_lr_001_1209.pth\"\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_model=torch.load(image_model_path)\n",
        "image_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNszVZDGGinN",
        "outputId": "c72b4fe0-8ecb-4f08-d7b6-74f9f20c2ed1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ImageDataset class\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, animal_category, image_size = 255, class_size = 100,transform = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sketch_dir (string): Directory to all the sketch images.\n",
        "            realworld_dir (string): Directory to all the real world images.\n",
        "            animal_category: list to fruit catogory\n",
        "            class_size: Num of images in each category\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.animal_category = animal_category\n",
        "        self.class_size = class_size\n",
        "        self.data_dict = dict(np.load(img_dir,allow_pickle=True))\n",
        "        self.image_size = image_size\n",
        "       \n",
        "        self.transform_img = transforms.Compose([transforms.ToPILImage(),\n",
        "                                                transforms.Resize((image_size,image_size)),\n",
        "                                                transforms.ToTensor(),])\n",
        "                                              # transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n",
        "        self.transform_label = transforms.Compose([transforms.ToTensor()])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.class_size * len(self.animal_category)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        class_index = int(idx // self.class_size)\n",
        "        category =  self.animal_category[class_index]\n",
        "        category_idx = int(idx % self.class_size)\n",
        "        label = np.zeros((len(self.animal_category), 1))\n",
        "        label[class_index] = 1\n",
        "        \n",
        "#         label = class_index\n",
        "        image_ary =  self.data_dict[category][category_idx]\n",
        "        sample = {'image': image_ary, 'label': label}\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform_img(sample['image'])\n",
        "            sample['label'] = self.transform_label(sample['label'])\n",
        "        return sample\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "_Wp1PI2znLNY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get search image data\n",
        "QURIES = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]  \n",
        "\n",
        "search_realworld_dir = \"/content/drive/MyDrive/kaggle/imagenet/all_images.npz\"\n",
        "search_realworld = ImageDataset(search_realworld_dir, QURIES, image_size = 224, class_size = 100,transform = True)\n",
        "search_loader_realworld = DataLoader(search_realworld, batch_size=128, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "8GSrsMmTHRWN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "\n",
        "outputs_= []\n",
        "def hook(module, input, output):\n",
        "\n",
        "    outputs_.append(output.detach())\n",
        "\n",
        "image_model.classifier[5].register_forward_hook(hook)\n",
        "# output = model()\n",
        "# print(activation['fc2'])\n",
        "features = defaultdict(list)\n",
        "feature_average = []\n",
        "with torch.no_grad():\n",
        "    for data in search_loader_realworld:\n",
        "        \n",
        "        data['label'] = torch.argmax(data['label'].squeeze(),dim = 1)\n",
        "        images, labels = data['image'].to(device), data['label'].to(device)\n",
        "        outputs = image_model(images)\n",
        "        labels_sq = labels.tolist()\n",
        "        \n",
        "        for i in range(outputs.shape[0]):\n",
        "          features[labels_sq[i]].append(outputs_[-1][i])\n",
        "          # print(len(outputs_))\n",
        "          # print(type(outputs_[-1][i]))\n",
        "\n",
        "\n",
        "    \n",
        "    for i in range(len(features)):\n",
        "      features_real = torch.empty((100,4096))\n",
        "      features_real[0:100] = torch.stack(features[i])\n",
        "      feature_average.append(features_real)"
      ],
      "metadata": {
        "id": "utA1fXgNMt_l"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_real_image_ = torch.empty((100*10,4096))\n",
        "for i in range(len(features)):\n",
        "  feature_real_image_[i*100:(i+1)*100] = torch.stack(features[i])"
      ],
      "metadata": {
        "id": "ENZNlxCxuFtk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_real_image_.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0RAuNfDutih",
        "outputId": "61576f75-d031-4ebc-c586-55a4c61efbe5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zQ9suaKuwNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_ = {}\n",
        "for i in range(len(feature_average)):\n",
        "  avg_[i] = torch.mean(feature_average[i],dim = 0)\n",
        "avg_image_feature = avg_[0].repeat(100,1)\n",
        "\n",
        "for i in range(1,len(avg_)):\n",
        "\n",
        "    avg_image_feature = torch.cat((avg_image_feature, avg_[i].repeat(100,1)), 0)"
      ],
      "metadata": {
        "id": "iegySBe3pHdK"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_image_feature.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4yAIsf9rWku",
        "outputId": "bdcc1bfa-6e1f-4f3d-b953-a77bee0a1cfa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_image_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6mcyEkb-iFR",
        "outputId": "911c6481-0992-48ba-8cae-aadbc2412d5b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1862, 0.0017,  ..., 0.1262, 0.4323, 0.1128],\n",
              "        [0.0000, 0.1862, 0.0017,  ..., 0.1262, 0.4323, 0.1128],\n",
              "        [0.0000, 0.1862, 0.0017,  ..., 0.1262, 0.4323, 0.1128],\n",
              "        ...,\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_image_feature = avg_image_feature\n",
        "image_sketch_label = [(7,0),\n",
        "                      (17,1),\n",
        "                      (18,55),\n",
        "                      (4,68),\n",
        "                      (8,87),\n",
        "                      (4,40),\n",
        "                      (33,90),\n",
        "                      (35,93),\n",
        "                      (41,85),\n",
        "                      (43,65),\n",
        "                      (46,58),\n",
        "                      (49,27),\n",
        "                      (60,33),\n",
        "                      (60,38),\n",
        "                      (49,67),\n",
        "                      (65,55),\n",
        "                      (69,74),\n",
        "                      (98,99),\n",
        "                      (87,98),\n",
        "                      (52,72)]\n",
        "for change in range(len(image_sketch_label)):\n",
        "    image_label, sketch_label = image_sketch_label[change]\n",
        "    new_image_feature[sketch_label] = feature_real_image_[image_label]"
      ],
      "metadata": {
        "id": "YVabZs2Hf5Tl"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2VQgY4t-eGV",
        "outputId": "76129dd8-a2c4-4d0c-d602-eed176d9635a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.2938, 0.0000,  ..., 0.0000, 0.0000, 0.0300],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.5224, 0.0000],\n",
              "        [0.0000, 0.1862, 0.0017,  ..., 0.1262, 0.4323, 0.1128],\n",
              "        ...,\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_2 = [(0,14),\n",
        "           (1,15),\n",
        "           (11,18),\n",
        "           (12,24),\n",
        "           (14,28),\n",
        "           (17,33),\n",
        "           (30,54),\n",
        "           (56,70),\n",
        "           (61,74),\n",
        "           (67,79),\n",
        "           (70,83),\n",
        "           (68,91),\n",
        "           (68,99),\n",
        "           (72,42),\n",
        "           (75,19),\n",
        "           (83,28),\n",
        "           (86,40),\n",
        "           (91,42),\n",
        "           (93,45),\n",
        "           (88,32)\n",
        "           ]\n",
        "for change in range(len(label_2)):\n",
        "    image_label, sketch_label = label_2[change]\n",
        "    new_image_feature[sketch_label+100] = feature_real_image_[image_label+100]\n",
        "\n",
        "label_3 = [(5,2),\n",
        "           (8,9),\n",
        "           (10,10),\n",
        "           (42,21),\n",
        "           (46,24),\n",
        "           (63,25),\n",
        "           (64,27),\n",
        "           (66,28),\n",
        "           (71,55),\n",
        "           (82,64),\n",
        "           (86,66),\n",
        "           (91,71),\n",
        "           (95,84),\n",
        "           (90,95),\n",
        "           (97,98),\n",
        "           (98,99),\n",
        "           (72,8),\n",
        "           (76,13),\n",
        "           (1,0),\n",
        "           (73,56),\n",
        "]\n",
        "\n",
        "for change in range(len(label_3)):\n",
        "    image_label, sketch_label = label_3[change]\n",
        "    new_image_feature[sketch_label+100*2] = feature_real_image_[image_label+100*2]\n",
        "\n",
        "\n",
        "\n",
        "label_4 = [(5,6),\n",
        "           (9,1),\n",
        "           (12,5),\n",
        "           (20,9),\n",
        "           (22,10),\n",
        "           (26,12),\n",
        "           (27,13),\n",
        "           (38,18),\n",
        "           (42,19),\n",
        "           (47,26),\n",
        "           (55,26),\n",
        "           (62,27),\n",
        "           (67,32),\n",
        "           (68,48),\n",
        "           (71,52),\n",
        "           (59,56),\n",
        "           (46,61),\n",
        "           (36,71),\n",
        "           (40,86),\n",
        "           (55,93),\n",
        "\n",
        "]\n",
        "for change in range(len(label_4)):\n",
        "    image_label, sketch_label = label_4[change]\n",
        "    new_image_feature[sketch_label+100*3] = feature_real_image_[image_label+100*3]\n",
        "\n",
        "label_5 = [(84,8),\n",
        "           (69,10),\n",
        "           (72,13),\n",
        "           (54,23),\n",
        "           (61,29),\n",
        "           (64,33),\n",
        "           (45,36),\n",
        "           (40,38),\n",
        "           (30,43),\n",
        "           (47,62),\n",
        "           (20,65),\n",
        "           (9,71),\n",
        "           (6,72),\n",
        "           (23,83),\n",
        "           (28,84),\n",
        "           (31,88),\n",
        "           (33,91),\n",
        "           (37,94),\n",
        "           (41,97),\n",
        "           (28,70),\n",
        "\n",
        "]\n",
        "for change in range(len(label_5)):\n",
        "    image_label, sketch_label = label_5[change]\n",
        "    new_image_feature[sketch_label+100*4] = feature_real_image_[image_label+100*4]\n",
        "\n",
        "label_6 = [(2,27),\n",
        "           (19,28),\n",
        "           (15,30),\n",
        "           (18,31),\n",
        "           (23,34),\n",
        "           (28,37),\n",
        "           (31,40),\n",
        "           (44,49),\n",
        "           (66,54),\n",
        "           (52,58),\n",
        "           (61,70),\n",
        "           (65,72),\n",
        "           (64,83),\n",
        "           (67,89),\n",
        "           (61,75),\n",
        "           (38,38),\n",
        "           (44,24),\n",
        "           (46,28),\n",
        "           (38,50),\n",
        "           (23,57),\n",
        "\n",
        "]\n",
        "for change in range(len(label_6)):\n",
        "    image_label, sketch_label = label_6[change]\n",
        "    new_image_feature[sketch_label+100*5] = feature_real_image_[image_label+100*5]\n",
        "\n",
        "label_7 = [(46,25),\n",
        "           (21,2),\n",
        "           (24,4),\n",
        "           (26,6),\n",
        "           (28,7),\n",
        "           (34,11),\n",
        "           (26,71),\n",
        "           (14,77),\n",
        "           (74,35),\n",
        "           (65,50),\n",
        "           (61,56),\n",
        "           (63,59),\n",
        "           (26,63),\n",
        "           (21,64),\n",
        "           (24,68),\n",
        "           (0,74),\n",
        "           (26,71),\n",
        "           (16,78),\n",
        "           (20,80),\n",
        "           (20,84),\n",
        "\n",
        "]\n",
        "for change in range(len(label_7)):\n",
        "    image_label, sketch_label = label_7[change]\n",
        "    new_image_feature[sketch_label+100*6] = feature_real_image_[image_label+100*6]\n",
        "\n",
        "label_8 = [(81,67),\n",
        "           (90,0),\n",
        "           (99,9),\n",
        "           (88,13),\n",
        "           (61,29),\n",
        "           (62,40),\n",
        "           (64,44),\n",
        "           (69,56),\n",
        "           (74,63),\n",
        "           (88,69),\n",
        "           (90,78),\n",
        "           (90,86),\n",
        "           (90,97),\n",
        "           (81,82),\n",
        "           (79,65),\n",
        "           (69,46),\n",
        "           (70,48),\n",
        "           (69,62),\n",
        "           (70,55),\n",
        "           (74,66),\n",
        "\n",
        "]\n",
        "for change in range(len(label_8)):\n",
        "    image_label, sketch_label = label_8[change]\n",
        "    new_image_feature[sketch_label+100*7] = feature_real_image_[image_label+100*7]\n",
        "\n",
        "label_9 = [(80,3),\n",
        "           (77,6),\n",
        "           (76,14),\n",
        "           (84,32),\n",
        "           (54,33),\n",
        "           (46,43),\n",
        "           (62,57),\n",
        "           (86,89),\n",
        "           (89,98),\n",
        "           (77,65),\n",
        "           (76,70),\n",
        "           (77,81),\n",
        "           (82,87),\n",
        "           (53,10),\n",
        "           (54,4),\n",
        "           (65,23),\n",
        "           (67,34),\n",
        "           (67,37),\n",
        "           (69,41),\n",
        "           (64,46),\n",
        "\n",
        "]\n",
        "for change in range(len(label_9)):\n",
        "    image_label, sketch_label = label_9[change]\n",
        "    new_image_feature[sketch_label+100*8] = feature_real_image_[image_label+100*8]\n",
        "\n",
        "label_10 = [(18,1),\n",
        "           (2,3),\n",
        "           (8,4),\n",
        "           (8,5),\n",
        "           (18,10),\n",
        "           (21,12),\n",
        "           (33,28),\n",
        "           (42,33),\n",
        "           (46,29),\n",
        "           (66,37),\n",
        "           (81,43),\n",
        "           (85,49),\n",
        "           (90,53),\n",
        "           (91,57),\n",
        "           (99,60),\n",
        "           (91,67),\n",
        "           (83,69),\n",
        "           (59,72),\n",
        "           (64,75),\n",
        "           (59,85),\n",
        "\n",
        "]\n",
        "for change in range(len(label_10)):\n",
        "    image_label, sketch_label = label_10[change]\n",
        "    new_image_feature[sketch_label+100*9] = feature_real_image_[image_label+100*9]\n"
      ],
      "metadata": {
        "id": "__mqMTsE7qL7"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image_feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsq6q3QGinJO",
        "outputId": "09d9f40b-a817-426a-a353-f79bd8bc8eb7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.2938, 0.0000,  ..., 0.0000, 0.0000, 0.0300],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.5224, 0.0000],\n",
              "        [0.0000, 0.1862, 0.0017,  ..., 0.1262, 0.4323, 0.1128],\n",
              "        ...,\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802],\n",
              "        [0.0149, 0.1908, 0.0049,  ..., 0.0751, 0.1730, 0.0802]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BD-vMOmy7fv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "from io import BytesIO # Use When expecting bytes-like objects\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from os import path\n",
        "import ast\n",
        "import random\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from torch_snippets import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Image manipulation utilities: \n",
        "\n",
        "def convert_to_PIL(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert from drawing to PIL image.\n",
        "    INPUT:\n",
        "        drawing - drawing from 'drawing' column\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        pil_img - (PIL Image) image\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize empty (white) PIL image\n",
        "    pil_img = Image.new('RGB', (width, height), 'white')\n",
        "    pixels = pil_img.load()\n",
        "            \n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    \n",
        "    # draw strokes as lines\n",
        "    for x,y in drawing:\n",
        "        for i in range(1, len(x)):\n",
        "            draw.line((x[i-1], y[i-1], x[i], y[i]), fill=0)\n",
        "        \n",
        "    return pil_img\n",
        "\n",
        "\n",
        "def convert_to_np_raw(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        drawing - drawing in initial format\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        img - drawing converted to the numpy array (28 X 28)\n",
        "    \"\"\"\n",
        "    # initialize empty numpy array\n",
        "    img = np.zeros((28, 28))\n",
        "    \n",
        "    # create a PIL image out of drawing\n",
        "    pil_img = convert_to_PIL(drawing)\n",
        "    \n",
        "    #resize to 28,28\n",
        "    pil_img.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    pil_img = pil_img.convert('RGB')\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    # fill in numpy array with pixel values\n",
        "    for i in range(0, 28):\n",
        "        for j in range(0, 28):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "    \n",
        "    return img\n",
        "\n",
        "def convert_to_np(pil_img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert PIL Image to numpy array.\n",
        "    INPUT:\n",
        "        pil_img - (PIL Image) image to be converted\n",
        "    OUTPUT:\n",
        "        img - (numpy array) converted image with shape (width, height)\n",
        "    \"\"\"\n",
        "    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "    img = np.zeros((width, height))\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    for i in range(0, width):\n",
        "      for j in range(0, height):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def view_image(img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to view numpy image with matplotlib.\n",
        "    The function saves the image as png.\n",
        "    INPUT:\n",
        "        img - (numpy array) image from train dataset with size (1, 784)\n",
        "    OUTPUT:\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,9))\n",
        "    ax.imshow(img.reshape(width, height).squeeze())\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "def crop_image(image):\n",
        "    \"\"\"\n",
        "    Crops image (crops out white spaces).\n",
        "    INPUT:\n",
        "        image - PIL image of original size to be cropped\n",
        "    OUTPUT:\n",
        "        cropped_image - PIL image cropped to the center  and resized to (28, 28)\n",
        "    \"\"\"\n",
        "    cropped_image = image\n",
        "\n",
        "    # get image size\n",
        "    width, height = cropped_image.size\n",
        "\n",
        "    # get image pixels\n",
        "    pixels = cropped_image.load()\n",
        "\n",
        "    image_strokes_rows = []\n",
        "    image_strokes_cols = []\n",
        "\n",
        "    # run through the image\n",
        "    for i in range(0, width):\n",
        "        for j in range(0, height):\n",
        "            # save coordinates of the image\n",
        "            if (pixels[i,j][0] > 0):\n",
        "                image_strokes_cols.append(i)\n",
        "                image_strokes_rows.append(j)\n",
        "\n",
        "    # if image is not empty then crop to contents of the image\n",
        "    if (len(image_strokes_rows)) > 0:\n",
        "        # find the box for image\n",
        "        row_min = np.array(image_strokes_rows).min()\n",
        "        row_max = np.array(image_strokes_rows).max()\n",
        "        col_min = np.array(image_strokes_cols).min()\n",
        "        col_max = np.array(image_strokes_cols).max()\n",
        "\n",
        "        # find the box for cropping\n",
        "        margin = min(row_min, height - row_max, col_min, width - col_max)\n",
        "\n",
        "        # crop image\n",
        "        border = (col_min, row_min, width - col_max, height - row_max)\n",
        "        cropped_image = ImageOps.crop(cropped_image, border)\n",
        "\n",
        "    # get cropped image size\n",
        "    width_cropped, height_cropped = cropped_image.size\n",
        "\n",
        "    # create square resulting image to paste cropped image into the center\n",
        "    dst_im = Image.new(\"RGBA\", (max(width_cropped, height_cropped), max(width_cropped, height_cropped)), \"white\")\n",
        "    offset = ((max(width_cropped, height_cropped) - width_cropped) // 2, (max(width_cropped, height_cropped) - height_cropped) // 2)\n",
        "    # paste to the center of a resulting image\n",
        "    dst_im.paste(cropped_image, offset)\n",
        "\n",
        "    #resize to 28,28\n",
        "    dst_im.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    return dst_im\n",
        "def normalize(arr):\n",
        "    \"\"\"\n",
        "    Function performs the linear normalizarion of the array.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
        "    INPUT:\n",
        "        arr - orginal numpy array\n",
        "    OUTPUT:\n",
        "        arr - normalized numpy array\n",
        "    \"\"\"\n",
        "    arr = arr.astype('float')\n",
        "    # Do not touch the alpha channel\n",
        "    for i in range(3):\n",
        "        minval = arr[...,i].min()\n",
        "        maxval = arr[...,i].max()\n",
        "        if minval != maxval:\n",
        "            arr[...,i] -= minval\n",
        "            arr[...,i] *= (255.0/(maxval-minval))\n",
        "    return arr\n",
        "\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Function performs the normalization of the image.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    INPUT:\n",
        "        image - PIL image to be normalized\n",
        "    OUTPUT:\n",
        "        new_img - PIL image normalized\n",
        "    \"\"\"\n",
        "    arr = np.array(image)\n",
        "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGBA')\n",
        "    return new_img\n",
        "\n",
        "def rotate_image(src_im, angle = 45, size = (28,28)):\n",
        "    \"\"\"\n",
        "    Function to rotate PIL Image file\n",
        "    INPUT:\n",
        "        src_im - (PIL Image) 28x28 image to be rotated\n",
        "        angle - angle to rotate the image\n",
        "        size - (tuple) size of the output image\n",
        "    OUTPUT:\n",
        "    dst_im - (PIL Image) rotated image\n",
        "    \"\"\"\n",
        "    dst_im = Image.new(\"RGBA\", size, \"white\")\n",
        "    src_im = src_im.convert('RGBA')\n",
        "\n",
        "    rot = src_im.rotate(angle)\n",
        "    dst_im.paste(rot, (0, 0), rot)\n",
        "\n",
        "    return dst_im\n",
        "def flip_image(src_im):\n",
        "    \"\"\"\n",
        "    Function to flip a PIL Image file.\n",
        "    INPUT:\n",
        "        scr_im - (PIL Image) image to be flipped\n",
        "    OUTPUT:\n",
        "        dst_im - (PIL Image) flipped image\n",
        "    \"\"\"\n",
        "    dst_im = src_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return dst_im\n",
        "\n",
        "\n",
        "## sketch based on LeNet \n",
        "def resize(x, kernel_size, dilation, stride, padding):\n",
        "    x = int(1 + (x + 2*padding - dilation * (kernel_size - 1) - 1)/stride)\n",
        "    return x\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, specs, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.specs = specs\n",
        "        H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding = specs\n",
        "        pooling = 2\n",
        "        #pooling = 1 # skips pooling\n",
        "        stride = 1\n",
        "        dilation = 1\n",
        "\n",
        "        C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "        C1=26\n",
        "        C2=32\n",
        "        kernel_size=5\n",
        "        padding=0\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "                    nn.Conv2d(C0, C1, kernel_size=kernel_size, padding=padding), nn.LeakyReLU(), nn.Dropout(dropout),\n",
        "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "                    nn.Conv2d(C1, C2, kernel_size=kernel_size, padding=padding), nn.LeakyReLU(), nn.Dropout(dropout),\n",
        "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(512, 256), nn.LeakyReLU(), nn.Dropout(dropout),\n",
        "                    nn.Linear(256, 128), nn.LeakyReLU(), nn.Dropout(dropout),\n",
        "                    nn.Linear(128, 64), nn.LeakyReLU(), nn.Dropout(dropout),\n",
        "                    nn.Linear(64, 10))\n",
        "\n",
        "        for m in self.modules():\n",
        "            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return self.net(input_data)\n",
        "\n",
        "def build_model(input_size, output_size, hidden_sizes, dropout = 0.0):\n",
        "    '''\n",
        "    Function creates deep learning model based on parameters passed.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - layer sizes\n",
        "        dropout - dropout (probability of keeping a node)\n",
        "\n",
        "    OUTPUT:\n",
        "        model - deep learning model\n",
        "    '''\n",
        "\n",
        "    # Build a feed-forward network\n",
        "    #modelCNN = CNNModel()\n",
        "\n",
        "    H=28    # don't change -- actual images are 28x28, not 32x32 -- H = height of image\n",
        "    W=28    # don't change -- actual images are 28x28, not 32x32 -- W = width of image\n",
        "    C0=1    # don't change -- # input channels -- 1 gray scale channel\n",
        "    C1=6\n",
        "    C2=16\n",
        "    kernel_size=5\n",
        "    F1 = 120\n",
        "    F2 = 84\n",
        "    nDigits=10    # don't change -- # outputs -- 10 digits\n",
        "    padding=2\n",
        "    specs = [H, W, C0, C1, C2, kernel_size, F1, F2, nDigits, padding]\n",
        "    modelLeNet = LeNet(specs, dropout=0.1)\n",
        "    \n",
        "    return modelLeNet\n",
        "\n",
        "\n",
        "\n",
        "def fit_model(model, X_train, y_train, epochs = 100, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function which fits the model.\n",
        "    INPUT:\n",
        "        model - pytorch model to fit\n",
        "        X_train - (tensor) train dataset\n",
        "        y_train - (tensor) train dataset labels\n",
        "        epochs - number of epochs\n",
        "        n_chunks - number of chunks to cplit the dataset\n",
        "        learning_rate - learning rate value\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    print(\"Fitting model with epochs = {epochs}, learning rate = {lr}\\n\".format(epochs = epochs, lr = learning_rate))\n",
        "    \n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if (optimizer == 'SGD'):\n",
        "      optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay)\n",
        "\n",
        "    print_every = 10\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "        images = torch.chunk(X_train, n_chunks)\n",
        "        labels = torch.chunk(y_train, n_chunks)\n",
        "\n",
        "        for i in range(n_chunks):\n",
        "            steps += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward passes\n",
        "            output = model.forward(images[i])\n",
        "            loss = criterion(output, labels[i].squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        if epochs % print_every == 0:\n",
        "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
        "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
        "            running_loss = 0\n",
        "\n",
        "                            \n",
        "def view_classify(img, ps):\n",
        "    \"\"\"\n",
        "    Function for viewing an image and it's predicted classes\n",
        "    with matplotlib.\n",
        "\n",
        "    INPUT:\n",
        "        img - (tensor) image file\n",
        "        ps - (tensor) predicted probabilities for each class\n",
        "    \"\"\"\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    ax2.set_yticklabels([\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"], size='small');\n",
        "    ax2.set_title('Class Probability')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_model(model, img):\n",
        "    \"\"\"\n",
        "    Function creates test view of the model's prediction for image.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        img - (tensor) image from the dataset\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert 2D image to 1D vector\n",
        "    img = img.resize_(1, 784)\n",
        "\n",
        "    ps = get_preds(model, img)\n",
        "    view_classify(img.resize_(1, 28, 28), ps)\n",
        "\n",
        "\n",
        "def get_preds(model, input):\n",
        "    \"\"\"\n",
        "    Function to get predicted probabilities from the model for each class.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        input - (tensor) input vector\n",
        "\n",
        "    OUTPUT:\n",
        "        ps - (tensor) vector of predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Turn off gradients to speed up this part\n",
        "    with torch.no_grad():\n",
        "        logits = model.forward(input)\n",
        "    ps = F.softmax(logits, dim=1)\n",
        "    return ps\n",
        "\n",
        "def get_labels(pred):\n",
        "    \"\"\"\n",
        "        Function to get the vector of predicted labels for the images in\n",
        "        the dataset.\n",
        "\n",
        "        INPUT:\n",
        "            pred - (tensor) vector of predictions (probabilities for each class)\n",
        "        OUTPUT:\n",
        "            pred_labels - (numpy) array of predicted classes for each vector\n",
        "    \"\"\"\n",
        "\n",
        "    pred_np = pred.numpy()\n",
        "    pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
        "    pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
        "    pred_labels = pred_labels.reshape(len(pred_np), 1)\n",
        "\n",
        "    return pred_labels\n",
        "\n",
        "def evaluate_model(model, train, y_train, test, y_test):\n",
        "    \"\"\"\n",
        "    Function to print out train and test accuracy of the model.\n",
        "\n",
        "    INPUT:\n",
        "        model - pytorch model\n",
        "        train - (tensor) train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "\n",
        "    OUTPUT:\n",
        "        accuracy_train - accuracy on train dataset\n",
        "        accuracy_test - accuracy on test dataset\n",
        "    \"\"\"\n",
        "    train_pred = get_preds(model, train)\n",
        "    train_pred_labels = get_labels(train_pred)\n",
        "\n",
        "    test_pred = get_preds(model, test)\n",
        "    test_pred_labels = get_labels(test_pred)\n",
        "\n",
        "    accuracy_train = accuracy_score(y_train, train_pred_labels)\n",
        "    accuracy_test = accuracy_score(y_test, test_pred_labels)\n",
        "\n",
        "    print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
        "    print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
        "\n",
        "    return accuracy_train, accuracy_test\n",
        "\n",
        "def plot_learning_curve(input_size, output_size, hidden_sizes, train, labels, y_train, test, y_test, learning_rate = 0.003, weight_decay = 0.0, dropout = 0.0, n_chunks = 1000, optimizer = 'SGD'):\n",
        "    \"\"\"\n",
        "    Function to plot learning curve depending on the number of epochs.\n",
        "\n",
        "    INPUT:\n",
        "        input_size, output_size, hidden_sizes - model parameters\n",
        "        train - (tensor) train dataset\n",
        "        labels - (tensor) labels for train dataset\n",
        "        y_train - (numpy) labels for train dataset\n",
        "        test - (tensor) test dataset\n",
        "        y_test - (numpy) labels for test dataset\n",
        "        learning_rate - learning rate hyperparameter\n",
        "        weight_decay - weight decay (regularization)\n",
        "        dropout - dropout for hidden layer\n",
        "        n_chunks - the number of minibatches to train the model\n",
        "        optimizer - optimizer to be used for training (SGD or Adam)\n",
        "\n",
        "    OUTPUT: None\n",
        "    \"\"\"\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for epochs in np.arange(10, 60, 10):\n",
        "        # create model\n",
        "        sketch_model = build_model(input_size, output_size, hidden_sizes, dropout = dropout)\n",
        "\n",
        "        # fit model\n",
        "        fit_model(sketch_model, train, labels, epochs = epochs, n_chunks = n_chunks, learning_rate = learning_rate, weight_decay = weight_decay, optimizer = 'SGD')\n",
        "        # get accuracy\n",
        "        accuracy_train, accuracy_test = evaluate_model(sketch_model, train, y_train, test, y_test)\n",
        "\n",
        "        train_acc.append(accuracy_train)\n",
        "        test_acc.append(accuracy_test)\n",
        "\n",
        "    \n",
        "    return train_acc, test_acc, sketch_model\n",
        "\n",
        "## My trial:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch import optim,nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "###############################################################################\n",
        "import time, os, sys, random, datetime\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class DatasetFromCSV(Dataset):\n",
        "    def __init__(self,datas,labels,height,width,transforms=None):\n",
        "        #self.data = pd.read_csv(csv_path)\n",
        "        self.data = datas\n",
        "        self.labels = labels\n",
        "        #view_images_grid(self.data, self.labels)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        '''\n",
        "        # lzx's trial:(getitemmlgbz)\n",
        "        lzx_image=self.data[index].reshape(28,28)\n",
        "        #fig,axs=plt.subplots(1,1,figsize=(5,5))\n",
        "        plt.imshow(lzx_image)\n",
        "        plt.title(label_dict[self.labels[index]],loc=\"left\")\n",
        "        plt.show()\n",
        "        # mlgbz\n",
        "        '''\n",
        "        #  1D array ([784]) reshape  2D array ([28,28])\n",
        "        #img_as_np = np.asarray(self.data[index][:]).reshape(28, 28).astype(float)\n",
        "        img_as_np = np.asarray(self.data[index]).reshape(28, 28).astype(float)\n",
        "        #  numpy array  PIL image\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        #img_as_img = img_as_img.convert('L')\n",
        "        #  tensor\n",
        "        if self.transforms is not None:\n",
        "            img_as_tensor = self.transforms(img_as_img)\n",
        "            #  label\n",
        "        return (img_as_tensor, single_image_label)\n",
        " \n",
        "    def __len__(self):\n",
        "        #datacopy=self.data.copy().tolist()\n",
        "        #return len(datacopy.index)\n",
        "        return len(self.data)\n",
        " \n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "      return arr.cuda()\n",
        "    return arr\n",
        "def train_LeNet(model, train_loader, test_loader):\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # define the loss functions\n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    # choose an optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "         mode = 'max', patience = 3, \n",
        "         min_lr = learning_rate * 0.001)\n",
        "\n",
        "    start = time.time()\n",
        "    w_decay = 0.95 # smoothing factor for reporting results\n",
        "\n",
        "    for e in range(nEpochs):\n",
        "        total_train_images = 0\n",
        "        total_train_loss = 0\n",
        "        train_images = 0\n",
        "        train_loss = 0\n",
        "        w_images = 0\n",
        "        w_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        tqdm_ = tqdm(iterable=train_loader, mininterval=1, ncols=100)\n",
        "        # for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for sample in tqdm_:\n",
        "            data, target = sample\n",
        "            data, target = cuda(data), cuda(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_images += len(data)\n",
        "            train_loss += loss.data.item()\n",
        "\n",
        "            if train_images > log_interval:\n",
        "                total_train_images += train_images\n",
        "                total_train_loss += train_loss\n",
        "                if w_images == 0:\n",
        "                    w_loss = train_loss\n",
        "                    w_images = train_images\n",
        "                else:\n",
        "                    w_images = w_decay * w_images + train_images\n",
        "                    w_loss = w_decay * w_loss + train_loss\n",
        "                \n",
        "                #print(\"epoch %3d total train images: %8d loss: %8.3f loss: %8.3f     time: %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "\n",
        "                train_images = 0\n",
        "                train_loss = 0\n",
        "                #break\n",
        "\n",
        "        test_images = 0\n",
        "        test_loss = 0\n",
        "        nCorrect = 0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                data, target = cuda(data), cuda(target)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                test_images += len(data)\n",
        "                test_loss += loss.data.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max value\n",
        "                nCorrect += pred.eq(target.view_as(pred)).sum().item() # count correct items\n",
        "        # print(\"================================================================================================\")\n",
        "        # print(\"%3d %8d %8.3f %8.3f %8.3f %8.1f%%     %6.1f\" % (e, (e+1)*total_train_images, total_train_loss/total_train_images, w_loss/w_images, test_loss/test_images, 100*nCorrect/test_images, (time.time()-start)))\n",
        "\n",
        "        print(\"================================================================================================\")\n",
        "        print(f'current learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "        print(f'epoch {e}:  training loss {round(total_train_loss/total_train_images, 4) }, \\\n",
        "            test loss {round(test_loss/test_images, 4) }, accuracy {round(100*nCorrect/test_images, 4)}')\n",
        "\n",
        "\n",
        "        scheduler.step(round(100*nCorrect/test_images, 4))\n",
        "\n",
        "        \n",
        "    return model"
      ],
      "metadata": {
        "id": "sZCp0cXzSt8r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_model_path = \"/content/drive/MyDrive/kaggle/sketch\"+'/LeNet_model_sketch_epoch80_70.pth'\n",
        "sketch_model=torch.load(sketch_model_path)"
      ],
      "metadata": {
        "id": "JdgksG8uU5GV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image manipulation utilities: \n",
        "\n",
        "def convert_to_PIL(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert from drawing to PIL image.\n",
        "    INPUT:\n",
        "        drawing - drawing from 'drawing' column\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        pil_img - (PIL Image) image\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize empty (white) PIL image\n",
        "    pil_img = Image.new('RGB', (width, height), 'white')\n",
        "    pixels = pil_img.load()\n",
        "            \n",
        "    draw = ImageDraw.Draw(pil_img)\n",
        "    \n",
        "    # draw strokes as lines\n",
        "    for x,y in drawing:\n",
        "        for i in range(1, len(x)):\n",
        "            draw.line((x[i-1], y[i-1], x[i], y[i]), fill=0)\n",
        "        \n",
        "    return pil_img\n",
        "\n",
        "\n",
        "def convert_to_np_raw(drawing, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        drawing - drawing in initial format\n",
        "        width - width of the initial image\n",
        "        height - height of the initial image\n",
        "    OUTPUT:\n",
        "        img - drawing converted to the numpy array (28 X 28)\n",
        "    \"\"\"\n",
        "    # initialize empty numpy array\n",
        "    img = np.zeros((28, 28))\n",
        "    \n",
        "    # create a PIL image out of drawing\n",
        "    pil_img = convert_to_PIL(drawing)\n",
        "    \n",
        "    #resize to 28,28\n",
        "    pil_img.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    pil_img = pil_img.convert('RGB')\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    # fill in numpy array with pixel values\n",
        "    for i in range(0, 28):\n",
        "        for j in range(0, 28):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "    \n",
        "    return img\n",
        "\n",
        "def convert_to_np(pil_img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to convert PIL Image to numpy array.\n",
        "    INPUT:\n",
        "        pil_img - (PIL Image) image to be converted\n",
        "    OUTPUT:\n",
        "        img - (numpy array) converted image with shape (width, height)\n",
        "    \"\"\"\n",
        "    pil_img = pil_img.convert('RGB')\n",
        "\n",
        "    img = np.zeros((width, height))\n",
        "    pixels = pil_img.load()\n",
        "\n",
        "    for i in range(0, width):\n",
        "      for j in range(0, height):\n",
        "            img[i, j] = 1 - pixels[j, i][0] / 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def view_image(img, width = 256, height = 256):\n",
        "    \"\"\"\n",
        "    Function to view numpy image with matplotlib.\n",
        "    The function saves the image as png.\n",
        "    INPUT:\n",
        "        img - (numpy array) image from train dataset with size (1, 784)\n",
        "    OUTPUT:\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6,9))\n",
        "    ax.imshow(img.reshape(width, height).squeeze())\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "def crop_image(image):\n",
        "    \"\"\"\n",
        "    Crops image (crops out white spaces).\n",
        "    INPUT:\n",
        "        image - PIL image of original size to be cropped\n",
        "    OUTPUT:\n",
        "        cropped_image - PIL image cropped to the center  and resized to (28, 28)\n",
        "    \"\"\"\n",
        "    cropped_image = image\n",
        "\n",
        "    # get image size\n",
        "    width, height = cropped_image.size\n",
        "\n",
        "    # get image pixels\n",
        "    pixels = cropped_image.load()\n",
        "\n",
        "    image_strokes_rows = []\n",
        "    image_strokes_cols = []\n",
        "\n",
        "    # run through the image\n",
        "    for i in range(0, width):\n",
        "        for j in range(0, height):\n",
        "            # save coordinates of the image\n",
        "            if (pixels[i,j][0] > 0):\n",
        "                image_strokes_cols.append(i)\n",
        "                image_strokes_rows.append(j)\n",
        "\n",
        "    # if image is not empty then crop to contents of the image\n",
        "    if (len(image_strokes_rows)) > 0:\n",
        "        # find the box for image\n",
        "        row_min = np.array(image_strokes_rows).min()\n",
        "        row_max = np.array(image_strokes_rows).max()\n",
        "        col_min = np.array(image_strokes_cols).min()\n",
        "        col_max = np.array(image_strokes_cols).max()\n",
        "\n",
        "        # find the box for cropping\n",
        "        margin = min(row_min, height - row_max, col_min, width - col_max)\n",
        "\n",
        "        # crop image\n",
        "        border = (col_min, row_min, width - col_max, height - row_max)\n",
        "        cropped_image = ImageOps.crop(cropped_image, border)\n",
        "\n",
        "    # get cropped image size\n",
        "    width_cropped, height_cropped = cropped_image.size\n",
        "\n",
        "    # create square resulting image to paste cropped image into the center\n",
        "    dst_im = Image.new(\"RGBA\", (max(width_cropped, height_cropped), max(width_cropped, height_cropped)), \"white\")\n",
        "    offset = ((max(width_cropped, height_cropped) - width_cropped) // 2, (max(width_cropped, height_cropped) - height_cropped) // 2)\n",
        "    # paste to the center of a resulting image\n",
        "    dst_im.paste(cropped_image, offset)\n",
        "\n",
        "    #resize to 28,28\n",
        "    dst_im.thumbnail((28,28), Image.ANTIALIAS)\n",
        "    \n",
        "    return dst_im\n",
        "def normalize(arr):\n",
        "    \"\"\"\n",
        "    Function performs the linear normalizarion of the array.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
        "    INPUT:\n",
        "        arr - orginal numpy array\n",
        "    OUTPUT:\n",
        "        arr - normalized numpy array\n",
        "    \"\"\"\n",
        "    arr = arr.astype('float')\n",
        "    # Do not touch the alpha channel\n",
        "    for i in range(3):\n",
        "        minval = arr[...,i].min()\n",
        "        maxval = arr[...,i].max()\n",
        "        if minval != maxval:\n",
        "            arr[...,i] -= minval\n",
        "            arr[...,i] *= (255.0/(maxval-minval))\n",
        "    return arr\n",
        "\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Function performs the normalization of the image.\n",
        "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
        "    INPUT:\n",
        "        image - PIL image to be normalized\n",
        "    OUTPUT:\n",
        "        new_img - PIL image normalized\n",
        "    \"\"\"\n",
        "    arr = np.array(image)\n",
        "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGBA')\n",
        "    return new_img\n",
        "\n",
        "def rotate_image(src_im, angle = 45, size = (28,28)):\n",
        "    \"\"\"\n",
        "    Function to rotate PIL Image file\n",
        "    INPUT:\n",
        "        src_im - (PIL Image) 28x28 image to be rotated\n",
        "        angle - angle to rotate the image\n",
        "        size - (tuple) size of the output image\n",
        "    OUTPUT:\n",
        "    dst_im - (PIL Image) rotated image\n",
        "    \"\"\"\n",
        "    dst_im = Image.new(\"RGBA\", size, \"white\")\n",
        "    src_im = src_im.convert('RGBA')\n",
        "\n",
        "    rot = src_im.rotate(angle)\n",
        "    dst_im.paste(rot, (0, 0), rot)\n",
        "\n",
        "    return dst_im\n",
        "def flip_image(src_im):\n",
        "    \"\"\"\n",
        "    Function to flip a PIL Image file.\n",
        "    INPUT:\n",
        "        scr_im - (PIL Image) image to be flipped\n",
        "    OUTPUT:\n",
        "        dst_im - (PIL Image) flipped image\n",
        "    \"\"\"\n",
        "    dst_im = src_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return dst_im\n"
      ],
      "metadata": {
        "id": "8ZyDMAGCeE2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_model_path = \"/content/drive/MyDrive/kaggle/sketch/lenet_model_sketch_epoch40_lr00005.pth\"\n",
        "sketch_model=torch.load(sketch_model_path)"
      ],
      "metadata": {
        "id": "Zx_Vf6SZmjo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"bear\",\"camel\",\"cat\",\"dog\",\"elephant\",\"frog\",\"lion\",\"panda\",\"rabbit\",\"squirrel\"]\n",
        "# categories = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "label_dict = {0:'bear',1:'camel', 2:'cat', 3:'dog', 4:'elephant',\n",
        "                      5:'frog',6:'lion', 7:'panda', 8:'rabbit', 9:'squirrel'}\n",
        "\n",
        "# load data for each category\n",
        "classes = {}\n",
        "for category in categories:\n",
        "    data = pd.read_csv(\"./drive/MyDrive/kaggle/sketch/\" + category + \".csv\")\n",
        "    classes[category] = data\n",
        "\n",
        "\n",
        "# shrinking the images\n",
        "\n",
        "# create the dictionary containing classes names as keys and images as values\n",
        "values_dict = {}\n",
        "for category in categories:\n",
        "    data = classes[category][5000:5100]\n",
        "    values = [convert_to_np_raw(ast.literal_eval(img)).reshape(1, 784) for img in data['drawing'].values]\n",
        "    values_dict[category] = values\n",
        "    \n",
        "# concatenate to create X (values) and y (labels) datasets\n",
        "X_search = []\n",
        "y_search = []\n",
        "\n",
        "for key, value in label_dict.items():\n",
        "    data_i = values_dict[value]\n",
        "    Xi = np.concatenate(data_i, axis = 0)\n",
        "    yi = np.full((len(Xi), 1), key).ravel()\n",
        "    \n",
        "    X_search.append(Xi)\n",
        "    y_search.append(yi)\n",
        "X_search = np.concatenate(X_search, axis = 0)\n",
        "y_search = np.concatenate(y_search, axis = 0)\n",
        "sketch_search = torch.from_numpy(X_search).float()\n",
        "test_labels = torch.from_numpy(y_search).long()\n",
        "batch_size = 10\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eV__2Cr5ILwc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sketch_model.eval())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHTY6YZYTShR",
        "outputId": "3cc584f8-5e0b-4d1c-f16d-cd4f369f8510"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 26, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (4): Conv2d(26, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (8): Flatten(start_dim=1, end_dim=-1)\n",
            "    (9): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (10): LeakyReLU(negative_slope=0.01)\n",
            "    (11): Dropout(p=0.1, inplace=False)\n",
            "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (13): LeakyReLU(negative_slope=0.01)\n",
            "    (14): Dropout(p=0.1, inplace=False)\n",
            "    (15): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (16): LeakyReLU(negative_slope=0.01)\n",
            "    (17): Dropout(p=0.1, inplace=False)\n",
            "    (18): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## My trial:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch import optim,nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "###############################################################################\n",
        "import time, os, sys, random, datetime\n",
        "use_cuda = torch.cuda.is_available()\n",
        "L2_lambda = 0.001\n",
        "global nEpochs\n",
        "nEpochs = 2\n",
        "nEpochs = 40\n",
        "\n",
        "log_interval = 100\n",
        "#log_interval = 10\n",
        "learning_rate = 0.0005  # default smaller than 0.0005(1e-4) is \n",
        "\n",
        "class DatasetFromCSV(Dataset):\n",
        "    def __init__(self,datas,labels,height,width,transforms=None):\n",
        "        #self.data = pd.read_csv(csv_path)\n",
        "        self.data = datas\n",
        "        self.labels = labels\n",
        "        #view_images_grid(self.data, self.labels)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transforms = transforms\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        '''\n",
        "        # lzx's trial:(getitemmlgbz)\n",
        "        lzx_image=self.data[index].reshape(28,28)\n",
        "        #fig,axs=plt.subplots(1,1,figsize=(5,5))\n",
        "        plt.imshow(lzx_image)\n",
        "        plt.title(label_dict[self.labels[index]],loc=\"left\")\n",
        "        plt.show()\n",
        "        # mlgbz\n",
        "        '''\n",
        "        #  1D array ([784]) reshape  2D array ([28,28])\n",
        "        #img_as_np = np.asarray(self.data[index][:]).reshape(28, 28).astype(float)\n",
        "        img_as_np = np.asarray(self.data[index]).reshape(28, 28).astype(float)\n",
        "        #  numpy array  PIL image\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        #img_as_img = img_as_img.convert('L')\n",
        "        #  tensor\n",
        "        if self.transforms is not None:\n",
        "            img_as_tensor = self.transforms(img_as_img)\n",
        "            #  label\n",
        "        return (img_as_tensor, single_image_label)\n",
        " \n",
        "    def __len__(self):\n",
        "        #datacopy=self.data.copy().tolist()\n",
        "        #return len(datacopy.index)\n",
        "        return len(self.data)\n",
        " \n",
        "def cuda(arr):\n",
        "    if use_cuda:\n",
        "      return arr.cuda()\n",
        "    return arr\n",
        "def train_LeNet(model, train_loader, test_loader):\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # define the loss functions\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "    # choose an optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "    #optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\n",
        "\n",
        "    start = time.time()\n",
        "    w_decay = 0.95 # smoothing factor for reporting results\n",
        "    for e in range(nEpochs):\n",
        "        total_train_images = 0\n",
        "        total_train_loss = 0\n",
        "        train_images = 0\n",
        "        train_loss = 0\n",
        "        w_images = 0\n",
        "        w_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = cuda(data), cuda(target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_images += len(data)\n",
        "            train_loss += loss.data.item()\n",
        "\n",
        "            if train_images > log_interval:\n",
        "                total_train_images += train_images\n",
        "                total_train_loss += train_loss\n",
        "                if w_images == 0:\n",
        "                    w_loss = train_loss\n",
        "                    w_images = train_images\n",
        "                else:\n",
        "                    w_images = w_decay * w_images + train_images\n",
        "                    w_loss = w_decay * w_loss + train_loss\n",
        "                \n",
        "                #print(\"epoch %3d total train images: %8d loss: %8.3f loss: %8.3f     time: %6.1f\" % (e, total_train_images, train_loss/train_images, w_loss/w_images, (time.time()-start)))\n",
        "\n",
        "                train_images = 0\n",
        "                train_loss = 0\n",
        "                #break\n",
        "\n",
        "        test_images = 0\n",
        "        test_loss = 0\n",
        "        nCorrect = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                data, target = cuda(data), cuda(target)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                test_images += len(data)\n",
        "                test_loss += loss.data.item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max value\n",
        "                nCorrect += pred.eq(target.view_as(pred)).sum().item() # count correct items\n",
        "        print(\"================================================================================================\")\n",
        "        print(\"%3d %8d %8.3f %8.3f %8.3f %8.1f%%     %6.1f\" % (e, (e+1)*total_train_images, total_train_loss/total_train_images, w_loss/w_images, test_loss/test_images, 100*nCorrect/test_images, (time.time()-start)))\n",
        "        print(\"================================================================================================\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "15ZkI-VhiDYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_features = defaultdict(list)\n",
        "correct = 0\n",
        "total = 0\n",
        "sketch_search= DatasetFromCSV(X_search,y_search,28,28,transform)\n",
        "\n",
        "search_loader = torch.utils.data.DataLoader(sketch_search,batch_size=batch_size)\n",
        "\n",
        "\n",
        "from collections import defaultdict \n",
        "\n",
        "outputs_sketch= []\n",
        "def hook2(module, input, output):\n",
        "    outputs_sketch.append(output.detach())\n",
        "\n",
        "sketch_model.net[15].register_forward_hook(hook2)\n",
        "# output = model()\n",
        "# print(activation['fc2'])\n",
        "features__ = defaultdict(list)\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data, target) in enumerate(search_loader):\n",
        "      data, target = cuda(data), cuda(target)\n",
        "      output = sketch_model(data)\n",
        "      labels_sq = target.tolist()\n",
        "      for i in range(output.shape[0]):\n",
        "        features__[labels_sq[i]].append(outputs_sketch[-1][i])\n",
        "\n",
        "\n",
        "  features_sketch_ = torch.empty((100*10,64))\n",
        "  for i in range(len(features__)):\n",
        "\n",
        "      features_sketch_[100*i:100*(i+1)] = torch.stack(features__[i])"
      ],
      "metadata": {
        "id": "VJLY_gvLSVXm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkDBKEh5fx_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_sketch_.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XpPmywBstUr",
        "outputId": "fcdc6848-8bc6-4e51-c1b3-02f352047a26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZNM98EQzgsiu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,input_size = 64, nLinear1 = 2000,nLinear2 = 5000,output_size = 4096,dropout = 0.0):\n",
        "        super(Net, self).__init__()\n",
        "        self.linear0 = nn.Linear(input_size, nLinear1)\n",
        "        self.linear1 = nn.Linear(nLinear1,nLinear2)\n",
        "        self.linear2 = nn.Linear(nLinear2, output_size)\n",
        "        # self.non_linear = nn.ReLU6(inplace=False)\n",
        "        # self.non_linear = nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)\n",
        "        # self.non_linear = nn.Softplus(beta=1, threshold=20)\n",
        "        # self.non_linear = nn.SELU(inplace=False)\n",
        "        self.non_linear = nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
        "\n",
        "        # self.non_linear = nn.Hardsigmoid()\n",
        "        # self.non_linear = nn.Hardswish()\n",
        "        # self.non_linear = nn.GELU()\n",
        "        # self.non_linear = nn.SiLU()\n",
        "        # self.non_linear = nn.Mish()\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.linear0(x)\n",
        "        x = self.non_linear(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.non_linear(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        out0 = self.linear2(x)\n",
        "\n",
        "        return out0.squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMU3RTRdu_WU",
        "outputId": "626964c0-e267-450c-bb11-e4dcfacd6493"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.23293255269527435\n",
            "epoch:  1  loss:  0.23246973752975464\n",
            "epoch:  2  loss:  0.23200953006744385\n",
            "epoch:  3  loss:  0.23155182600021362\n",
            "epoch:  4  loss:  0.23109646141529083\n",
            "epoch:  5  loss:  0.2306433469057083\n",
            "epoch:  6  loss:  0.23019233345985413\n",
            "epoch:  7  loss:  0.2297433465719223\n",
            "epoch:  8  loss:  0.22929620742797852\n",
            "epoch:  9  loss:  0.22885088622570038\n",
            "epoch:  10  loss:  0.22840723395347595\n",
            "epoch:  11  loss:  0.2279651165008545\n",
            "epoch:  12  loss:  0.22752447426319122\n",
            "epoch:  13  loss:  0.2270851731300354\n",
            "epoch:  14  loss:  0.22664712369441986\n",
            "epoch:  15  loss:  0.22621023654937744\n",
            "epoch:  16  loss:  0.2257743924856186\n",
            "epoch:  17  loss:  0.22533951699733734\n",
            "epoch:  18  loss:  0.22490553557872772\n",
            "epoch:  19  loss:  0.2244722992181778\n",
            "epoch:  20  loss:  0.2240397334098816\n",
            "epoch:  21  loss:  0.22360776364803314\n",
            "epoch:  22  loss:  0.2231762707233429\n",
            "epoch:  23  loss:  0.22274519503116608\n",
            "epoch:  24  loss:  0.22231441736221313\n",
            "epoch:  25  loss:  0.22188393771648407\n",
            "epoch:  26  loss:  0.22145357728004456\n",
            "epoch:  27  loss:  0.22102338075637817\n",
            "epoch:  28  loss:  0.2205931544303894\n",
            "epoch:  29  loss:  0.22016289830207825\n",
            "epoch:  30  loss:  0.21973253786563873\n",
            "epoch:  31  loss:  0.2193019539117813\n",
            "epoch:  32  loss:  0.2188710719347\n",
            "epoch:  33  loss:  0.21843983232975006\n",
            "epoch:  34  loss:  0.21800820529460907\n",
            "epoch:  35  loss:  0.21757599711418152\n",
            "epoch:  36  loss:  0.21714329719543457\n",
            "epoch:  37  loss:  0.2167098969221115\n",
            "epoch:  38  loss:  0.21627581119537354\n",
            "epoch:  39  loss:  0.21584099531173706\n",
            "epoch:  40  loss:  0.21540530025959015\n",
            "epoch:  41  loss:  0.21496877074241638\n",
            "epoch:  42  loss:  0.21453124284744263\n",
            "epoch:  43  loss:  0.21409271657466888\n",
            "epoch:  44  loss:  0.21365311741828918\n",
            "epoch:  45  loss:  0.21321240067481995\n",
            "epoch:  46  loss:  0.212770476937294\n",
            "epoch:  47  loss:  0.21232733130455017\n",
            "epoch:  48  loss:  0.21188287436962128\n",
            "epoch:  49  loss:  0.21143707633018494\n",
            "epoch:  50  loss:  0.210989847779274\n",
            "epoch:  51  loss:  0.21054117381572723\n",
            "epoch:  52  loss:  0.21009093523025513\n",
            "epoch:  53  loss:  0.20963917672634125\n",
            "epoch:  54  loss:  0.20918583869934082\n",
            "epoch:  55  loss:  0.2087307870388031\n",
            "epoch:  56  loss:  0.20827406644821167\n",
            "epoch:  57  loss:  0.20781557261943817\n",
            "epoch:  58  loss:  0.20735527575016022\n",
            "epoch:  59  loss:  0.206893190741539\n",
            "epoch:  60  loss:  0.20642918348312378\n",
            "epoch:  61  loss:  0.20596325397491455\n",
            "epoch:  62  loss:  0.2054954171180725\n",
            "epoch:  63  loss:  0.2050255686044693\n",
            "epoch:  64  loss:  0.20455369353294373\n",
            "epoch:  65  loss:  0.20407971739768982\n",
            "epoch:  66  loss:  0.2036036103963852\n",
            "epoch:  67  loss:  0.20312538743019104\n",
            "epoch:  68  loss:  0.2026449590921402\n",
            "epoch:  69  loss:  0.20216234028339386\n",
            "epoch:  70  loss:  0.20167748630046844\n",
            "epoch:  71  loss:  0.20119036734104156\n",
            "epoch:  72  loss:  0.20070098340511322\n",
            "epoch:  73  loss:  0.20020925998687744\n",
            "epoch:  74  loss:  0.19971521198749542\n",
            "epoch:  75  loss:  0.19921880960464478\n",
            "epoch:  76  loss:  0.1987200230360031\n",
            "epoch:  77  loss:  0.19821879267692566\n",
            "epoch:  78  loss:  0.1977151483297348\n",
            "epoch:  79  loss:  0.19720906019210815\n",
            "epoch:  80  loss:  0.19670048356056213\n",
            "epoch:  81  loss:  0.19618937373161316\n",
            "epoch:  82  loss:  0.19567576050758362\n",
            "epoch:  83  loss:  0.1951596736907959\n",
            "epoch:  84  loss:  0.19464102387428284\n",
            "epoch:  85  loss:  0.19411982595920563\n",
            "epoch:  86  loss:  0.19359606504440308\n",
            "epoch:  87  loss:  0.19306974112987518\n",
            "epoch:  88  loss:  0.19254089891910553\n",
            "epoch:  89  loss:  0.19200949370861053\n",
            "epoch:  90  loss:  0.1914755254983902\n",
            "epoch:  91  loss:  0.19093899428844452\n",
            "epoch:  92  loss:  0.1903999149799347\n",
            "epoch:  93  loss:  0.18985824286937714\n",
            "epoch:  94  loss:  0.18931402266025543\n",
            "epoch:  95  loss:  0.18876726925373077\n",
            "epoch:  96  loss:  0.18821796774864197\n",
            "epoch:  97  loss:  0.18766610324382782\n",
            "epoch:  98  loss:  0.1871117204427719\n",
            "epoch:  99  loss:  0.18655475974082947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.empty((90*10,64))\n",
        "y_train = torch.empty((90*10,4096))\n",
        "x_test = torch.empty((10*10,64))\n",
        "y_test = torch.empty((10*10,4096))\n",
        "for k in range(10):\n",
        "    x_train[90*k:90*(k+1)] = features_sketch_[100*k:100*k+90]\n",
        "    y_train[90*k:90*(k+1)] = avg_image_feature[100*k:100*k+90]\n",
        "    x_test[10*k:10*(k+1)] = features_sketch_[100*k+90:100*(k+1)]\n",
        "    y_test[10*k:10*(k+1)] = avg_image_feature[100*k+90:100*(k+1)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yFp6DiK3uSPQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.empty((90*10,64))\n",
        "y_train = torch.empty((90*10,4096))\n",
        "x_test = torch.empty((10*10,64))\n",
        "y_test = torch.empty((10*10,4096))\n",
        "for k in range(10):\n",
        "    x_train[90*k:90*(k+1)] = features_sketch_[100*k:100*k+90]\n",
        "    y_train[90*k:90*(k+1)] = new_image_feature[100*k:100*k+90]\n",
        "    x_test[10*k:10*(k+1)] = features_sketch_[100*k+90:100*(k+1)]\n",
        "    y_test[10*k:10*(k+1)] = new_image_feature[100*k+90:100*(k+1)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GDTyYov-1kbT"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 10)\n",
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(x_train)\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, y_train)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()\n",
        "   y_pred_test = model(x_test)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred_test, y_test)\n",
        "   print('validation ', epoch,' loss: ', loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5khG74gwxBl",
        "outputId": "ee02b04a-af09-4d76-d189-f1118f68f598"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.23695577681064606\n",
            "validation  0  loss:  0.21120822429656982\n",
            "epoch:  1  loss:  0.19950330257415771\n",
            "validation  1  loss:  0.16625259816646576\n",
            "epoch:  2  loss:  0.15433147549629211\n",
            "validation  2  loss:  0.11948360502719879\n",
            "epoch:  3  loss:  0.109094999730587\n",
            "validation  3  loss:  0.11101805418729782\n",
            "epoch:  4  loss:  0.10204508155584335\n",
            "validation  4  loss:  0.10943279415369034\n",
            "epoch:  5  loss:  0.09930127114057541\n",
            "validation  5  loss:  0.10621017217636108\n",
            "epoch:  6  loss:  0.09686258435249329\n",
            "validation  6  loss:  0.10488364845514297\n",
            "epoch:  7  loss:  0.09463868290185928\n",
            "validation  7  loss:  0.10212177038192749\n",
            "epoch:  8  loss:  0.09240484982728958\n",
            "validation  8  loss:  0.1006547063589096\n",
            "epoch:  9  loss:  0.09023290127515793\n",
            "validation  9  loss:  0.09809888899326324\n",
            "epoch:  10  loss:  0.0880589410662651\n",
            "validation  10  loss:  0.09654056280851364\n",
            "epoch:  11  loss:  0.0859338641166687\n",
            "validation  11  loss:  0.09414362162351608\n",
            "epoch:  12  loss:  0.08384835720062256\n",
            "validation  12  loss:  0.09259877353906631\n",
            "epoch:  13  loss:  0.08184564858675003\n",
            "validation  13  loss:  0.0903884619474411\n",
            "epoch:  14  loss:  0.07993031293153763\n",
            "validation  14  loss:  0.08896157890558243\n",
            "epoch:  15  loss:  0.07813188433647156\n",
            "validation  15  loss:  0.08697076141834259\n",
            "epoch:  16  loss:  0.07644885033369064\n",
            "validation  16  loss:  0.08572635799646378\n",
            "epoch:  17  loss:  0.07489463686943054\n",
            "validation  17  loss:  0.08395273238420486\n",
            "epoch:  18  loss:  0.0734580010175705\n",
            "validation  18  loss:  0.08291223645210266\n",
            "epoch:  19  loss:  0.07214295864105225\n",
            "validation  19  loss:  0.08132882416248322\n",
            "epoch:  20  loss:  0.07093167304992676\n",
            "validation  20  loss:  0.08049004524946213\n",
            "epoch:  21  loss:  0.06982509791851044\n",
            "validation  21  loss:  0.07905301451683044\n",
            "epoch:  22  loss:  0.06880276650190353\n",
            "validation  22  loss:  0.07840970158576965\n",
            "epoch:  23  loss:  0.06786945462226868\n",
            "validation  23  loss:  0.07707318663597107\n",
            "epoch:  24  loss:  0.06700214743614197\n",
            "validation  24  loss:  0.07662985473871231\n",
            "epoch:  25  loss:  0.06621402502059937\n",
            "validation  25  loss:  0.07534773647785187\n",
            "epoch:  26  loss:  0.06547582149505615\n",
            "validation  26  loss:  0.07512706518173218\n",
            "epoch:  27  loss:  0.06481729447841644\n",
            "validation  27  loss:  0.07385148108005524\n",
            "epoch:  28  loss:  0.06419339776039124\n",
            "validation  28  loss:  0.07390853762626648\n",
            "epoch:  29  loss:  0.06366946548223495\n",
            "validation  29  loss:  0.0725860670208931\n",
            "epoch:  30  loss:  0.06315860897302628\n",
            "validation  30  loss:  0.07302996516227722\n",
            "epoch:  31  loss:  0.06281084567308426\n",
            "validation  31  loss:  0.07159647345542908\n",
            "epoch:  32  loss:  0.06242829188704491\n",
            "validation  32  loss:  0.07264593243598938\n",
            "epoch:  33  loss:  0.06237594038248062\n",
            "validation  33  loss:  0.0710054412484169\n",
            "epoch:  34  loss:  0.0621396005153656\n",
            "validation  34  loss:  0.0730711817741394\n",
            "epoch:  35  loss:  0.062660813331604\n",
            "validation  35  loss:  0.07101471722126007\n",
            "epoch:  36  loss:  0.06250473856925964\n",
            "validation  36  loss:  0.07477900385856628\n",
            "epoch:  37  loss:  0.06413214653730392\n",
            "validation  37  loss:  0.07170187681913376\n",
            "epoch:  38  loss:  0.06357309967279434\n",
            "validation  38  loss:  0.07781804352998734\n",
            "epoch:  39  loss:  0.06688225269317627\n",
            "validation  39  loss:  0.07242577522993088\n",
            "epoch:  40  loss:  0.06459151208400726\n",
            "validation  40  loss:  0.08036582171916962\n",
            "epoch:  41  loss:  0.06921859830617905\n",
            "validation  41  loss:  0.07211340963840485\n",
            "epoch:  42  loss:  0.06436450034379959\n",
            "validation  42  loss:  0.07994139194488525\n",
            "epoch:  43  loss:  0.06878198683261871\n",
            "validation  43  loss:  0.07099150866270065\n",
            "epoch:  44  loss:  0.06317103654146194\n",
            "validation  44  loss:  0.07761812210083008\n",
            "epoch:  45  loss:  0.06658220291137695\n",
            "validation  45  loss:  0.06980886310338974\n",
            "epoch:  46  loss:  0.061874933540821075\n",
            "validation  46  loss:  0.07532026618719101\n",
            "epoch:  47  loss:  0.06442727148532867\n",
            "validation  47  loss:  0.06878833472728729\n",
            "epoch:  48  loss:  0.06074589118361473\n",
            "validation  48  loss:  0.07349004596471786\n",
            "epoch:  49  loss:  0.06271810084581375\n",
            "validation  49  loss:  0.0679294764995575\n",
            "epoch:  50  loss:  0.05979318916797638\n",
            "validation  50  loss:  0.07206124812364578\n",
            "epoch:  51  loss:  0.06138366833329201\n",
            "validation  51  loss:  0.06720118969678879\n",
            "epoch:  52  loss:  0.05898689851164818\n",
            "validation  52  loss:  0.07093183696269989\n",
            "epoch:  53  loss:  0.06032435595989227\n",
            "validation  53  loss:  0.06657593697309494\n",
            "epoch:  54  loss:  0.05829796940088272\n",
            "validation  54  loss:  0.07002530992031097\n",
            "epoch:  55  loss:  0.059467863291502\n",
            "validation  55  loss:  0.06603232026100159\n",
            "epoch:  56  loss:  0.05770188197493553\n",
            "validation  56  loss:  0.06928326189517975\n",
            "epoch:  57  loss:  0.0587601400911808\n",
            "validation  57  loss:  0.06555226445198059\n",
            "epoch:  58  loss:  0.057179197669029236\n",
            "validation  58  loss:  0.06866656988859177\n",
            "epoch:  59  loss:  0.05816435441374779\n",
            "validation  59  loss:  0.0651235282421112\n",
            "epoch:  60  loss:  0.056715983897447586\n",
            "validation  60  loss:  0.06814809143543243\n",
            "epoch:  61  loss:  0.0576561763882637\n",
            "validation  61  loss:  0.0647367462515831\n",
            "epoch:  62  loss:  0.05630149692296982\n",
            "validation  62  loss:  0.06770642101764679\n",
            "epoch:  63  loss:  0.057216376066207886\n",
            "validation  63  loss:  0.06438466906547546\n",
            "epoch:  64  loss:  0.05592638999223709\n",
            "validation  64  loss:  0.06732364743947983\n",
            "epoch:  65  loss:  0.056829266250133514\n",
            "validation  65  loss:  0.06406013667583466\n",
            "epoch:  66  loss:  0.05558310076594353\n",
            "validation  66  loss:  0.0669882744550705\n",
            "epoch:  67  loss:  0.056483734399080276\n",
            "validation  67  loss:  0.06375851482152939\n",
            "epoch:  68  loss:  0.055265747010707855\n",
            "validation  68  loss:  0.06668973714113235\n",
            "epoch:  69  loss:  0.05617083981633186\n",
            "validation  69  loss:  0.06347557157278061\n",
            "epoch:  70  loss:  0.05496853590011597\n",
            "validation  70  loss:  0.06641477346420288\n",
            "epoch:  71  loss:  0.055879030376672745\n",
            "validation  71  loss:  0.06320656836032867\n",
            "epoch:  72  loss:  0.054686058312654495\n",
            "validation  72  loss:  0.06615909934043884\n",
            "epoch:  73  loss:  0.0556047298014164\n",
            "validation  73  loss:  0.06295012682676315\n",
            "epoch:  74  loss:  0.054416995495557785\n",
            "validation  74  loss:  0.06591707468032837\n",
            "epoch:  75  loss:  0.05534282326698303\n",
            "validation  75  loss:  0.06270339339971542\n",
            "epoch:  76  loss:  0.054157014936208725\n",
            "validation  76  loss:  0.06568403542041779\n",
            "epoch:  77  loss:  0.05508944019675255\n",
            "validation  77  loss:  0.06246567144989967\n",
            "epoch:  78  loss:  0.053904544562101364\n",
            "validation  78  loss:  0.06545589119195938\n",
            "epoch:  79  loss:  0.05484122037887573\n",
            "validation  79  loss:  0.06223495304584503\n",
            "epoch:  80  loss:  0.053658828139305115\n",
            "validation  80  loss:  0.06523071974515915\n",
            "epoch:  81  loss:  0.05459626019001007\n",
            "validation  81  loss:  0.0620100274682045\n",
            "epoch:  82  loss:  0.053417786955833435\n",
            "validation  82  loss:  0.06500861793756485\n",
            "epoch:  83  loss:  0.05435509234666824\n",
            "validation  83  loss:  0.061791520565748215\n",
            "epoch:  84  loss:  0.053182121366262436\n",
            "validation  84  loss:  0.06479039043188095\n",
            "epoch:  85  loss:  0.054117873311042786\n",
            "validation  85  loss:  0.06157884746789932\n",
            "epoch:  86  loss:  0.052951693534851074\n",
            "validation  86  loss:  0.06457316130399704\n",
            "epoch:  87  loss:  0.053882211446762085\n",
            "validation  87  loss:  0.06137055531144142\n",
            "epoch:  88  loss:  0.052724260836839676\n",
            "validation  88  loss:  0.06435719132423401\n",
            "epoch:  89  loss:  0.053647879511117935\n",
            "validation  89  loss:  0.061166372150182724\n",
            "epoch:  90  loss:  0.052500348538160324\n",
            "validation  90  loss:  0.06414155662059784\n",
            "epoch:  91  loss:  0.05341450870037079\n",
            "validation  91  loss:  0.06096753478050232\n",
            "epoch:  92  loss:  0.05228125676512718\n",
            "validation  92  loss:  0.06392829120159149\n",
            "epoch:  93  loss:  0.05318453907966614\n",
            "validation  93  loss:  0.06077473238110542\n",
            "epoch:  94  loss:  0.0520671047270298\n",
            "validation  94  loss:  0.06371954828500748\n",
            "epoch:  95  loss:  0.05295928567647934\n",
            "validation  95  loss:  0.060588642954826355\n",
            "epoch:  96  loss:  0.05185917764902115\n",
            "validation  96  loss:  0.06351808458566666\n",
            "epoch:  97  loss:  0.05274129658937454\n",
            "validation  97  loss:  0.0604085698723793\n",
            "epoch:  98  loss:  0.051657091826200485\n",
            "validation  98  loss:  0.06332191079854965\n",
            "epoch:  99  loss:  0.05252882093191147\n",
            "validation  99  loss:  0.060234807431697845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyO0j12__qDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(x_train)\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, y_train)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()\n",
        "   y_pred_test = model(x_test)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred_test, y_test)\n",
        "   print('validation ', epoch,' loss: ', loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq_GMjt056Gd",
        "outputId": "9f66ed9a-f2e0-4df4-f501-e67c7bac1b22"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.05146072432398796\n",
            "validation  0  loss:  0.05946976691484451\n",
            "epoch:  1  loss:  0.050462573766708374\n",
            "validation  1  loss:  0.05906820669770241\n",
            "epoch:  2  loss:  0.049890052527189255\n",
            "validation  2  loss:  0.05885770916938782\n",
            "epoch:  3  loss:  0.049551427364349365\n",
            "validation  3  loss:  0.058749645948410034\n",
            "epoch:  4  loss:  0.04934588819742203\n",
            "validation  4  loss:  0.05869670212268829\n",
            "epoch:  5  loss:  0.049218013882637024\n",
            "validation  5  loss:  0.05867312476038933\n",
            "epoch:  6  loss:  0.0491361990571022\n",
            "validation  6  loss:  0.05866456404328346\n",
            "epoch:  7  loss:  0.049082159996032715\n",
            "validation  7  loss:  0.05866313353180885\n",
            "epoch:  8  loss:  0.04904504492878914\n",
            "validation  8  loss:  0.05866454169154167\n",
            "epoch:  9  loss:  0.04901833087205887\n",
            "validation  9  loss:  0.058666523545980453\n",
            "epoch:  10  loss:  0.04899802431464195\n",
            "validation  10  loss:  0.05866791680455208\n",
            "epoch:  11  loss:  0.04898170754313469\n",
            "validation  11  loss:  0.05866822227835655\n",
            "epoch:  12  loss:  0.048967886716127396\n",
            "validation  12  loss:  0.05866725742816925\n",
            "epoch:  13  loss:  0.04895561933517456\n",
            "validation  13  loss:  0.058665040880441666\n",
            "epoch:  14  loss:  0.04894435405731201\n",
            "validation  14  loss:  0.05866169556975365\n",
            "epoch:  15  loss:  0.04893370717763901\n",
            "validation  15  loss:  0.05865735560655594\n",
            "epoch:  16  loss:  0.04892345890402794\n",
            "validation  16  loss:  0.05865219980478287\n",
            "epoch:  17  loss:  0.04891347512602806\n",
            "validation  17  loss:  0.05864635482430458\n",
            "epoch:  18  loss:  0.04890366271138191\n",
            "validation  18  loss:  0.058639928698539734\n",
            "epoch:  19  loss:  0.048893965780735016\n",
            "validation  19  loss:  0.05863304063677788\n",
            "epoch:  20  loss:  0.04888434335589409\n",
            "validation  20  loss:  0.058625783771276474\n",
            "epoch:  21  loss:  0.04887479171156883\n",
            "validation  21  loss:  0.0586182177066803\n",
            "epoch:  22  loss:  0.048865269869565964\n",
            "validation  22  loss:  0.05861040577292442\n",
            "epoch:  23  loss:  0.048855800181627274\n",
            "validation  23  loss:  0.05860240384936333\n",
            "epoch:  24  loss:  0.04884634166955948\n",
            "validation  24  loss:  0.058594271540641785\n",
            "epoch:  25  loss:  0.04883692413568497\n",
            "validation  25  loss:  0.05858601629734039\n",
            "epoch:  26  loss:  0.048827532678842545\n",
            "validation  26  loss:  0.05857767164707184\n",
            "epoch:  27  loss:  0.048818156123161316\n",
            "validation  27  loss:  0.05856925994157791\n",
            "epoch:  28  loss:  0.04880880191922188\n",
            "validation  28  loss:  0.0585608147084713\n",
            "epoch:  29  loss:  0.04879947751760483\n",
            "validation  29  loss:  0.05855231732130051\n",
            "epoch:  30  loss:  0.048790160566568375\n",
            "validation  30  loss:  0.05854380130767822\n",
            "epoch:  31  loss:  0.04878087341785431\n",
            "validation  31  loss:  0.058535270392894745\n",
            "epoch:  32  loss:  0.04877160117030144\n",
            "validation  32  loss:  0.05852673202753067\n",
            "epoch:  33  loss:  0.048762351274490356\n",
            "validation  33  loss:  0.0585181824862957\n",
            "epoch:  34  loss:  0.04875312000513077\n",
            "validation  34  loss:  0.05850963667035103\n",
            "epoch:  35  loss:  0.04874390736222267\n",
            "validation  35  loss:  0.05850109085440636\n",
            "epoch:  36  loss:  0.048734717071056366\n",
            "validation  36  loss:  0.05849255621433258\n",
            "epoch:  37  loss:  0.04872555285692215\n",
            "validation  37  loss:  0.058484021574258804\n",
            "epoch:  38  loss:  0.04871639236807823\n",
            "validation  38  loss:  0.05847550556063652\n",
            "epoch:  39  loss:  0.0487072579562664\n",
            "validation  39  loss:  0.05846698582172394\n",
            "epoch:  40  loss:  0.04869814217090607\n",
            "validation  40  loss:  0.05845848098397255\n",
            "epoch:  41  loss:  0.04868904501199722\n",
            "validation  41  loss:  0.05844998359680176\n",
            "epoch:  42  loss:  0.04867996647953987\n",
            "validation  42  loss:  0.05844150856137276\n",
            "epoch:  43  loss:  0.04867091029882431\n",
            "validation  43  loss:  0.05843304097652435\n",
            "epoch:  44  loss:  0.048661865293979645\n",
            "validation  44  loss:  0.05842459201812744\n",
            "epoch:  45  loss:  0.04865284264087677\n",
            "validation  45  loss:  0.058416157960891724\n",
            "epoch:  46  loss:  0.04864383861422539\n",
            "validation  46  loss:  0.0584077350795269\n",
            "epoch:  47  loss:  0.0486348457634449\n",
            "validation  47  loss:  0.058399323374032974\n",
            "epoch:  48  loss:  0.0486258789896965\n",
            "validation  48  loss:  0.058390937745571136\n",
            "epoch:  49  loss:  0.0486169271171093\n",
            "validation  49  loss:  0.058382548391819\n",
            "epoch:  50  loss:  0.04860799014568329\n",
            "validation  50  loss:  0.05837419629096985\n",
            "epoch:  51  loss:  0.04859906807541847\n",
            "validation  51  loss:  0.058365825563669205\n",
            "epoch:  52  loss:  0.048590172082185745\n",
            "validation  52  loss:  0.05835747718811035\n",
            "epoch:  53  loss:  0.04858129471540451\n",
            "validation  53  loss:  0.05834915116429329\n",
            "epoch:  54  loss:  0.04857243597507477\n",
            "validation  54  loss:  0.058340851217508316\n",
            "epoch:  55  loss:  0.04856358841061592\n",
            "validation  55  loss:  0.05833256617188454\n",
            "epoch:  56  loss:  0.048554763197898865\n",
            "validation  56  loss:  0.058324288576841354\n",
            "epoch:  57  loss:  0.04854594171047211\n",
            "validation  57  loss:  0.058316029608249664\n",
            "epoch:  58  loss:  0.048537153750658035\n",
            "validation  58  loss:  0.058307796716690063\n",
            "epoch:  59  loss:  0.04852837324142456\n",
            "validation  59  loss:  0.05829957500100136\n",
            "epoch:  60  loss:  0.04851961508393288\n",
            "validation  60  loss:  0.05829136446118355\n",
            "epoch:  61  loss:  0.048510871827602386\n",
            "validation  61  loss:  0.058283187448978424\n",
            "epoch:  62  loss:  0.04850213974714279\n",
            "validation  62  loss:  0.0582750029861927\n",
            "epoch:  63  loss:  0.04849342629313469\n",
            "validation  63  loss:  0.05826684832572937\n",
            "epoch:  64  loss:  0.04848472774028778\n",
            "validation  64  loss:  0.05825870856642723\n",
            "epoch:  65  loss:  0.048476047813892365\n",
            "validation  65  loss:  0.05825059488415718\n",
            "epoch:  66  loss:  0.04846738651394844\n",
            "validation  66  loss:  0.05824247747659683\n",
            "epoch:  67  loss:  0.04845873638987541\n",
            "validation  67  loss:  0.05823437124490738\n",
            "epoch:  68  loss:  0.04845011234283447\n",
            "validation  68  loss:  0.05822627991437912\n",
            "epoch:  69  loss:  0.04844149202108383\n",
            "validation  69  loss:  0.058218225836753845\n",
            "epoch:  70  loss:  0.04843290522694588\n",
            "validation  70  loss:  0.05821016803383827\n",
            "epoch:  71  loss:  0.04842432215809822\n",
            "validation  71  loss:  0.05820212513208389\n",
            "epoch:  72  loss:  0.04841574653983116\n",
            "validation  72  loss:  0.05819409340620041\n",
            "epoch:  73  loss:  0.04840719699859619\n",
            "validation  73  loss:  0.05818609893321991\n",
            "epoch:  74  loss:  0.048398662358522415\n",
            "validation  74  loss:  0.05817811191082001\n",
            "epoch:  75  loss:  0.048390138894319534\n",
            "validation  75  loss:  0.0581701435148716\n",
            "epoch:  76  loss:  0.04838163033127785\n",
            "validation  76  loss:  0.05816219002008438\n",
            "epoch:  77  loss:  0.048373136669397354\n",
            "validation  77  loss:  0.05815424025058746\n",
            "epoch:  78  loss:  0.04836466535925865\n",
            "validation  78  loss:  0.05814632773399353\n",
            "epoch:  79  loss:  0.048356205224990845\n",
            "validation  79  loss:  0.05813843384385109\n",
            "epoch:  80  loss:  0.048347752541303635\n",
            "validation  80  loss:  0.058130547404289246\n",
            "epoch:  81  loss:  0.04833931475877762\n",
            "validation  81  loss:  0.058122679591178894\n",
            "epoch:  82  loss:  0.04833090305328369\n",
            "validation  82  loss:  0.05811484158039093\n",
            "epoch:  83  loss:  0.04832250252366066\n",
            "validation  83  loss:  0.05810701474547386\n",
            "epoch:  84  loss:  0.04831411689519882\n",
            "validation  84  loss:  0.05809921771287918\n",
            "epoch:  85  loss:  0.04830573871731758\n",
            "validation  85  loss:  0.058091431856155396\n",
            "epoch:  86  loss:  0.04829738289117813\n",
            "validation  86  loss:  0.05808365345001221\n",
            "epoch:  87  loss:  0.04828903451561928\n",
            "validation  87  loss:  0.058075882494449615\n",
            "epoch:  88  loss:  0.048280708491802216\n",
            "validation  88  loss:  0.05806813761591911\n",
            "epoch:  89  loss:  0.04827239736914635\n",
            "validation  89  loss:  0.05806039646267891\n",
            "epoch:  90  loss:  0.048264093697071075\n",
            "validation  90  loss:  0.05805269256234169\n",
            "epoch:  91  loss:  0.048255804926157\n",
            "validation  91  loss:  0.058044999837875366\n",
            "epoch:  92  loss:  0.04824754595756531\n",
            "validation  92  loss:  0.05803733691573143\n",
            "epoch:  93  loss:  0.04823928698897362\n",
            "validation  93  loss:  0.05802970007061958\n",
            "epoch:  94  loss:  0.04823105409741402\n",
            "validation  94  loss:  0.05802207812666893\n",
            "epoch:  95  loss:  0.04822282865643501\n",
            "validation  95  loss:  0.05801446735858917\n",
            "epoch:  96  loss:  0.0482146255671978\n",
            "validation  96  loss:  0.05800687149167061\n",
            "epoch:  97  loss:  0.048206426203250885\n",
            "validation  97  loss:  0.05799930542707443\n",
            "epoch:  98  loss:  0.04819824919104576\n",
            "validation  98  loss:  0.057991739362478256\n",
            "epoch:  99  loss:  0.04819008335471153\n",
            "validation  99  loss:  0.057984210550785065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(features_sketch_)"
      ],
      "metadata": {
        "id": "rx5yxONkzazW"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 5)\n",
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(x_train)\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, y_train)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()\n",
        "   y_pred_test = model(x_test)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred_test, y_test)\n",
        "   print('validation ', epoch,' loss: ', loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17uBGjXq1vIC",
        "outputId": "16132a15-2d5e-42ca-ee60-5e225b39fe79"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.3246161639690399\n",
            "validation  0  loss:  0.29512035846710205\n",
            "epoch:  1  loss:  0.3037310242652893\n",
            "validation  1  loss:  0.27497604489326477\n",
            "epoch:  2  loss:  0.28302744030952454\n",
            "validation  2  loss:  0.2497144341468811\n",
            "epoch:  3  loss:  0.25723880529403687\n",
            "validation  3  loss:  0.220860555768013\n",
            "epoch:  4  loss:  0.22807811200618744\n",
            "validation  4  loss:  0.19721944630146027\n",
            "epoch:  5  loss:  0.20462462306022644\n",
            "validation  5  loss:  0.18596376478672028\n",
            "epoch:  6  loss:  0.19388659298419952\n",
            "validation  6  loss:  0.18240539729595184\n",
            "epoch:  7  loss:  0.1906055063009262\n",
            "validation  7  loss:  0.18073423206806183\n",
            "epoch:  8  loss:  0.18893399834632874\n",
            "validation  8  loss:  0.17946690320968628\n",
            "epoch:  9  loss:  0.187593013048172\n",
            "validation  9  loss:  0.17832794785499573\n",
            "epoch:  10  loss:  0.18636493384838104\n",
            "validation  10  loss:  0.1772400438785553\n",
            "epoch:  11  loss:  0.18518143892288208\n",
            "validation  11  loss:  0.17617572844028473\n",
            "epoch:  12  loss:  0.18401698768138885\n",
            "validation  12  loss:  0.17512348294258118\n",
            "epoch:  13  loss:  0.18286113440990448\n",
            "validation  13  loss:  0.17407751083374023\n",
            "epoch:  14  loss:  0.18170881271362305\n",
            "validation  14  loss:  0.1730348914861679\n",
            "epoch:  15  loss:  0.18055804073810577\n",
            "validation  15  loss:  0.17199456691741943\n",
            "epoch:  16  loss:  0.17940831184387207\n",
            "validation  16  loss:  0.17095588147640228\n",
            "epoch:  17  loss:  0.17826031148433685\n",
            "validation  17  loss:  0.16991962492465973\n",
            "epoch:  18  loss:  0.17711575329303741\n",
            "validation  18  loss:  0.16888709366321564\n",
            "epoch:  19  loss:  0.17597682774066925\n",
            "validation  19  loss:  0.16786041855812073\n",
            "epoch:  20  loss:  0.17484648525714874\n",
            "validation  20  loss:  0.16684190928936005\n",
            "epoch:  21  loss:  0.1737278550863266\n",
            "validation  21  loss:  0.16583381593227386\n",
            "epoch:  22  loss:  0.17262457311153412\n",
            "validation  22  loss:  0.1648389846086502\n",
            "epoch:  23  loss:  0.17154008150100708\n",
            "validation  23  loss:  0.1638602316379547\n",
            "epoch:  24  loss:  0.17047767341136932\n",
            "validation  24  loss:  0.16290001571178436\n",
            "epoch:  25  loss:  0.1694406121969223\n",
            "validation  25  loss:  0.1619608998298645\n",
            "epoch:  26  loss:  0.1684318333864212\n",
            "validation  26  loss:  0.16104553639888763\n",
            "epoch:  27  loss:  0.16745372116565704\n",
            "validation  27  loss:  0.1601554900407791\n",
            "epoch:  28  loss:  0.1665080040693283\n",
            "validation  28  loss:  0.1592920422554016\n",
            "epoch:  29  loss:  0.16559627652168274\n",
            "validation  29  loss:  0.15845610201358795\n",
            "epoch:  30  loss:  0.16471926867961884\n",
            "validation  30  loss:  0.15764813125133514\n",
            "epoch:  31  loss:  0.1638774573802948\n",
            "validation  31  loss:  0.1568681001663208\n",
            "epoch:  32  loss:  0.16307052969932556\n",
            "validation  32  loss:  0.15611596405506134\n",
            "epoch:  33  loss:  0.16229796409606934\n",
            "validation  33  loss:  0.1553911417722702\n",
            "epoch:  34  loss:  0.16155871748924255\n",
            "validation  34  loss:  0.15469297766685486\n",
            "epoch:  35  loss:  0.16085153818130493\n",
            "validation  35  loss:  0.1540207415819168\n",
            "epoch:  36  loss:  0.16017521917819977\n",
            "validation  36  loss:  0.15337343513965607\n",
            "epoch:  37  loss:  0.15952831506729126\n",
            "validation  37  loss:  0.15275025367736816\n",
            "epoch:  38  loss:  0.15890929102897644\n",
            "validation  38  loss:  0.1521502584218979\n",
            "epoch:  39  loss:  0.15831655263900757\n",
            "validation  39  loss:  0.15157245099544525\n",
            "epoch:  40  loss:  0.1577484905719757\n",
            "validation  40  loss:  0.1510155349969864\n",
            "epoch:  41  loss:  0.15720368921756744\n",
            "validation  41  loss:  0.1504787653684616\n",
            "epoch:  42  loss:  0.15668071806430817\n",
            "validation  42  loss:  0.14996111392974854\n",
            "epoch:  43  loss:  0.15617813169956207\n",
            "validation  43  loss:  0.14946161210536957\n",
            "epoch:  44  loss:  0.15569467842578888\n",
            "validation  44  loss:  0.14897944033145905\n",
            "epoch:  45  loss:  0.15522921085357666\n",
            "validation  45  loss:  0.1485140174627304\n",
            "epoch:  46  loss:  0.15478064119815826\n",
            "validation  46  loss:  0.14806464314460754\n",
            "epoch:  47  loss:  0.15434803068637848\n",
            "validation  47  loss:  0.14763066172599792\n",
            "epoch:  48  loss:  0.1539304256439209\n",
            "validation  48  loss:  0.14721129834651947\n",
            "epoch:  49  loss:  0.15352700650691986\n",
            "validation  49  loss:  0.1468060314655304\n",
            "epoch:  50  loss:  0.1531369984149933\n",
            "validation  50  loss:  0.14641423523426056\n",
            "epoch:  51  loss:  0.15275974571704865\n",
            "validation  51  loss:  0.14603574573993683\n",
            "epoch:  52  loss:  0.15239465236663818\n",
            "validation  52  loss:  0.1456700563430786\n",
            "epoch:  53  loss:  0.15204116702079773\n",
            "validation  53  loss:  0.14531654119491577\n",
            "epoch:  54  loss:  0.15169872343540192\n",
            "validation  54  loss:  0.14497490227222443\n",
            "epoch:  55  loss:  0.15136682987213135\n",
            "validation  55  loss:  0.14464464783668518\n",
            "epoch:  56  loss:  0.15104500949382782\n",
            "validation  56  loss:  0.14432533085346222\n",
            "epoch:  57  loss:  0.1507328450679779\n",
            "validation  57  loss:  0.1440165638923645\n",
            "epoch:  58  loss:  0.15042999386787415\n",
            "validation  58  loss:  0.14371822774410248\n",
            "epoch:  59  loss:  0.1501360684633255\n",
            "validation  59  loss:  0.14342977106571198\n",
            "epoch:  60  loss:  0.14985065162181854\n",
            "validation  60  loss:  0.14315089583396912\n",
            "epoch:  61  loss:  0.1495734453201294\n",
            "validation  61  loss:  0.14288116991519928\n",
            "epoch:  62  loss:  0.1493040919303894\n",
            "validation  62  loss:  0.1426202654838562\n",
            "epoch:  63  loss:  0.14904220402240753\n",
            "validation  63  loss:  0.1423678994178772\n",
            "epoch:  64  loss:  0.14878760278224945\n",
            "validation  64  loss:  0.14212355017662048\n",
            "epoch:  65  loss:  0.1485399305820465\n",
            "validation  65  loss:  0.1418870985507965\n",
            "epoch:  66  loss:  0.1482989490032196\n",
            "validation  66  loss:  0.14165791869163513\n",
            "epoch:  67  loss:  0.14806434512138367\n",
            "validation  67  loss:  0.14143607020378113\n",
            "epoch:  68  loss:  0.14783591032028198\n",
            "validation  68  loss:  0.14122110605239868\n",
            "epoch:  69  loss:  0.1476133167743683\n",
            "validation  69  loss:  0.14101263880729675\n",
            "epoch:  70  loss:  0.14739637076854706\n",
            "validation  70  loss:  0.1408105343580246\n",
            "epoch:  71  loss:  0.14718475937843323\n",
            "validation  71  loss:  0.1406145691871643\n",
            "epoch:  72  loss:  0.14697828888893127\n",
            "validation  72  loss:  0.14042429625988007\n",
            "epoch:  73  loss:  0.14677676558494568\n",
            "validation  73  loss:  0.14023935794830322\n",
            "epoch:  74  loss:  0.14657999575138092\n",
            "validation  74  loss:  0.14005962014198303\n",
            "epoch:  75  loss:  0.14638769626617432\n",
            "validation  75  loss:  0.1398850381374359\n",
            "epoch:  76  loss:  0.14619971811771393\n",
            "validation  76  loss:  0.13971535861492157\n",
            "epoch:  77  loss:  0.14601589739322662\n",
            "validation  77  loss:  0.1395501345396042\n",
            "epoch:  78  loss:  0.1458359807729721\n",
            "validation  78  loss:  0.13938912749290466\n",
            "epoch:  79  loss:  0.14565981924533844\n",
            "validation  79  loss:  0.13923229277133942\n",
            "epoch:  80  loss:  0.14548730850219727\n",
            "validation  80  loss:  0.1390794813632965\n",
            "epoch:  81  loss:  0.14531826972961426\n",
            "validation  81  loss:  0.13893045485019684\n",
            "epoch:  82  loss:  0.1451525241136551\n",
            "validation  82  loss:  0.1387852430343628\n",
            "epoch:  83  loss:  0.1449899822473526\n",
            "validation  83  loss:  0.138643279671669\n",
            "epoch:  84  loss:  0.14483055472373962\n",
            "validation  84  loss:  0.13850465416908264\n",
            "epoch:  85  loss:  0.14467403292655945\n",
            "validation  85  loss:  0.1383691132068634\n",
            "epoch:  86  loss:  0.14452040195465088\n",
            "validation  86  loss:  0.13823647797107697\n",
            "epoch:  87  loss:  0.144369438290596\n",
            "validation  87  loss:  0.13810667395591736\n",
            "epoch:  88  loss:  0.14422102272510529\n",
            "validation  88  loss:  0.13797956705093384\n",
            "epoch:  89  loss:  0.14407512545585632\n",
            "validation  89  loss:  0.1378551423549652\n",
            "epoch:  90  loss:  0.14393161237239838\n",
            "validation  90  loss:  0.1377333402633667\n",
            "epoch:  91  loss:  0.14379045367240906\n",
            "validation  91  loss:  0.13761401176452637\n",
            "epoch:  92  loss:  0.14365150034427643\n",
            "validation  92  loss:  0.13749706745147705\n",
            "epoch:  93  loss:  0.1435147374868393\n",
            "validation  93  loss:  0.13738220930099487\n",
            "epoch:  94  loss:  0.1433800458908081\n",
            "validation  94  loss:  0.1372693032026291\n",
            "epoch:  95  loss:  0.14324736595153809\n",
            "validation  95  loss:  0.13715873658657074\n",
            "epoch:  96  loss:  0.14311659336090088\n",
            "validation  96  loss:  0.1370501071214676\n",
            "epoch:  97  loss:  0.14298774302005768\n",
            "validation  97  loss:  0.13694332540035248\n",
            "epoch:  98  loss:  0.14286069571971893\n",
            "validation  98  loss:  0.13683831691741943\n",
            "epoch:  99  loss:  0.14273536205291748\n",
            "validation  99  loss:  0.13673478364944458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "x_train = features_sketch_\n",
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yb_fGNxvEQK",
        "outputId": "aa4f5a73-2e12-4a5b-aba9-e4d625bd9821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.35159003734588623\n",
            "epoch:  1  loss:  0.3489125669002533\n",
            "epoch:  2  loss:  0.346277117729187\n",
            "epoch:  3  loss:  0.3436828553676605\n",
            "epoch:  4  loss:  0.3411288559436798\n",
            "epoch:  5  loss:  0.338614284992218\n",
            "epoch:  6  loss:  0.33613839745521545\n",
            "epoch:  7  loss:  0.3337003290653229\n",
            "epoch:  8  loss:  0.3312993347644806\n",
            "epoch:  9  loss:  0.3289347290992737\n",
            "epoch:  10  loss:  0.3266056478023529\n",
            "epoch:  11  loss:  0.3243115246295929\n",
            "epoch:  12  loss:  0.32205161452293396\n",
            "epoch:  13  loss:  0.3198252022266388\n",
            "epoch:  14  loss:  0.31763172149658203\n",
            "epoch:  15  loss:  0.31547051668167114\n",
            "epoch:  16  loss:  0.3133409321308136\n",
            "epoch:  17  loss:  0.3112424314022064\n",
            "epoch:  18  loss:  0.30917441844940186\n",
            "epoch:  19  loss:  0.30713629722595215\n",
            "epoch:  20  loss:  0.30512750148773193\n",
            "epoch:  21  loss:  0.30314749479293823\n",
            "epoch:  22  loss:  0.30119580030441284\n",
            "epoch:  23  loss:  0.2992718517780304\n",
            "epoch:  24  loss:  0.2973751723766327\n",
            "epoch:  25  loss:  0.2955053150653839\n",
            "epoch:  26  loss:  0.2936617136001587\n",
            "epoch:  27  loss:  0.2918439507484436\n",
            "epoch:  28  loss:  0.29005154967308044\n",
            "epoch:  29  loss:  0.28828415274620056\n",
            "epoch:  30  loss:  0.286541223526001\n",
            "epoch:  31  loss:  0.28482234477996826\n",
            "epoch:  32  loss:  0.28312715888023376\n",
            "epoch:  33  loss:  0.28145527839660645\n",
            "epoch:  34  loss:  0.2798062562942505\n",
            "epoch:  35  loss:  0.27817970514297485\n",
            "epoch:  36  loss:  0.2765752375125885\n",
            "epoch:  37  loss:  0.27499252557754517\n",
            "epoch:  38  loss:  0.2734312117099762\n",
            "epoch:  39  loss:  0.27189093828201294\n",
            "epoch:  40  loss:  0.27037131786346436\n",
            "epoch:  41  loss:  0.2688720226287842\n",
            "epoch:  42  loss:  0.26739275455474854\n",
            "epoch:  43  loss:  0.26593318581581116\n",
            "epoch:  44  loss:  0.26449301838874817\n",
            "epoch:  45  loss:  0.2630718946456909\n",
            "epoch:  46  loss:  0.2616695165634155\n",
            "epoch:  47  loss:  0.2602855861186981\n",
            "epoch:  48  loss:  0.2589198648929596\n",
            "epoch:  49  loss:  0.2575720250606537\n",
            "epoch:  50  loss:  0.2562417984008789\n",
            "epoch:  51  loss:  0.254928857088089\n",
            "epoch:  52  loss:  0.2536330223083496\n",
            "epoch:  53  loss:  0.2523539662361145\n",
            "epoch:  54  loss:  0.25109148025512695\n",
            "epoch:  55  loss:  0.24984528124332428\n",
            "epoch:  56  loss:  0.2486150860786438\n",
            "epoch:  57  loss:  0.24740071594715118\n",
            "epoch:  58  loss:  0.24620187282562256\n",
            "epoch:  59  loss:  0.24501842260360718\n",
            "epoch:  60  loss:  0.24385002255439758\n",
            "epoch:  61  loss:  0.24269649386405945\n",
            "epoch:  62  loss:  0.24155756831169128\n",
            "epoch:  63  loss:  0.24043312668800354\n",
            "epoch:  64  loss:  0.23932288587093353\n",
            "epoch:  65  loss:  0.23822663724422455\n",
            "epoch:  66  loss:  0.23714420199394226\n",
            "epoch:  67  loss:  0.23607532680034637\n",
            "epoch:  68  loss:  0.23501989245414734\n",
            "epoch:  69  loss:  0.23397763073444366\n",
            "epoch:  70  loss:  0.23294837772846222\n",
            "epoch:  71  loss:  0.23193193972110748\n",
            "epoch:  72  loss:  0.23092815279960632\n",
            "epoch:  73  loss:  0.22993676364421844\n",
            "epoch:  74  loss:  0.22895769774913788\n",
            "epoch:  75  loss:  0.22799071669578552\n",
            "epoch:  76  loss:  0.22703562676906586\n",
            "epoch:  77  loss:  0.22609232366085052\n",
            "epoch:  78  loss:  0.22516058385372162\n",
            "epoch:  79  loss:  0.2242402583360672\n",
            "epoch:  80  loss:  0.22333122789859772\n",
            "epoch:  81  loss:  0.22243322432041168\n",
            "epoch:  82  loss:  0.22154618799686432\n",
            "epoch:  83  loss:  0.2206699699163437\n",
            "epoch:  84  loss:  0.2198043316602707\n",
            "epoch:  85  loss:  0.21894919872283936\n",
            "epoch:  86  loss:  0.21810442209243774\n",
            "epoch:  87  loss:  0.21726980805397034\n",
            "epoch:  88  loss:  0.21644523739814758\n",
            "epoch:  89  loss:  0.21563059091567993\n",
            "epoch:  90  loss:  0.21482568979263306\n",
            "epoch:  91  loss:  0.2140304297208786\n",
            "epoch:  92  loss:  0.21324466168880463\n",
            "epoch:  93  loss:  0.21246826648712158\n",
            "epoch:  94  loss:  0.21170108020305634\n",
            "epoch:  95  loss:  0.2109430581331253\n",
            "epoch:  96  loss:  0.21019399166107178\n",
            "epoch:  97  loss:  0.2094537764787674\n",
            "epoch:  98  loss:  0.2087223082780838\n",
            "epoch:  99  loss:  0.20799943804740906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZQ4U8arvcP6",
        "outputId": "7f72a9c9-4b65-4ec7-c8d6-50f2ed7144c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.08207865804433823\n",
            "epoch:  1  loss:  0.0820590928196907\n",
            "epoch:  2  loss:  0.08203954249620438\n",
            "epoch:  3  loss:  0.08202002197504044\n",
            "epoch:  4  loss:  0.0820005014538765\n",
            "epoch:  5  loss:  0.08198101818561554\n",
            "epoch:  6  loss:  0.08196154981851578\n",
            "epoch:  7  loss:  0.0819421112537384\n",
            "epoch:  8  loss:  0.08192268013954163\n",
            "epoch:  9  loss:  0.08190327137708664\n",
            "epoch:  10  loss:  0.08188387751579285\n",
            "epoch:  11  loss:  0.08186452090740204\n",
            "epoch:  12  loss:  0.08184517174959183\n",
            "epoch:  13  loss:  0.08182583749294281\n",
            "epoch:  14  loss:  0.08180652558803558\n",
            "epoch:  15  loss:  0.08178724348545074\n",
            "epoch:  16  loss:  0.0817679762840271\n",
            "epoch:  17  loss:  0.08174871653318405\n",
            "epoch:  18  loss:  0.08172948658466339\n",
            "epoch:  19  loss:  0.08171027898788452\n",
            "epoch:  20  loss:  0.08169108629226685\n",
            "epoch:  21  loss:  0.08167190849781036\n",
            "epoch:  22  loss:  0.08165276050567627\n",
            "epoch:  23  loss:  0.08163362741470337\n",
            "epoch:  24  loss:  0.08161450922489166\n",
            "epoch:  25  loss:  0.08159539848566055\n",
            "epoch:  26  loss:  0.08157632499933243\n",
            "epoch:  27  loss:  0.0815572664141655\n",
            "epoch:  28  loss:  0.08153822273015976\n",
            "epoch:  29  loss:  0.08151920139789581\n",
            "epoch:  30  loss:  0.08150019496679306\n",
            "epoch:  31  loss:  0.0814812108874321\n",
            "epoch:  32  loss:  0.08146224915981293\n",
            "epoch:  33  loss:  0.08144329488277435\n",
            "epoch:  34  loss:  0.08142437040805817\n",
            "epoch:  35  loss:  0.08140544593334198\n",
            "epoch:  36  loss:  0.08138656616210938\n",
            "epoch:  37  loss:  0.08136769384145737\n",
            "epoch:  38  loss:  0.08134882897138596\n",
            "epoch:  39  loss:  0.08132999390363693\n",
            "epoch:  40  loss:  0.0813111811876297\n",
            "epoch:  41  loss:  0.08129237592220306\n",
            "epoch:  42  loss:  0.08127359300851822\n",
            "epoch:  43  loss:  0.08125482499599457\n",
            "epoch:  44  loss:  0.0812360867857933\n",
            "epoch:  45  loss:  0.08121735602617264\n",
            "epoch:  46  loss:  0.08119863271713257\n",
            "epoch:  47  loss:  0.08117993921041489\n",
            "epoch:  48  loss:  0.0811612606048584\n",
            "epoch:  49  loss:  0.0811426118016243\n",
            "epoch:  50  loss:  0.0811239629983902\n",
            "epoch:  51  loss:  0.08110532909631729\n",
            "epoch:  52  loss:  0.08108673244714737\n",
            "epoch:  53  loss:  0.08106815069913864\n",
            "epoch:  54  loss:  0.08104957640171051\n",
            "epoch:  55  loss:  0.08103103190660477\n",
            "epoch:  56  loss:  0.08101249486207962\n",
            "epoch:  57  loss:  0.08099397271871567\n",
            "epoch:  58  loss:  0.0809754729270935\n",
            "epoch:  59  loss:  0.08095698803663254\n",
            "epoch:  60  loss:  0.08093852549791336\n",
            "epoch:  61  loss:  0.08092007786035538\n",
            "epoch:  62  loss:  0.08090166002511978\n",
            "epoch:  63  loss:  0.08088322728872299\n",
            "epoch:  64  loss:  0.08086483180522919\n",
            "epoch:  65  loss:  0.08084645122289658\n",
            "epoch:  66  loss:  0.08082809299230576\n",
            "epoch:  67  loss:  0.08080974221229553\n",
            "epoch:  68  loss:  0.0807914212346077\n",
            "epoch:  69  loss:  0.08077310025691986\n",
            "epoch:  70  loss:  0.08075480908155441\n",
            "epoch:  71  loss:  0.08073653280735016\n",
            "epoch:  72  loss:  0.0807182714343071\n",
            "epoch:  73  loss:  0.08070002496242523\n",
            "epoch:  74  loss:  0.08068179339170456\n",
            "epoch:  75  loss:  0.08066359162330627\n",
            "epoch:  76  loss:  0.08064538985490799\n",
            "epoch:  77  loss:  0.08062721788883209\n",
            "epoch:  78  loss:  0.08060906082391739\n",
            "epoch:  79  loss:  0.08059091866016388\n",
            "epoch:  80  loss:  0.08057278394699097\n",
            "epoch:  81  loss:  0.08055467158555984\n",
            "epoch:  82  loss:  0.08053657412528992\n",
            "epoch:  83  loss:  0.08051849156618118\n",
            "epoch:  84  loss:  0.08050043135881424\n",
            "epoch:  85  loss:  0.08048240095376968\n",
            "epoch:  86  loss:  0.08046436309814453\n",
            "epoch:  87  loss:  0.08044635504484177\n",
            "epoch:  88  loss:  0.0804283544421196\n",
            "epoch:  89  loss:  0.08041036874055862\n",
            "epoch:  90  loss:  0.08039240539073944\n",
            "epoch:  91  loss:  0.08037445694208145\n",
            "epoch:  92  loss:  0.08035652339458466\n",
            "epoch:  93  loss:  0.08033860474824905\n",
            "epoch:  94  loss:  0.08032070845365524\n",
            "epoch:  95  loss:  0.08030282706022263\n",
            "epoch:  96  loss:  0.0802849531173706\n",
            "epoch:  97  loss:  0.08026709407567978\n",
            "epoch:  98  loss:  0.08024925738573074\n",
            "epoch:  99  loss:  0.0802314430475235\n",
            "epoch:  100  loss:  0.08021363615989685\n",
            "epoch:  101  loss:  0.080195851624012\n",
            "epoch:  102  loss:  0.08017807453870773\n",
            "epoch:  103  loss:  0.08016031980514526\n",
            "epoch:  104  loss:  0.08014257252216339\n",
            "epoch:  105  loss:  0.0801248550415039\n",
            "epoch:  106  loss:  0.08010713756084442\n",
            "epoch:  107  loss:  0.08008944988250732\n",
            "epoch:  108  loss:  0.08007176220417023\n",
            "epoch:  109  loss:  0.08005409687757492\n",
            "epoch:  110  loss:  0.08003644645214081\n",
            "epoch:  111  loss:  0.08001881092786789\n",
            "epoch:  112  loss:  0.08000120520591736\n",
            "epoch:  113  loss:  0.07998360693454742\n",
            "epoch:  114  loss:  0.07996601611375809\n",
            "epoch:  115  loss:  0.07994844019412994\n",
            "epoch:  116  loss:  0.07993089407682419\n",
            "epoch:  117  loss:  0.07991334795951843\n",
            "epoch:  118  loss:  0.07989582419395447\n",
            "epoch:  119  loss:  0.0798783078789711\n",
            "epoch:  120  loss:  0.07986080646514893\n",
            "epoch:  121  loss:  0.07984333485364914\n",
            "epoch:  122  loss:  0.07982586324214935\n",
            "epoch:  123  loss:  0.07980842143297195\n",
            "epoch:  124  loss:  0.07979099452495575\n",
            "epoch:  125  loss:  0.07977356761693954\n",
            "epoch:  126  loss:  0.07975616306066513\n",
            "epoch:  127  loss:  0.07973878085613251\n",
            "epoch:  128  loss:  0.07972140610218048\n",
            "epoch:  129  loss:  0.07970403879880905\n",
            "epoch:  130  loss:  0.07968670129776001\n",
            "epoch:  131  loss:  0.07966937869787216\n",
            "epoch:  132  loss:  0.07965205609798431\n",
            "epoch:  133  loss:  0.07963475584983826\n",
            "epoch:  134  loss:  0.0796174705028534\n",
            "epoch:  135  loss:  0.07960020750761032\n",
            "epoch:  136  loss:  0.07958295196294785\n",
            "epoch:  137  loss:  0.07956571131944656\n",
            "epoch:  138  loss:  0.07954848557710648\n",
            "epoch:  139  loss:  0.07953127473592758\n",
            "epoch:  140  loss:  0.07951407879590988\n",
            "epoch:  141  loss:  0.07949689775705338\n",
            "epoch:  142  loss:  0.07947973161935806\n",
            "epoch:  143  loss:  0.07946257293224335\n",
            "epoch:  144  loss:  0.07944543659687042\n",
            "epoch:  145  loss:  0.07942832261323929\n",
            "epoch:  146  loss:  0.07941120117902756\n",
            "epoch:  147  loss:  0.07939410954713821\n",
            "epoch:  148  loss:  0.07937703281641006\n",
            "epoch:  149  loss:  0.07935997098684311\n",
            "epoch:  150  loss:  0.07934292405843735\n",
            "epoch:  151  loss:  0.07932588458061218\n",
            "epoch:  152  loss:  0.07930886745452881\n",
            "epoch:  153  loss:  0.07929185777902603\n",
            "epoch:  154  loss:  0.07927485555410385\n",
            "epoch:  155  loss:  0.07925788313150406\n",
            "epoch:  156  loss:  0.07924091070890427\n",
            "epoch:  157  loss:  0.07922396808862686\n",
            "epoch:  158  loss:  0.07920703291893005\n",
            "epoch:  159  loss:  0.07919011265039444\n",
            "epoch:  160  loss:  0.07917319238185883\n",
            "epoch:  161  loss:  0.0791563093662262\n",
            "epoch:  162  loss:  0.07913943380117416\n",
            "epoch:  163  loss:  0.07912256568670273\n",
            "epoch:  164  loss:  0.07910571992397308\n",
            "epoch:  165  loss:  0.07908887416124344\n",
            "epoch:  166  loss:  0.07907205820083618\n",
            "epoch:  167  loss:  0.07905523478984833\n",
            "epoch:  168  loss:  0.07903844863176346\n",
            "epoch:  169  loss:  0.07902166992425919\n",
            "epoch:  170  loss:  0.07900490611791611\n",
            "epoch:  171  loss:  0.07898814231157303\n",
            "epoch:  172  loss:  0.07897141575813293\n",
            "epoch:  173  loss:  0.07895468920469284\n",
            "epoch:  174  loss:  0.07893797755241394\n",
            "epoch:  175  loss:  0.07892128825187683\n",
            "epoch:  176  loss:  0.07890458405017853\n",
            "epoch:  177  loss:  0.0788879320025444\n",
            "epoch:  178  loss:  0.07887127250432968\n",
            "epoch:  179  loss:  0.07885462045669556\n",
            "epoch:  180  loss:  0.07883800566196442\n",
            "epoch:  181  loss:  0.07882139086723328\n",
            "epoch:  182  loss:  0.07880478352308273\n",
            "epoch:  183  loss:  0.07878819108009338\n",
            "epoch:  184  loss:  0.07877162098884583\n",
            "epoch:  185  loss:  0.07875506579875946\n",
            "epoch:  186  loss:  0.07873851805925369\n",
            "epoch:  187  loss:  0.07872197777032852\n",
            "epoch:  188  loss:  0.07870545983314514\n",
            "epoch:  189  loss:  0.07868896424770355\n",
            "epoch:  190  loss:  0.07867246866226196\n",
            "epoch:  191  loss:  0.07865598797798157\n",
            "epoch:  192  loss:  0.07863952219486237\n",
            "epoch:  193  loss:  0.07862307876348495\n",
            "epoch:  194  loss:  0.07860663533210754\n",
            "epoch:  195  loss:  0.07859019935131073\n",
            "epoch:  196  loss:  0.0785738006234169\n",
            "epoch:  197  loss:  0.07855740189552307\n",
            "epoch:  198  loss:  0.07854101806879044\n",
            "epoch:  199  loss:  0.0785246416926384\n",
            "epoch:  200  loss:  0.07850829511880875\n",
            "epoch:  201  loss:  0.0784919410943985\n",
            "epoch:  202  loss:  0.07847561687231064\n",
            "epoch:  203  loss:  0.07845930010080338\n",
            "epoch:  204  loss:  0.07844298332929611\n",
            "epoch:  205  loss:  0.07842670381069183\n",
            "epoch:  206  loss:  0.07841042429208755\n",
            "epoch:  207  loss:  0.07839415222406387\n",
            "epoch:  208  loss:  0.07837790995836258\n",
            "epoch:  209  loss:  0.07836167514324188\n",
            "epoch:  210  loss:  0.07834544032812119\n",
            "epoch:  211  loss:  0.07832922786474228\n",
            "epoch:  212  loss:  0.07831302285194397\n",
            "epoch:  213  loss:  0.07829684019088745\n",
            "epoch:  214  loss:  0.07828067243099213\n",
            "epoch:  215  loss:  0.078264519572258\n",
            "epoch:  216  loss:  0.07824835926294327\n",
            "epoch:  217  loss:  0.07823223620653152\n",
            "epoch:  218  loss:  0.07821612060070038\n",
            "epoch:  219  loss:  0.07820000499486923\n",
            "epoch:  220  loss:  0.07818391174077988\n",
            "epoch:  221  loss:  0.07816783338785172\n",
            "epoch:  222  loss:  0.07815176248550415\n",
            "epoch:  223  loss:  0.07813571393489838\n",
            "epoch:  224  loss:  0.078119657933712\n",
            "epoch:  225  loss:  0.07810362428426743\n",
            "epoch:  226  loss:  0.07808761298656464\n",
            "epoch:  227  loss:  0.07807160913944244\n",
            "epoch:  228  loss:  0.07805561274290085\n",
            "epoch:  229  loss:  0.07803963869810104\n",
            "epoch:  230  loss:  0.07802367210388184\n",
            "epoch:  231  loss:  0.07800772041082382\n",
            "epoch:  232  loss:  0.0779917761683464\n",
            "epoch:  233  loss:  0.07797584682703018\n",
            "epoch:  234  loss:  0.07795993983745575\n",
            "epoch:  235  loss:  0.07794403284788132\n",
            "epoch:  236  loss:  0.07792814821004868\n",
            "epoch:  237  loss:  0.07791227102279663\n",
            "epoch:  238  loss:  0.07789640873670578\n",
            "epoch:  239  loss:  0.07788054645061493\n",
            "epoch:  240  loss:  0.07786471396684647\n",
            "epoch:  241  loss:  0.077848881483078\n",
            "epoch:  242  loss:  0.07783307135105133\n",
            "epoch:  243  loss:  0.07781726866960526\n",
            "epoch:  244  loss:  0.07780148088932037\n",
            "epoch:  245  loss:  0.07778570801019669\n",
            "epoch:  246  loss:  0.0777699425816536\n",
            "epoch:  247  loss:  0.0777541995048523\n",
            "epoch:  248  loss:  0.077738456428051\n",
            "epoch:  249  loss:  0.07772273570299149\n",
            "epoch:  250  loss:  0.07770701497793198\n",
            "epoch:  251  loss:  0.07769131660461426\n",
            "epoch:  252  loss:  0.07767563313245773\n",
            "epoch:  253  loss:  0.07765994966030121\n",
            "epoch:  254  loss:  0.07764428853988647\n",
            "epoch:  255  loss:  0.07762864232063293\n",
            "epoch:  256  loss:  0.0776129961013794\n",
            "epoch:  257  loss:  0.07759736478328705\n",
            "epoch:  258  loss:  0.07758175581693649\n",
            "epoch:  259  loss:  0.07756616175174713\n",
            "epoch:  260  loss:  0.07755057513713837\n",
            "epoch:  261  loss:  0.0775349959731102\n",
            "epoch:  262  loss:  0.07751943171024323\n",
            "epoch:  263  loss:  0.07750387489795685\n",
            "epoch:  264  loss:  0.07748833298683167\n",
            "epoch:  265  loss:  0.07747280597686768\n",
            "epoch:  266  loss:  0.07745729386806488\n",
            "epoch:  267  loss:  0.07744179666042328\n",
            "epoch:  268  loss:  0.07742629945278168\n",
            "epoch:  269  loss:  0.07741081714630127\n",
            "epoch:  270  loss:  0.07739535719156265\n",
            "epoch:  271  loss:  0.07737989723682404\n",
            "epoch:  272  loss:  0.07736445963382721\n",
            "epoch:  273  loss:  0.07734902948141098\n",
            "epoch:  274  loss:  0.07733361423015594\n",
            "epoch:  275  loss:  0.07731819897890091\n",
            "epoch:  276  loss:  0.07730281352996826\n",
            "epoch:  277  loss:  0.07728742808103561\n",
            "epoch:  278  loss:  0.07727206498384476\n",
            "epoch:  279  loss:  0.0772567018866539\n",
            "epoch:  280  loss:  0.07724134624004364\n",
            "epoch:  281  loss:  0.07722602039575577\n",
            "epoch:  282  loss:  0.0772106945514679\n",
            "epoch:  283  loss:  0.07719538360834122\n",
            "epoch:  284  loss:  0.07718009501695633\n",
            "epoch:  285  loss:  0.07716479897499084\n",
            "epoch:  286  loss:  0.07714952528476715\n",
            "epoch:  287  loss:  0.07713426649570465\n",
            "epoch:  288  loss:  0.07711900770664215\n",
            "epoch:  289  loss:  0.07710377871990204\n",
            "epoch:  290  loss:  0.07708854973316193\n",
            "epoch:  291  loss:  0.07707333564758301\n",
            "epoch:  292  loss:  0.07705813646316528\n",
            "epoch:  293  loss:  0.07704295217990875\n",
            "epoch:  294  loss:  0.07702776044607162\n",
            "epoch:  295  loss:  0.07701259851455688\n",
            "epoch:  296  loss:  0.07699744403362274\n",
            "epoch:  297  loss:  0.07698230445384979\n",
            "epoch:  298  loss:  0.07696716487407684\n",
            "epoch:  299  loss:  0.07695205509662628\n",
            "epoch:  300  loss:  0.07693693786859512\n",
            "epoch:  301  loss:  0.07692183554172516\n",
            "epoch:  302  loss:  0.07690677046775818\n",
            "epoch:  303  loss:  0.07689168304204941\n",
            "epoch:  304  loss:  0.07687661796808243\n",
            "epoch:  305  loss:  0.07686158269643784\n",
            "epoch:  306  loss:  0.07684653997421265\n",
            "epoch:  307  loss:  0.07683151215314865\n",
            "epoch:  308  loss:  0.07681649178266525\n",
            "epoch:  309  loss:  0.07680150121450424\n",
            "epoch:  310  loss:  0.07678649574518204\n",
            "epoch:  311  loss:  0.07677152007818222\n",
            "epoch:  312  loss:  0.076756551861763\n",
            "epoch:  313  loss:  0.07674159109592438\n",
            "epoch:  314  loss:  0.07672665268182755\n",
            "epoch:  315  loss:  0.07671171426773071\n",
            "epoch:  316  loss:  0.07669679820537567\n",
            "epoch:  317  loss:  0.07668188214302063\n",
            "epoch:  318  loss:  0.07666697353124619\n",
            "epoch:  319  loss:  0.07665210217237473\n",
            "epoch:  320  loss:  0.07663722336292267\n",
            "epoch:  321  loss:  0.0766223594546318\n",
            "epoch:  322  loss:  0.07660750299692154\n",
            "epoch:  323  loss:  0.07659266144037247\n",
            "epoch:  324  loss:  0.07657782733440399\n",
            "epoch:  325  loss:  0.07656300067901611\n",
            "epoch:  326  loss:  0.07654820382595062\n",
            "epoch:  327  loss:  0.07653339952230453\n",
            "epoch:  328  loss:  0.07651861757040024\n",
            "epoch:  329  loss:  0.07650384306907654\n",
            "epoch:  330  loss:  0.07648909091949463\n",
            "epoch:  331  loss:  0.07647433131933212\n",
            "epoch:  332  loss:  0.07645959407091141\n",
            "epoch:  333  loss:  0.07644486427307129\n",
            "epoch:  334  loss:  0.07643014192581177\n",
            "epoch:  335  loss:  0.07641543447971344\n",
            "epoch:  336  loss:  0.0764007493853569\n",
            "epoch:  337  loss:  0.07638606429100037\n",
            "epoch:  338  loss:  0.07637138664722443\n",
            "epoch:  339  loss:  0.07635672390460968\n",
            "epoch:  340  loss:  0.07634206861257553\n",
            "epoch:  341  loss:  0.07632743567228317\n",
            "epoch:  342  loss:  0.07631280273199081\n",
            "epoch:  343  loss:  0.07629819214344025\n",
            "epoch:  344  loss:  0.07628358155488968\n",
            "epoch:  345  loss:  0.0762689858675003\n",
            "epoch:  346  loss:  0.07625441253185272\n",
            "epoch:  347  loss:  0.07623983919620514\n",
            "epoch:  348  loss:  0.07622527331113815\n",
            "epoch:  349  loss:  0.07621072232723236\n",
            "epoch:  350  loss:  0.07619618624448776\n",
            "epoch:  351  loss:  0.07618166506290436\n",
            "epoch:  352  loss:  0.07616715133190155\n",
            "epoch:  353  loss:  0.07615265250205994\n",
            "epoch:  354  loss:  0.07613814622163773\n",
            "epoch:  355  loss:  0.07612365484237671\n",
            "epoch:  356  loss:  0.07610919326543808\n",
            "epoch:  357  loss:  0.07609471678733826\n",
            "epoch:  358  loss:  0.07608028501272202\n",
            "epoch:  359  loss:  0.07606583833694458\n",
            "epoch:  360  loss:  0.07605141401290894\n",
            "epoch:  361  loss:  0.07603700459003448\n",
            "epoch:  362  loss:  0.07602258771657944\n",
            "epoch:  363  loss:  0.07600819319486618\n",
            "epoch:  364  loss:  0.07599380612373352\n",
            "epoch:  365  loss:  0.07597944140434265\n",
            "epoch:  366  loss:  0.07596507668495178\n",
            "epoch:  367  loss:  0.07595071941614151\n",
            "epoch:  368  loss:  0.07593638449907303\n",
            "epoch:  369  loss:  0.07592204958200455\n",
            "epoch:  370  loss:  0.07590773701667786\n",
            "epoch:  371  loss:  0.07589342445135117\n",
            "epoch:  372  loss:  0.07587913423776627\n",
            "epoch:  373  loss:  0.07586484402418137\n",
            "epoch:  374  loss:  0.07585056871175766\n",
            "epoch:  375  loss:  0.07583630084991455\n",
            "epoch:  376  loss:  0.07582204788923264\n",
            "epoch:  377  loss:  0.07580780237913132\n",
            "epoch:  378  loss:  0.07579357177019119\n",
            "epoch:  379  loss:  0.07577935606241226\n",
            "epoch:  380  loss:  0.07576514780521393\n",
            "epoch:  381  loss:  0.07575094699859619\n",
            "epoch:  382  loss:  0.07573675364255905\n",
            "epoch:  383  loss:  0.0757225826382637\n",
            "epoch:  384  loss:  0.07570840418338776\n",
            "epoch:  385  loss:  0.0756942480802536\n",
            "epoch:  386  loss:  0.07568010687828064\n",
            "epoch:  387  loss:  0.07566597312688828\n",
            "epoch:  388  loss:  0.07565183937549591\n",
            "epoch:  389  loss:  0.07563772797584534\n",
            "epoch:  390  loss:  0.07562361657619476\n",
            "epoch:  391  loss:  0.07560952752828598\n",
            "epoch:  392  loss:  0.0755954459309578\n",
            "epoch:  393  loss:  0.0755813717842102\n",
            "epoch:  394  loss:  0.07556731253862381\n",
            "epoch:  395  loss:  0.07555326074361801\n",
            "epoch:  396  loss:  0.07553922384977341\n",
            "epoch:  397  loss:  0.0755251869559288\n",
            "epoch:  398  loss:  0.07551117241382599\n",
            "epoch:  399  loss:  0.07549716532230377\n",
            "epoch:  400  loss:  0.07548317313194275\n",
            "epoch:  401  loss:  0.07546918094158173\n",
            "epoch:  402  loss:  0.0754552036523819\n",
            "epoch:  403  loss:  0.07544124126434326\n",
            "epoch:  404  loss:  0.07542728632688522\n",
            "epoch:  405  loss:  0.07541333884000778\n",
            "epoch:  406  loss:  0.07539939880371094\n",
            "epoch:  407  loss:  0.07538547366857529\n",
            "epoch:  408  loss:  0.07537155598402023\n",
            "epoch:  409  loss:  0.07535766065120697\n",
            "epoch:  410  loss:  0.07534375786781311\n",
            "epoch:  411  loss:  0.07532987743616104\n",
            "epoch:  412  loss:  0.07531601935625076\n",
            "epoch:  413  loss:  0.07530214637517929\n",
            "epoch:  414  loss:  0.0752883031964302\n",
            "epoch:  415  loss:  0.07527446001768112\n",
            "epoch:  416  loss:  0.07526061683893204\n",
            "epoch:  417  loss:  0.07524680346250534\n",
            "epoch:  418  loss:  0.07523299753665924\n",
            "epoch:  419  loss:  0.07521919161081314\n",
            "epoch:  420  loss:  0.07520540803670883\n",
            "epoch:  421  loss:  0.07519162446260452\n",
            "epoch:  422  loss:  0.07517785578966141\n",
            "epoch:  423  loss:  0.07516409456729889\n",
            "epoch:  424  loss:  0.07515035569667816\n",
            "epoch:  425  loss:  0.07513661682605743\n",
            "epoch:  426  loss:  0.0751228854060173\n",
            "epoch:  427  loss:  0.07510916888713837\n",
            "epoch:  428  loss:  0.07509545981884003\n",
            "epoch:  429  loss:  0.07508177310228348\n",
            "epoch:  430  loss:  0.07506807148456573\n",
            "epoch:  431  loss:  0.07505439966917038\n",
            "epoch:  432  loss:  0.07504073530435562\n",
            "epoch:  433  loss:  0.07502707839012146\n",
            "epoch:  434  loss:  0.0750134289264679\n",
            "epoch:  435  loss:  0.07499979436397552\n",
            "epoch:  436  loss:  0.07498617470264435\n",
            "epoch:  437  loss:  0.07497255504131317\n",
            "epoch:  438  loss:  0.07495895028114319\n",
            "epoch:  439  loss:  0.0749453604221344\n",
            "epoch:  440  loss:  0.07493177801370621\n",
            "epoch:  441  loss:  0.07491820305585861\n",
            "epoch:  442  loss:  0.07490463554859161\n",
            "epoch:  443  loss:  0.07489108294248581\n",
            "epoch:  444  loss:  0.07487753033638\n",
            "epoch:  445  loss:  0.07486400008201599\n",
            "epoch:  446  loss:  0.07485047727823257\n",
            "epoch:  447  loss:  0.07483696937561035\n",
            "epoch:  448  loss:  0.07482345402240753\n",
            "epoch:  449  loss:  0.0748099684715271\n",
            "epoch:  450  loss:  0.07479647547006607\n",
            "epoch:  451  loss:  0.07478300482034683\n",
            "epoch:  452  loss:  0.07476954907178879\n",
            "epoch:  453  loss:  0.07475607842206955\n",
            "epoch:  454  loss:  0.0747426450252533\n",
            "epoch:  455  loss:  0.07472921162843704\n",
            "epoch:  456  loss:  0.07471578568220139\n",
            "epoch:  457  loss:  0.07470237463712692\n",
            "epoch:  458  loss:  0.07468897104263306\n",
            "epoch:  459  loss:  0.07467557489871979\n",
            "epoch:  460  loss:  0.07466218620538712\n",
            "epoch:  461  loss:  0.07464881986379623\n",
            "epoch:  462  loss:  0.07463545352220535\n",
            "epoch:  463  loss:  0.07462209463119507\n",
            "epoch:  464  loss:  0.07460875064134598\n",
            "epoch:  465  loss:  0.07459542155265808\n",
            "epoch:  466  loss:  0.07458209246397018\n",
            "epoch:  467  loss:  0.07456877827644348\n",
            "epoch:  468  loss:  0.07455547899007797\n",
            "epoch:  469  loss:  0.07454216480255127\n",
            "epoch:  470  loss:  0.07452889531850815\n",
            "epoch:  471  loss:  0.07451561093330383\n",
            "epoch:  472  loss:  0.07450234889984131\n",
            "epoch:  473  loss:  0.07448909431695938\n",
            "epoch:  474  loss:  0.07447584718465805\n",
            "epoch:  475  loss:  0.07446260005235672\n",
            "epoch:  476  loss:  0.07444939017295837\n",
            "epoch:  477  loss:  0.07443616539239883\n",
            "epoch:  478  loss:  0.07442296296358109\n",
            "epoch:  479  loss:  0.07440976053476334\n",
            "epoch:  480  loss:  0.07439657300710678\n",
            "epoch:  481  loss:  0.07438340038061142\n",
            "epoch:  482  loss:  0.07437024265527725\n",
            "epoch:  483  loss:  0.07435707002878189\n",
            "epoch:  484  loss:  0.07434392720460892\n",
            "epoch:  485  loss:  0.07433078438043594\n",
            "epoch:  486  loss:  0.07431766390800476\n",
            "epoch:  487  loss:  0.07430454343557358\n",
            "epoch:  488  loss:  0.07429143786430359\n",
            "epoch:  489  loss:  0.0742783173918724\n",
            "epoch:  490  loss:  0.0742652416229248\n",
            "epoch:  491  loss:  0.0742521658539772\n",
            "epoch:  492  loss:  0.0742390900850296\n",
            "epoch:  493  loss:  0.0742260292172432\n",
            "epoch:  494  loss:  0.07421298325061798\n",
            "epoch:  495  loss:  0.07419993728399277\n",
            "epoch:  496  loss:  0.07418691366910934\n",
            "epoch:  497  loss:  0.07417388260364532\n",
            "epoch:  498  loss:  0.0741608738899231\n",
            "epoch:  499  loss:  0.07414786517620087\n",
            "epoch:  500  loss:  0.07413487136363983\n",
            "epoch:  501  loss:  0.07412189245223999\n",
            "epoch:  502  loss:  0.07410891354084015\n",
            "epoch:  503  loss:  0.0740959495306015\n",
            "epoch:  504  loss:  0.07408299297094345\n",
            "epoch:  505  loss:  0.074070043861866\n",
            "epoch:  506  loss:  0.07405710965394974\n",
            "epoch:  507  loss:  0.07404418289661407\n",
            "epoch:  508  loss:  0.0740312710404396\n",
            "epoch:  509  loss:  0.07401835918426514\n",
            "epoch:  510  loss:  0.07400546222925186\n",
            "epoch:  511  loss:  0.07399256527423859\n",
            "epoch:  512  loss:  0.0739796906709671\n",
            "epoch:  513  loss:  0.07396683096885681\n",
            "epoch:  514  loss:  0.07395396381616592\n",
            "epoch:  515  loss:  0.07394111901521683\n",
            "epoch:  516  loss:  0.07392826676368713\n",
            "epoch:  517  loss:  0.07391544431447983\n",
            "epoch:  518  loss:  0.07390262186527252\n",
            "epoch:  519  loss:  0.07388979941606522\n",
            "epoch:  520  loss:  0.0738769993185997\n",
            "epoch:  521  loss:  0.07386421412229538\n",
            "epoch:  522  loss:  0.07385141402482986\n",
            "epoch:  523  loss:  0.07383864372968674\n",
            "epoch:  524  loss:  0.0738258808851242\n",
            "epoch:  525  loss:  0.07381311804056168\n",
            "epoch:  526  loss:  0.07380037009716034\n",
            "epoch:  527  loss:  0.0737876296043396\n",
            "epoch:  528  loss:  0.07377491146326065\n",
            "epoch:  529  loss:  0.0737621933221817\n",
            "epoch:  530  loss:  0.07374947518110275\n",
            "epoch:  531  loss:  0.073736771941185\n",
            "epoch:  532  loss:  0.07372408360242844\n",
            "epoch:  533  loss:  0.07371139526367188\n",
            "epoch:  534  loss:  0.0736987292766571\n",
            "epoch:  535  loss:  0.07368606328964233\n",
            "epoch:  536  loss:  0.07367341220378876\n",
            "epoch:  537  loss:  0.07366076856851578\n",
            "epoch:  538  loss:  0.0736481323838234\n",
            "epoch:  539  loss:  0.07363550364971161\n",
            "epoch:  540  loss:  0.07362288236618042\n",
            "epoch:  541  loss:  0.07361027598381042\n",
            "epoch:  542  loss:  0.07359766960144043\n",
            "epoch:  543  loss:  0.07358509302139282\n",
            "epoch:  544  loss:  0.07357250899076462\n",
            "epoch:  545  loss:  0.07355993986129761\n",
            "epoch:  546  loss:  0.0735473781824112\n",
            "epoch:  547  loss:  0.07353482395410538\n",
            "epoch:  548  loss:  0.07352226972579956\n",
            "epoch:  549  loss:  0.07350974529981613\n",
            "epoch:  550  loss:  0.0734972134232521\n",
            "epoch:  551  loss:  0.07348469644784927\n",
            "epoch:  552  loss:  0.07347218692302704\n",
            "epoch:  553  loss:  0.073459692299366\n",
            "epoch:  554  loss:  0.07344719022512436\n",
            "epoch:  555  loss:  0.0734347254037857\n",
            "epoch:  556  loss:  0.07342223823070526\n",
            "epoch:  557  loss:  0.0734097883105278\n",
            "epoch:  558  loss:  0.07339732348918915\n",
            "epoch:  559  loss:  0.07338488847017288\n",
            "epoch:  560  loss:  0.07337245345115662\n",
            "epoch:  561  loss:  0.07336003333330154\n",
            "epoch:  562  loss:  0.07334761321544647\n",
            "epoch:  563  loss:  0.073335200548172\n",
            "epoch:  564  loss:  0.07332280278205872\n",
            "epoch:  565  loss:  0.07331041991710663\n",
            "epoch:  566  loss:  0.07329803705215454\n",
            "epoch:  567  loss:  0.07328566163778305\n",
            "epoch:  568  loss:  0.07327330857515335\n",
            "epoch:  569  loss:  0.07326094061136246\n",
            "epoch:  570  loss:  0.07324859499931335\n",
            "epoch:  571  loss:  0.07323625683784485\n",
            "epoch:  572  loss:  0.07322394102811813\n",
            "epoch:  573  loss:  0.07321162521839142\n",
            "epoch:  574  loss:  0.0731993094086647\n",
            "epoch:  575  loss:  0.07318700850009918\n",
            "epoch:  576  loss:  0.07317472249269485\n",
            "epoch:  577  loss:  0.07316244393587112\n",
            "epoch:  578  loss:  0.07315017282962799\n",
            "epoch:  579  loss:  0.07313790172338486\n",
            "epoch:  580  loss:  0.07312565296888351\n",
            "epoch:  581  loss:  0.07311340421438217\n",
            "epoch:  582  loss:  0.07310116291046143\n",
            "epoch:  583  loss:  0.07308894395828247\n",
            "epoch:  584  loss:  0.07307671010494232\n",
            "epoch:  585  loss:  0.07306450605392456\n",
            "epoch:  586  loss:  0.0730523094534874\n",
            "epoch:  587  loss:  0.07304011285305023\n",
            "epoch:  588  loss:  0.07302793115377426\n",
            "epoch:  589  loss:  0.07301576435565948\n",
            "epoch:  590  loss:  0.07300358265638351\n",
            "epoch:  591  loss:  0.07299143075942993\n",
            "epoch:  592  loss:  0.07297928631305695\n",
            "epoch:  593  loss:  0.07296714931726456\n",
            "epoch:  594  loss:  0.07295501977205276\n",
            "epoch:  595  loss:  0.07294288277626038\n",
            "epoch:  596  loss:  0.07293077558279037\n",
            "epoch:  597  loss:  0.07291866838932037\n",
            "epoch:  598  loss:  0.07290656864643097\n",
            "epoch:  599  loss:  0.07289449125528336\n",
            "epoch:  600  loss:  0.07288240641355515\n",
            "epoch:  601  loss:  0.07287033647298813\n",
            "epoch:  602  loss:  0.0728582814335823\n",
            "epoch:  603  loss:  0.07284623384475708\n",
            "epoch:  604  loss:  0.07283419370651245\n",
            "epoch:  605  loss:  0.07282215356826782\n",
            "epoch:  606  loss:  0.07281013578176498\n",
            "epoch:  607  loss:  0.07279811054468155\n",
            "epoch:  608  loss:  0.0727861076593399\n",
            "epoch:  609  loss:  0.07277411222457886\n",
            "epoch:  610  loss:  0.07276211678981781\n",
            "epoch:  611  loss:  0.07275012880563736\n",
            "epoch:  612  loss:  0.0727381557226181\n",
            "epoch:  613  loss:  0.07272619754076004\n",
            "epoch:  614  loss:  0.07271423935890198\n",
            "epoch:  615  loss:  0.07270228862762451\n",
            "epoch:  616  loss:  0.07269034534692764\n",
            "epoch:  617  loss:  0.07267842441797256\n",
            "epoch:  618  loss:  0.07266649603843689\n",
            "epoch:  619  loss:  0.072654590010643\n",
            "epoch:  620  loss:  0.07264268398284912\n",
            "epoch:  621  loss:  0.07263078540563583\n",
            "epoch:  622  loss:  0.07261889427900314\n",
            "epoch:  623  loss:  0.07260701805353165\n",
            "epoch:  624  loss:  0.07259514927864075\n",
            "epoch:  625  loss:  0.07258328050374985\n",
            "epoch:  626  loss:  0.07257143408060074\n",
            "epoch:  627  loss:  0.07255959510803223\n",
            "epoch:  628  loss:  0.07254774868488312\n",
            "epoch:  629  loss:  0.0725359246134758\n",
            "epoch:  630  loss:  0.07252410054206848\n",
            "epoch:  631  loss:  0.07251229137182236\n",
            "epoch:  632  loss:  0.07250048965215683\n",
            "epoch:  633  loss:  0.0724887028336525\n",
            "epoch:  634  loss:  0.07247691601514816\n",
            "epoch:  635  loss:  0.07246514409780502\n",
            "epoch:  636  loss:  0.07245336472988129\n",
            "epoch:  637  loss:  0.07244161516427994\n",
            "epoch:  638  loss:  0.07242986559867859\n",
            "epoch:  639  loss:  0.07241811603307724\n",
            "epoch:  640  loss:  0.07240638881921768\n",
            "epoch:  641  loss:  0.07239465415477753\n",
            "epoch:  642  loss:  0.07238294929265976\n",
            "epoch:  643  loss:  0.0723712295293808\n",
            "epoch:  644  loss:  0.07235953211784363\n",
            "epoch:  645  loss:  0.07234783470630646\n",
            "epoch:  646  loss:  0.07233615219593048\n",
            "epoch:  647  loss:  0.0723244771361351\n",
            "epoch:  648  loss:  0.07231281697750092\n",
            "epoch:  649  loss:  0.07230115681886673\n",
            "epoch:  650  loss:  0.07228951156139374\n",
            "epoch:  651  loss:  0.07227785885334015\n",
            "epoch:  652  loss:  0.07226622849702835\n",
            "epoch:  653  loss:  0.07225461304187775\n",
            "epoch:  654  loss:  0.07224299013614655\n",
            "epoch:  655  loss:  0.07223138213157654\n",
            "epoch:  656  loss:  0.07221977412700653\n",
            "epoch:  657  loss:  0.07220818102359772\n",
            "epoch:  658  loss:  0.0721966028213501\n",
            "epoch:  659  loss:  0.07218503206968307\n",
            "epoch:  660  loss:  0.07217346131801605\n",
            "epoch:  661  loss:  0.07216189801692963\n",
            "epoch:  662  loss:  0.0721503496170044\n",
            "epoch:  663  loss:  0.07213881611824036\n",
            "epoch:  664  loss:  0.07212727516889572\n",
            "epoch:  665  loss:  0.07211576402187347\n",
            "epoch:  666  loss:  0.07210424542427063\n",
            "epoch:  667  loss:  0.07209272682666779\n",
            "epoch:  668  loss:  0.07208123058080673\n",
            "epoch:  669  loss:  0.07206973433494568\n",
            "epoch:  670  loss:  0.07205826044082642\n",
            "epoch:  671  loss:  0.07204677164554596\n",
            "epoch:  672  loss:  0.0720353052020073\n",
            "epoch:  673  loss:  0.07202384620904922\n",
            "epoch:  674  loss:  0.07201239466667175\n",
            "epoch:  675  loss:  0.07200095057487488\n",
            "epoch:  676  loss:  0.0719895139336586\n",
            "epoch:  677  loss:  0.07197809964418411\n",
            "epoch:  678  loss:  0.07196667790412903\n",
            "epoch:  679  loss:  0.07195526361465454\n",
            "epoch:  680  loss:  0.07194386422634125\n",
            "epoch:  681  loss:  0.07193246483802795\n",
            "epoch:  682  loss:  0.07192108035087585\n",
            "epoch:  683  loss:  0.07190971076488495\n",
            "epoch:  684  loss:  0.07189834117889404\n",
            "epoch:  685  loss:  0.07188697904348373\n",
            "epoch:  686  loss:  0.07187562435865402\n",
            "epoch:  687  loss:  0.07186426967382431\n",
            "epoch:  688  loss:  0.07185293734073639\n",
            "epoch:  689  loss:  0.07184161245822906\n",
            "epoch:  690  loss:  0.07183029502630234\n",
            "epoch:  691  loss:  0.07181897759437561\n",
            "epoch:  692  loss:  0.07180766761302948\n",
            "epoch:  693  loss:  0.07179637998342514\n",
            "epoch:  694  loss:  0.0717850849032402\n",
            "epoch:  695  loss:  0.07177380472421646\n",
            "epoch:  696  loss:  0.07176253944635391\n",
            "epoch:  697  loss:  0.07175126671791077\n",
            "epoch:  698  loss:  0.07174001634120941\n",
            "epoch:  699  loss:  0.07172876596450806\n",
            "epoch:  700  loss:  0.0717175230383873\n",
            "epoch:  701  loss:  0.07170630246400833\n",
            "epoch:  702  loss:  0.07169507443904877\n",
            "epoch:  703  loss:  0.0716838538646698\n",
            "epoch:  704  loss:  0.07167265564203262\n",
            "epoch:  705  loss:  0.07166143506765366\n",
            "epoch:  706  loss:  0.07165025174617767\n",
            "epoch:  707  loss:  0.07163906842470169\n",
            "epoch:  708  loss:  0.0716278925538063\n",
            "epoch:  709  loss:  0.07161673158407211\n",
            "epoch:  710  loss:  0.07160556316375732\n",
            "epoch:  711  loss:  0.07159440964460373\n",
            "epoch:  712  loss:  0.07158325612545013\n",
            "epoch:  713  loss:  0.07157212495803833\n",
            "epoch:  714  loss:  0.07156099379062653\n",
            "epoch:  715  loss:  0.07154987007379532\n",
            "epoch:  716  loss:  0.0715387612581253\n",
            "epoch:  717  loss:  0.07152766734361649\n",
            "epoch:  718  loss:  0.07151656597852707\n",
            "epoch:  719  loss:  0.07150547951459885\n",
            "epoch:  720  loss:  0.07149439305067062\n",
            "epoch:  721  loss:  0.071483314037323\n",
            "epoch:  722  loss:  0.07147224992513657\n",
            "epoch:  723  loss:  0.07146120816469193\n",
            "epoch:  724  loss:  0.07145015150308609\n",
            "epoch:  725  loss:  0.07143910229206085\n",
            "epoch:  726  loss:  0.07142806798219681\n",
            "epoch:  727  loss:  0.07141704112291336\n",
            "epoch:  728  loss:  0.07140602916479111\n",
            "epoch:  729  loss:  0.07139501720666885\n",
            "epoch:  730  loss:  0.0713840201497078\n",
            "epoch:  731  loss:  0.07137301564216614\n",
            "epoch:  732  loss:  0.07136203348636627\n",
            "epoch:  733  loss:  0.0713510513305664\n",
            "epoch:  734  loss:  0.07134007662534714\n",
            "epoch:  735  loss:  0.07132911682128906\n",
            "epoch:  736  loss:  0.07131816446781158\n",
            "epoch:  737  loss:  0.07130720466375351\n",
            "epoch:  738  loss:  0.07129627466201782\n",
            "epoch:  739  loss:  0.07128532975912094\n",
            "epoch:  740  loss:  0.07127441465854645\n",
            "epoch:  741  loss:  0.07126349955797195\n",
            "epoch:  742  loss:  0.07125258445739746\n",
            "epoch:  743  loss:  0.07124169170856476\n",
            "epoch:  744  loss:  0.07123079895973206\n",
            "epoch:  745  loss:  0.07121989876031876\n",
            "epoch:  746  loss:  0.07120902836322784\n",
            "epoch:  747  loss:  0.07119815051555634\n",
            "epoch:  748  loss:  0.07118729501962662\n",
            "epoch:  749  loss:  0.0711764395236969\n",
            "epoch:  750  loss:  0.07116559147834778\n",
            "epoch:  751  loss:  0.07115475833415985\n",
            "epoch:  752  loss:  0.07114391028881073\n",
            "epoch:  753  loss:  0.071133092045784\n",
            "epoch:  754  loss:  0.07112228125333786\n",
            "epoch:  755  loss:  0.07111146301031113\n",
            "epoch:  756  loss:  0.07110065966844559\n",
            "epoch:  757  loss:  0.07108987122774124\n",
            "epoch:  758  loss:  0.0710790753364563\n",
            "epoch:  759  loss:  0.07106830924749374\n",
            "epoch:  760  loss:  0.07105753570795059\n",
            "epoch:  761  loss:  0.07104677706956863\n",
            "epoch:  762  loss:  0.07103601843118668\n",
            "epoch:  763  loss:  0.07102527469396591\n",
            "epoch:  764  loss:  0.07101452350616455\n",
            "epoch:  765  loss:  0.07100380212068558\n",
            "epoch:  766  loss:  0.07099307328462601\n",
            "epoch:  767  loss:  0.07098234444856644\n",
            "epoch:  768  loss:  0.07097163796424866\n",
            "epoch:  769  loss:  0.07096093893051147\n",
            "epoch:  770  loss:  0.07095023989677429\n",
            "epoch:  771  loss:  0.0709395557641983\n",
            "epoch:  772  loss:  0.07092887908220291\n",
            "epoch:  773  loss:  0.07091820240020752\n",
            "epoch:  774  loss:  0.07090753316879272\n",
            "epoch:  775  loss:  0.07089687883853912\n",
            "epoch:  776  loss:  0.07088623195886612\n",
            "epoch:  777  loss:  0.07087559252977371\n",
            "epoch:  778  loss:  0.0708649531006813\n",
            "epoch:  779  loss:  0.07085432857275009\n",
            "epoch:  780  loss:  0.07084371149539948\n",
            "epoch:  781  loss:  0.07083309441804886\n",
            "epoch:  782  loss:  0.07082249224185944\n",
            "epoch:  783  loss:  0.07081189751625061\n",
            "epoch:  784  loss:  0.07080130279064178\n",
            "epoch:  785  loss:  0.07079072296619415\n",
            "epoch:  786  loss:  0.07078014314174652\n",
            "epoch:  787  loss:  0.07076958566904068\n",
            "epoch:  788  loss:  0.07075902819633484\n",
            "epoch:  789  loss:  0.0707484632730484\n",
            "epoch:  790  loss:  0.07073792070150375\n",
            "epoch:  791  loss:  0.0707273855805397\n",
            "epoch:  792  loss:  0.07071685791015625\n",
            "epoch:  793  loss:  0.0707063302397728\n",
            "epoch:  794  loss:  0.07069581747055054\n",
            "epoch:  795  loss:  0.07068530470132828\n",
            "epoch:  796  loss:  0.07067480683326721\n",
            "epoch:  797  loss:  0.07066431641578674\n",
            "epoch:  798  loss:  0.07065382599830627\n",
            "epoch:  799  loss:  0.0706433579325676\n",
            "epoch:  800  loss:  0.07063288241624832\n",
            "epoch:  801  loss:  0.07062242180109024\n",
            "epoch:  802  loss:  0.07061196118593216\n",
            "epoch:  803  loss:  0.07060150802135468\n",
            "epoch:  804  loss:  0.07059106975793839\n",
            "epoch:  805  loss:  0.0705806314945221\n",
            "epoch:  806  loss:  0.070570208132267\n",
            "epoch:  807  loss:  0.0705597847700119\n",
            "epoch:  808  loss:  0.070549376308918\n",
            "epoch:  809  loss:  0.0705389678478241\n",
            "epoch:  810  loss:  0.07052858173847198\n",
            "epoch:  811  loss:  0.07051818817853928\n",
            "epoch:  812  loss:  0.07050779461860657\n",
            "epoch:  813  loss:  0.07049743086099625\n",
            "epoch:  814  loss:  0.07048705220222473\n",
            "epoch:  815  loss:  0.0704767033457756\n",
            "epoch:  816  loss:  0.07046634703874588\n",
            "epoch:  817  loss:  0.07045599818229675\n",
            "epoch:  818  loss:  0.07044566422700882\n",
            "epoch:  819  loss:  0.07043533027172089\n",
            "epoch:  820  loss:  0.07042500376701355\n",
            "epoch:  821  loss:  0.07041468471288681\n",
            "epoch:  822  loss:  0.07040437310934067\n",
            "epoch:  823  loss:  0.07039406895637512\n",
            "epoch:  824  loss:  0.07038377225399017\n",
            "epoch:  825  loss:  0.07037349045276642\n",
            "epoch:  826  loss:  0.07036320865154266\n",
            "epoch:  827  loss:  0.0703529343008995\n",
            "epoch:  828  loss:  0.07034266740083694\n",
            "epoch:  829  loss:  0.07033240795135498\n",
            "epoch:  830  loss:  0.07032215595245361\n",
            "epoch:  831  loss:  0.07031191140413284\n",
            "epoch:  832  loss:  0.07030167430639267\n",
            "epoch:  833  loss:  0.0702914446592331\n",
            "epoch:  834  loss:  0.07028122246265411\n",
            "epoch:  835  loss:  0.07027099281549454\n",
            "epoch:  836  loss:  0.07026079297065735\n",
            "epoch:  837  loss:  0.07025058567523956\n",
            "epoch:  838  loss:  0.07024039328098297\n",
            "epoch:  839  loss:  0.07023020833730698\n",
            "epoch:  840  loss:  0.07022002339363098\n",
            "epoch:  841  loss:  0.07020984590053558\n",
            "epoch:  842  loss:  0.07019968330860138\n",
            "epoch:  843  loss:  0.07018952816724777\n",
            "epoch:  844  loss:  0.07017937302589417\n",
            "epoch:  845  loss:  0.07016923278570175\n",
            "epoch:  846  loss:  0.07015908509492874\n",
            "epoch:  847  loss:  0.07014896720647812\n",
            "epoch:  848  loss:  0.0701388344168663\n",
            "epoch:  849  loss:  0.07012871652841568\n",
            "epoch:  850  loss:  0.07011859863996506\n",
            "epoch:  851  loss:  0.07010850310325623\n",
            "epoch:  852  loss:  0.0700984075665474\n",
            "epoch:  853  loss:  0.07008831948041916\n",
            "epoch:  854  loss:  0.07007823139429092\n",
            "epoch:  855  loss:  0.07006815820932388\n",
            "epoch:  856  loss:  0.07005809992551804\n",
            "epoch:  857  loss:  0.07004803419113159\n",
            "epoch:  858  loss:  0.07003797590732574\n",
            "epoch:  859  loss:  0.07002793252468109\n",
            "epoch:  860  loss:  0.07001788914203644\n",
            "epoch:  861  loss:  0.07000786066055298\n",
            "epoch:  862  loss:  0.06999783217906952\n",
            "epoch:  863  loss:  0.06998781859874725\n",
            "epoch:  864  loss:  0.06997780501842499\n",
            "epoch:  865  loss:  0.06996779888868332\n",
            "epoch:  866  loss:  0.06995779275894165\n",
            "epoch:  867  loss:  0.06994780898094177\n",
            "epoch:  868  loss:  0.0699378177523613\n",
            "epoch:  869  loss:  0.06992784887552261\n",
            "epoch:  870  loss:  0.06991787999868393\n",
            "epoch:  871  loss:  0.06990791857242584\n",
            "epoch:  872  loss:  0.06989796459674835\n",
            "epoch:  873  loss:  0.06988801062107086\n",
            "epoch:  874  loss:  0.06987807154655457\n",
            "epoch:  875  loss:  0.06986813247203827\n",
            "epoch:  876  loss:  0.06985820829868317\n",
            "epoch:  877  loss:  0.06984828412532806\n",
            "epoch:  878  loss:  0.06983835995197296\n",
            "epoch:  879  loss:  0.06982845813035965\n",
            "epoch:  880  loss:  0.06981856375932693\n",
            "epoch:  881  loss:  0.06980866938829422\n",
            "epoch:  882  loss:  0.0697987824678421\n",
            "epoch:  883  loss:  0.06978890299797058\n",
            "epoch:  884  loss:  0.06977903097867966\n",
            "epoch:  885  loss:  0.06976916640996933\n",
            "epoch:  886  loss:  0.069759301841259\n",
            "epoch:  887  loss:  0.06974945217370987\n",
            "epoch:  888  loss:  0.06973960995674133\n",
            "epoch:  889  loss:  0.0697297677397728\n",
            "epoch:  890  loss:  0.06971994042396545\n",
            "epoch:  891  loss:  0.06971011310815811\n",
            "epoch:  892  loss:  0.06970029324293137\n",
            "epoch:  893  loss:  0.06969048082828522\n",
            "epoch:  894  loss:  0.06968066841363907\n",
            "epoch:  895  loss:  0.06967087835073471\n",
            "epoch:  896  loss:  0.06966108828783035\n",
            "epoch:  897  loss:  0.069651298224926\n",
            "epoch:  898  loss:  0.06964152306318283\n",
            "epoch:  899  loss:  0.06963176280260086\n",
            "epoch:  900  loss:  0.0696219950914383\n",
            "epoch:  901  loss:  0.06961223483085632\n",
            "epoch:  902  loss:  0.06960248202085495\n",
            "epoch:  903  loss:  0.06959273666143417\n",
            "epoch:  904  loss:  0.06958300620317459\n",
            "epoch:  905  loss:  0.06957327574491501\n",
            "epoch:  906  loss:  0.06956355273723602\n",
            "epoch:  907  loss:  0.06955382972955704\n",
            "epoch:  908  loss:  0.06954412162303925\n",
            "epoch:  909  loss:  0.06953441351652145\n",
            "epoch:  910  loss:  0.06952472031116486\n",
            "epoch:  911  loss:  0.06951502710580826\n",
            "epoch:  912  loss:  0.06950534880161285\n",
            "epoch:  913  loss:  0.06949567049741745\n",
            "epoch:  914  loss:  0.06948599964380264\n",
            "epoch:  915  loss:  0.06947635114192963\n",
            "epoch:  916  loss:  0.06946668028831482\n",
            "epoch:  917  loss:  0.0694570392370224\n",
            "epoch:  918  loss:  0.06944739073514938\n",
            "epoch:  919  loss:  0.06943775713443756\n",
            "epoch:  920  loss:  0.06942813098430634\n",
            "epoch:  921  loss:  0.06941850483417511\n",
            "epoch:  922  loss:  0.06940890103578568\n",
            "epoch:  923  loss:  0.06939927488565445\n",
            "epoch:  924  loss:  0.06938967853784561\n",
            "epoch:  925  loss:  0.06938008219003677\n",
            "epoch:  926  loss:  0.06937049329280853\n",
            "epoch:  927  loss:  0.06936091184616089\n",
            "epoch:  928  loss:  0.06935133039951324\n",
            "epoch:  929  loss:  0.0693417638540268\n",
            "epoch:  930  loss:  0.06933218985795975\n",
            "epoch:  931  loss:  0.06932263821363449\n",
            "epoch:  932  loss:  0.06931308656930923\n",
            "epoch:  933  loss:  0.06930354982614517\n",
            "epoch:  934  loss:  0.06929401308298111\n",
            "epoch:  935  loss:  0.06928448379039764\n",
            "epoch:  936  loss:  0.06927496194839478\n",
            "epoch:  937  loss:  0.0692654401063919\n",
            "epoch:  938  loss:  0.06925592571496964\n",
            "epoch:  939  loss:  0.06924642622470856\n",
            "epoch:  940  loss:  0.06923693418502808\n",
            "epoch:  941  loss:  0.0692274421453476\n",
            "epoch:  942  loss:  0.06921795755624771\n",
            "epoch:  943  loss:  0.06920847296714783\n",
            "epoch:  944  loss:  0.06919900327920914\n",
            "epoch:  945  loss:  0.06918954104185104\n",
            "epoch:  946  loss:  0.06918008625507355\n",
            "epoch:  947  loss:  0.06917063146829605\n",
            "epoch:  948  loss:  0.06916118413209915\n",
            "epoch:  949  loss:  0.06915175169706345\n",
            "epoch:  950  loss:  0.06914231926202774\n",
            "epoch:  951  loss:  0.06913288682699203\n",
            "epoch:  952  loss:  0.06912347674369812\n",
            "epoch:  953  loss:  0.06911405920982361\n",
            "epoch:  954  loss:  0.06910465657711029\n",
            "epoch:  955  loss:  0.06909526139497757\n",
            "epoch:  956  loss:  0.06908585876226425\n",
            "epoch:  957  loss:  0.06907647103071213\n",
            "epoch:  958  loss:  0.0690670907497406\n",
            "epoch:  959  loss:  0.06905771791934967\n",
            "epoch:  960  loss:  0.06904835253953934\n",
            "epoch:  961  loss:  0.0690389946103096\n",
            "epoch:  962  loss:  0.06902962923049927\n",
            "epoch:  963  loss:  0.06902028620243073\n",
            "epoch:  964  loss:  0.06901095062494278\n",
            "epoch:  965  loss:  0.06900161504745483\n",
            "epoch:  966  loss:  0.06899227946996689\n",
            "epoch:  967  loss:  0.06898296624422073\n",
            "epoch:  968  loss:  0.06897364556789398\n",
            "epoch:  969  loss:  0.06896433234214783\n",
            "epoch:  970  loss:  0.06895503401756287\n",
            "epoch:  971  loss:  0.0689457356929779\n",
            "epoch:  972  loss:  0.06893644481897354\n",
            "epoch:  973  loss:  0.06892716884613037\n",
            "epoch:  974  loss:  0.0689178854227066\n",
            "epoch:  975  loss:  0.06890860944986343\n",
            "epoch:  976  loss:  0.06889934092760086\n",
            "epoch:  977  loss:  0.06889008730649948\n",
            "epoch:  978  loss:  0.0688808411359787\n",
            "epoch:  979  loss:  0.06887159496545792\n",
            "epoch:  980  loss:  0.06886234879493713\n",
            "epoch:  981  loss:  0.06885311752557755\n",
            "epoch:  982  loss:  0.06884388625621796\n",
            "epoch:  983  loss:  0.06883467733860016\n",
            "epoch:  984  loss:  0.06882546097040176\n",
            "epoch:  985  loss:  0.06881625205278397\n",
            "epoch:  986  loss:  0.06880705058574677\n",
            "epoch:  987  loss:  0.06879784911870956\n",
            "epoch:  988  loss:  0.06878866255283356\n",
            "epoch:  989  loss:  0.06877949088811874\n",
            "epoch:  990  loss:  0.06877031177282333\n",
            "epoch:  991  loss:  0.06876114010810852\n",
            "epoch:  992  loss:  0.0687519758939743\n",
            "epoch:  993  loss:  0.06874281167984009\n",
            "epoch:  994  loss:  0.06873366236686707\n",
            "epoch:  995  loss:  0.06872452795505524\n",
            "epoch:  996  loss:  0.06871538609266281\n",
            "epoch:  997  loss:  0.06870625168085098\n",
            "epoch:  998  loss:  0.06869711726903915\n",
            "epoch:  999  loss:  0.06868801265954971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykLCzj5YcEqJ",
        "outputId": "25641caf-1cc7-468b-e311-54596982fd32"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.06867889314889908\n",
            "epoch:  1  loss:  0.06866978853940964\n",
            "epoch:  2  loss:  0.0686606913805008\n",
            "epoch:  3  loss:  0.06865158677101135\n",
            "epoch:  4  loss:  0.0686425045132637\n",
            "epoch:  5  loss:  0.06863342225551605\n",
            "epoch:  6  loss:  0.068624347448349\n",
            "epoch:  7  loss:  0.06861528009176254\n",
            "epoch:  8  loss:  0.06860620528459549\n",
            "epoch:  9  loss:  0.06859715282917023\n",
            "epoch:  10  loss:  0.06858810782432556\n",
            "epoch:  11  loss:  0.0685790553689003\n",
            "epoch:  12  loss:  0.06857002526521683\n",
            "epoch:  13  loss:  0.06856098771095276\n",
            "epoch:  14  loss:  0.06855195760726929\n",
            "epoch:  15  loss:  0.06854294240474701\n",
            "epoch:  16  loss:  0.06853392720222473\n",
            "epoch:  17  loss:  0.06852491945028305\n",
            "epoch:  18  loss:  0.06851591169834137\n",
            "epoch:  19  loss:  0.06850691884756088\n",
            "epoch:  20  loss:  0.06849793344736099\n",
            "epoch:  21  loss:  0.0684889480471611\n",
            "epoch:  22  loss:  0.06847997009754181\n",
            "epoch:  23  loss:  0.06847100704908371\n",
            "epoch:  24  loss:  0.06846203655004501\n",
            "epoch:  25  loss:  0.06845308095216751\n",
            "epoch:  26  loss:  0.06844412535429001\n",
            "epoch:  27  loss:  0.0684351772069931\n",
            "epoch:  28  loss:  0.0684262365102768\n",
            "epoch:  29  loss:  0.06841730326414108\n",
            "epoch:  30  loss:  0.06840837001800537\n",
            "epoch:  31  loss:  0.06839945167303085\n",
            "epoch:  32  loss:  0.06839053332805634\n",
            "epoch:  33  loss:  0.06838162243366241\n",
            "epoch:  34  loss:  0.06837272644042969\n",
            "epoch:  35  loss:  0.06836382299661636\n",
            "epoch:  36  loss:  0.06835493445396423\n",
            "epoch:  37  loss:  0.0683460459113121\n",
            "epoch:  38  loss:  0.06833716481924057\n",
            "epoch:  39  loss:  0.06832828372716904\n",
            "epoch:  40  loss:  0.0683194249868393\n",
            "epoch:  41  loss:  0.06831056624650955\n",
            "epoch:  42  loss:  0.0683017149567604\n",
            "epoch:  43  loss:  0.06829286366701126\n",
            "epoch:  44  loss:  0.06828401982784271\n",
            "epoch:  45  loss:  0.06827518343925476\n",
            "epoch:  46  loss:  0.06826634705066681\n",
            "epoch:  47  loss:  0.06825752556324005\n",
            "epoch:  48  loss:  0.0682487040758133\n",
            "epoch:  49  loss:  0.06823988258838654\n",
            "epoch:  50  loss:  0.06823107600212097\n",
            "epoch:  51  loss:  0.068222276866436\n",
            "epoch:  52  loss:  0.06821348518133163\n",
            "epoch:  53  loss:  0.06820468604564667\n",
            "epoch:  54  loss:  0.06819590926170349\n",
            "epoch:  55  loss:  0.06818713247776031\n",
            "epoch:  56  loss:  0.06817836314439774\n",
            "epoch:  57  loss:  0.06816960126161575\n",
            "epoch:  58  loss:  0.06816082447767258\n",
            "epoch:  59  loss:  0.06815207749605179\n",
            "epoch:  60  loss:  0.0681433230638504\n",
            "epoch:  61  loss:  0.06813459098339081\n",
            "epoch:  62  loss:  0.06812585145235062\n",
            "epoch:  63  loss:  0.06811712682247162\n",
            "epoch:  64  loss:  0.06810839474201202\n",
            "epoch:  65  loss:  0.06809967756271362\n",
            "epoch:  66  loss:  0.06809097528457642\n",
            "epoch:  67  loss:  0.06808226555585861\n",
            "epoch:  68  loss:  0.0680735632777214\n",
            "epoch:  69  loss:  0.06806487590074539\n",
            "epoch:  70  loss:  0.06805618107318878\n",
            "epoch:  71  loss:  0.06804749369621277\n",
            "epoch:  72  loss:  0.06803882122039795\n",
            "epoch:  73  loss:  0.06803014874458313\n",
            "epoch:  74  loss:  0.06802148371934891\n",
            "epoch:  75  loss:  0.06801282614469528\n",
            "epoch:  76  loss:  0.06800416857004166\n",
            "epoch:  77  loss:  0.06799551844596863\n",
            "epoch:  78  loss:  0.0679868757724762\n",
            "epoch:  79  loss:  0.06797824054956436\n",
            "epoch:  80  loss:  0.06796962022781372\n",
            "epoch:  81  loss:  0.06796099245548248\n",
            "epoch:  82  loss:  0.06795237213373184\n",
            "epoch:  83  loss:  0.0679437592625618\n",
            "epoch:  84  loss:  0.06793515384197235\n",
            "epoch:  85  loss:  0.0679265484213829\n",
            "epoch:  86  loss:  0.06791795045137405\n",
            "epoch:  87  loss:  0.0679093599319458\n",
            "epoch:  88  loss:  0.06790077686309814\n",
            "epoch:  89  loss:  0.06789220124483109\n",
            "epoch:  90  loss:  0.06788363307714462\n",
            "epoch:  91  loss:  0.06787507981061935\n",
            "epoch:  92  loss:  0.06786651164293289\n",
            "epoch:  93  loss:  0.06785795837640762\n",
            "epoch:  94  loss:  0.06784941256046295\n",
            "epoch:  95  loss:  0.06784086674451828\n",
            "epoch:  96  loss:  0.0678323283791542\n",
            "epoch:  97  loss:  0.06782379746437073\n",
            "epoch:  98  loss:  0.06781526654958725\n",
            "epoch:  99  loss:  0.06780675798654556\n",
            "epoch:  100  loss:  0.06779822707176208\n",
            "epoch:  101  loss:  0.06778973340988159\n",
            "epoch:  102  loss:  0.0677812248468399\n",
            "epoch:  103  loss:  0.06777273118495941\n",
            "epoch:  104  loss:  0.06776424497365952\n",
            "epoch:  105  loss:  0.06775575131177902\n",
            "epoch:  106  loss:  0.06774726510047913\n",
            "epoch:  107  loss:  0.06773879379034042\n",
            "epoch:  108  loss:  0.06773033738136292\n",
            "epoch:  109  loss:  0.06772187352180481\n",
            "epoch:  110  loss:  0.0677134096622467\n",
            "epoch:  111  loss:  0.0677049532532692\n",
            "epoch:  112  loss:  0.06769651919603348\n",
            "epoch:  113  loss:  0.06768807023763657\n",
            "epoch:  114  loss:  0.06767964363098145\n",
            "epoch:  115  loss:  0.06767120957374573\n",
            "epoch:  116  loss:  0.0676627904176712\n",
            "epoch:  117  loss:  0.06765437126159668\n",
            "epoch:  118  loss:  0.06764596700668335\n",
            "epoch:  119  loss:  0.06763755530118942\n",
            "epoch:  120  loss:  0.06762915849685669\n",
            "epoch:  121  loss:  0.06762076914310455\n",
            "epoch:  122  loss:  0.06761237233877182\n",
            "epoch:  123  loss:  0.06760399788618088\n",
            "epoch:  124  loss:  0.06759561598300934\n",
            "epoch:  125  loss:  0.0675872415304184\n",
            "epoch:  126  loss:  0.06757888197898865\n",
            "epoch:  127  loss:  0.0675705149769783\n",
            "epoch:  128  loss:  0.06756217032670975\n",
            "epoch:  129  loss:  0.06755381077528\n",
            "epoch:  130  loss:  0.06754547357559204\n",
            "epoch:  131  loss:  0.06753712892532349\n",
            "epoch:  132  loss:  0.06752880662679672\n",
            "epoch:  133  loss:  0.06752047687768936\n",
            "epoch:  134  loss:  0.0675121545791626\n",
            "epoch:  135  loss:  0.06750383973121643\n",
            "epoch:  136  loss:  0.06749553233385086\n",
            "epoch:  137  loss:  0.06748722493648529\n",
            "epoch:  138  loss:  0.06747892498970032\n",
            "epoch:  139  loss:  0.06747063994407654\n",
            "epoch:  140  loss:  0.06746233999729156\n",
            "epoch:  141  loss:  0.06745406240224838\n",
            "epoch:  142  loss:  0.0674457848072052\n",
            "epoch:  143  loss:  0.06743751466274261\n",
            "epoch:  144  loss:  0.06742924451828003\n",
            "epoch:  145  loss:  0.06742098927497864\n",
            "epoch:  146  loss:  0.06741273403167725\n",
            "epoch:  147  loss:  0.06740447878837585\n",
            "epoch:  148  loss:  0.06739623844623566\n",
            "epoch:  149  loss:  0.06738800555467606\n",
            "epoch:  150  loss:  0.06737976521253586\n",
            "epoch:  151  loss:  0.06737155467271805\n",
            "epoch:  152  loss:  0.06736332923173904\n",
            "epoch:  153  loss:  0.06735511869192123\n",
            "epoch:  154  loss:  0.06734690070152283\n",
            "epoch:  155  loss:  0.06733869016170502\n",
            "epoch:  156  loss:  0.0673304945230484\n",
            "epoch:  157  loss:  0.06732230633497238\n",
            "epoch:  158  loss:  0.06731411814689636\n",
            "epoch:  159  loss:  0.06730592250823975\n",
            "epoch:  160  loss:  0.06729775667190552\n",
            "epoch:  161  loss:  0.06728959083557129\n",
            "epoch:  162  loss:  0.06728142499923706\n",
            "epoch:  163  loss:  0.06727326661348343\n",
            "epoch:  164  loss:  0.0672651082277298\n",
            "epoch:  165  loss:  0.06725694984197617\n",
            "epoch:  166  loss:  0.06724880635738373\n",
            "epoch:  167  loss:  0.06724067032337189\n",
            "epoch:  168  loss:  0.06723253428936005\n",
            "epoch:  169  loss:  0.0672244131565094\n",
            "epoch:  170  loss:  0.06721628457307816\n",
            "epoch:  171  loss:  0.0672081708908081\n",
            "epoch:  172  loss:  0.06720005720853806\n",
            "epoch:  173  loss:  0.0671919584274292\n",
            "epoch:  174  loss:  0.06718385219573975\n",
            "epoch:  175  loss:  0.06717575341463089\n",
            "epoch:  176  loss:  0.06716766953468323\n",
            "epoch:  177  loss:  0.06715958565473557\n",
            "epoch:  178  loss:  0.0671515017747879\n",
            "epoch:  179  loss:  0.06714343279600143\n",
            "epoch:  180  loss:  0.06713536381721497\n",
            "epoch:  181  loss:  0.0671272873878479\n",
            "epoch:  182  loss:  0.06711924076080322\n",
            "epoch:  183  loss:  0.06711119413375854\n",
            "epoch:  184  loss:  0.06710314005613327\n",
            "epoch:  185  loss:  0.06709509342908859\n",
            "epoch:  186  loss:  0.0670870691537857\n",
            "epoch:  187  loss:  0.06707902997732162\n",
            "epoch:  188  loss:  0.06707101315259933\n",
            "epoch:  189  loss:  0.06706298887729645\n",
            "epoch:  190  loss:  0.06705497205257416\n",
            "epoch:  191  loss:  0.06704697012901306\n",
            "epoch:  192  loss:  0.06703895330429077\n",
            "epoch:  193  loss:  0.06703095883131027\n",
            "epoch:  194  loss:  0.06702296435832977\n",
            "epoch:  195  loss:  0.06701497733592987\n",
            "epoch:  196  loss:  0.06700699776411057\n",
            "epoch:  197  loss:  0.06699901819229126\n",
            "epoch:  198  loss:  0.06699104607105255\n",
            "epoch:  199  loss:  0.06698308140039444\n",
            "epoch:  200  loss:  0.06697510927915573\n",
            "epoch:  201  loss:  0.06696715950965881\n",
            "epoch:  202  loss:  0.0669592022895813\n",
            "epoch:  203  loss:  0.06695125252008438\n",
            "epoch:  204  loss:  0.06694331765174866\n",
            "epoch:  205  loss:  0.06693539023399353\n",
            "epoch:  206  loss:  0.0669274628162384\n",
            "epoch:  207  loss:  0.06691953539848328\n",
            "epoch:  208  loss:  0.06691160798072815\n",
            "epoch:  209  loss:  0.06690370291471481\n",
            "epoch:  210  loss:  0.06689579784870148\n",
            "epoch:  211  loss:  0.06688789278268814\n",
            "epoch:  212  loss:  0.0668799877166748\n",
            "epoch:  213  loss:  0.06687209755182266\n",
            "epoch:  214  loss:  0.06686420738697052\n",
            "epoch:  215  loss:  0.06685633212327957\n",
            "epoch:  216  loss:  0.06684844195842743\n",
            "epoch:  217  loss:  0.06684057414531708\n",
            "epoch:  218  loss:  0.06683272123336792\n",
            "epoch:  219  loss:  0.06682485342025757\n",
            "epoch:  220  loss:  0.06681699305772781\n",
            "epoch:  221  loss:  0.06680913269519806\n",
            "epoch:  222  loss:  0.0668012946844101\n",
            "epoch:  223  loss:  0.06679345667362213\n",
            "epoch:  224  loss:  0.06678561121225357\n",
            "epoch:  225  loss:  0.0667777955532074\n",
            "epoch:  226  loss:  0.06676996499300003\n",
            "epoch:  227  loss:  0.06676214933395386\n",
            "epoch:  228  loss:  0.06675432622432709\n",
            "epoch:  229  loss:  0.06674651801586151\n",
            "epoch:  230  loss:  0.06673871725797653\n",
            "epoch:  231  loss:  0.06673091650009155\n",
            "epoch:  232  loss:  0.06672313064336777\n",
            "epoch:  233  loss:  0.06671532988548279\n",
            "epoch:  234  loss:  0.0667075514793396\n",
            "epoch:  235  loss:  0.06669976562261581\n",
            "epoch:  236  loss:  0.06669200211763382\n",
            "epoch:  237  loss:  0.06668422371149063\n",
            "epoch:  238  loss:  0.06667646765708923\n",
            "epoch:  239  loss:  0.06666871160268784\n",
            "epoch:  240  loss:  0.06666095554828644\n",
            "epoch:  241  loss:  0.06665320694446564\n",
            "epoch:  242  loss:  0.06664545834064484\n",
            "epoch:  243  loss:  0.06663772463798523\n",
            "epoch:  244  loss:  0.06662999838590622\n",
            "epoch:  245  loss:  0.06662226468324661\n",
            "epoch:  246  loss:  0.0666145384311676\n",
            "epoch:  247  loss:  0.06660682708024979\n",
            "epoch:  248  loss:  0.06659911572933197\n",
            "epoch:  249  loss:  0.06659141182899475\n",
            "epoch:  250  loss:  0.06658370792865753\n",
            "epoch:  251  loss:  0.0665760189294815\n",
            "epoch:  252  loss:  0.06656831502914429\n",
            "epoch:  253  loss:  0.06656063348054886\n",
            "epoch:  254  loss:  0.06655295193195343\n",
            "epoch:  255  loss:  0.066545270383358\n",
            "epoch:  256  loss:  0.06653759628534317\n",
            "epoch:  257  loss:  0.06652993708848953\n",
            "epoch:  258  loss:  0.0665222778916359\n",
            "epoch:  259  loss:  0.06651461869478226\n",
            "epoch:  260  loss:  0.06650696694850922\n",
            "epoch:  261  loss:  0.06649933010339737\n",
            "epoch:  262  loss:  0.06649168580770493\n",
            "epoch:  263  loss:  0.06648404896259308\n",
            "epoch:  264  loss:  0.06647641956806183\n",
            "epoch:  265  loss:  0.06646879017353058\n",
            "epoch:  266  loss:  0.06646116822957993\n",
            "epoch:  267  loss:  0.06645355373620987\n",
            "epoch:  268  loss:  0.06644594669342041\n",
            "epoch:  269  loss:  0.06643833965063095\n",
            "epoch:  270  loss:  0.06643073260784149\n",
            "epoch:  271  loss:  0.06642313301563263\n",
            "epoch:  272  loss:  0.06641554832458496\n",
            "epoch:  273  loss:  0.06640796363353729\n",
            "epoch:  274  loss:  0.06640038639307022\n",
            "epoch:  275  loss:  0.06639280915260315\n",
            "epoch:  276  loss:  0.06638523936271667\n",
            "epoch:  277  loss:  0.0663776695728302\n",
            "epoch:  278  loss:  0.06637009978294373\n",
            "epoch:  279  loss:  0.06636255234479904\n",
            "epoch:  280  loss:  0.06635499745607376\n",
            "epoch:  281  loss:  0.06634745746850967\n",
            "epoch:  282  loss:  0.06633991748094559\n",
            "epoch:  283  loss:  0.0663323849439621\n",
            "epoch:  284  loss:  0.06632484495639801\n",
            "epoch:  285  loss:  0.06631731986999512\n",
            "epoch:  286  loss:  0.06630980223417282\n",
            "epoch:  287  loss:  0.06630228459835052\n",
            "epoch:  288  loss:  0.06629476696252823\n",
            "epoch:  289  loss:  0.06628726422786713\n",
            "epoch:  290  loss:  0.06627976894378662\n",
            "epoch:  291  loss:  0.06627226620912552\n",
            "epoch:  292  loss:  0.06626477092504501\n",
            "epoch:  293  loss:  0.0662572830915451\n",
            "epoch:  294  loss:  0.0662498027086258\n",
            "epoch:  295  loss:  0.06624232232570648\n",
            "epoch:  296  loss:  0.06623484939336777\n",
            "epoch:  297  loss:  0.06622738391160965\n",
            "epoch:  298  loss:  0.06621992588043213\n",
            "epoch:  299  loss:  0.06621246039867401\n",
            "epoch:  300  loss:  0.06620501726865768\n",
            "epoch:  301  loss:  0.06619757413864136\n",
            "epoch:  302  loss:  0.06619013100862503\n",
            "epoch:  303  loss:  0.0661826878786087\n",
            "epoch:  304  loss:  0.06617525964975357\n",
            "epoch:  305  loss:  0.06616783142089844\n",
            "epoch:  306  loss:  0.0661604106426239\n",
            "epoch:  307  loss:  0.06615298241376877\n",
            "epoch:  308  loss:  0.06614556908607483\n",
            "epoch:  309  loss:  0.06613817065954208\n",
            "epoch:  310  loss:  0.06613075733184814\n",
            "epoch:  311  loss:  0.066123366355896\n",
            "epoch:  312  loss:  0.06611596047878265\n",
            "epoch:  313  loss:  0.0661085769534111\n",
            "epoch:  314  loss:  0.06610119342803955\n",
            "epoch:  315  loss:  0.0660938024520874\n",
            "epoch:  316  loss:  0.06608644127845764\n",
            "epoch:  317  loss:  0.06607906520366669\n",
            "epoch:  318  loss:  0.06607171148061752\n",
            "epoch:  319  loss:  0.06606434285640717\n",
            "epoch:  320  loss:  0.0660569816827774\n",
            "epoch:  321  loss:  0.06604963541030884\n",
            "epoch:  322  loss:  0.06604228913784027\n",
            "epoch:  323  loss:  0.0660349503159523\n",
            "epoch:  324  loss:  0.06602761149406433\n",
            "epoch:  325  loss:  0.06602028757333755\n",
            "epoch:  326  loss:  0.06601294875144958\n",
            "epoch:  327  loss:  0.0660056322813034\n",
            "epoch:  328  loss:  0.06599831581115723\n",
            "epoch:  329  loss:  0.06599100679159164\n",
            "epoch:  330  loss:  0.06598369777202606\n",
            "epoch:  331  loss:  0.06597638875246048\n",
            "epoch:  332  loss:  0.06596909463405609\n",
            "epoch:  333  loss:  0.0659618079662323\n",
            "epoch:  334  loss:  0.06595451384782791\n",
            "epoch:  335  loss:  0.06594722718000412\n",
            "epoch:  336  loss:  0.06593994796276093\n",
            "epoch:  337  loss:  0.06593267619609833\n",
            "epoch:  338  loss:  0.06592540442943573\n",
            "epoch:  339  loss:  0.06591814756393433\n",
            "epoch:  340  loss:  0.06591089069843292\n",
            "epoch:  341  loss:  0.06590363383293152\n",
            "epoch:  342  loss:  0.06589637696743011\n",
            "epoch:  343  loss:  0.06588912755250931\n",
            "epoch:  344  loss:  0.06588190048933029\n",
            "epoch:  345  loss:  0.06587465852499008\n",
            "epoch:  346  loss:  0.06586743146181107\n",
            "epoch:  347  loss:  0.06586019694805145\n",
            "epoch:  348  loss:  0.06585297733545303\n",
            "epoch:  349  loss:  0.06584575772285461\n",
            "epoch:  350  loss:  0.06583854556083679\n",
            "epoch:  351  loss:  0.06583133339881897\n",
            "epoch:  352  loss:  0.06582413613796234\n",
            "epoch:  353  loss:  0.06581693142652512\n",
            "epoch:  354  loss:  0.06580974161624908\n",
            "epoch:  355  loss:  0.06580255180597305\n",
            "epoch:  356  loss:  0.06579536944627762\n",
            "epoch:  357  loss:  0.06578818708658218\n",
            "epoch:  358  loss:  0.06578101217746735\n",
            "epoch:  359  loss:  0.0657738447189331\n",
            "epoch:  360  loss:  0.06576667726039886\n",
            "epoch:  361  loss:  0.06575951725244522\n",
            "epoch:  362  loss:  0.06575236469507217\n",
            "epoch:  363  loss:  0.06574521213769913\n",
            "epoch:  364  loss:  0.06573806703090668\n",
            "epoch:  365  loss:  0.06573092192411423\n",
            "epoch:  366  loss:  0.06572377681732178\n",
            "epoch:  367  loss:  0.06571665406227112\n",
            "epoch:  368  loss:  0.06570951640605927\n",
            "epoch:  369  loss:  0.0657023936510086\n",
            "epoch:  370  loss:  0.06569527089595795\n",
            "epoch:  371  loss:  0.06568816304206848\n",
            "epoch:  372  loss:  0.06568106263875961\n",
            "epoch:  373  loss:  0.06567393988370895\n",
            "epoch:  374  loss:  0.06566684693098068\n",
            "epoch:  375  loss:  0.06565974652767181\n",
            "epoch:  376  loss:  0.06565265357494354\n",
            "epoch:  377  loss:  0.06564556807279587\n",
            "epoch:  378  loss:  0.06563849002122879\n",
            "epoch:  379  loss:  0.06563141196966171\n",
            "epoch:  380  loss:  0.06562433391809464\n",
            "epoch:  381  loss:  0.06561726331710815\n",
            "epoch:  382  loss:  0.06561020761728287\n",
            "epoch:  383  loss:  0.06560314446687698\n",
            "epoch:  384  loss:  0.0655960813164711\n",
            "epoch:  385  loss:  0.06558903306722641\n",
            "epoch:  386  loss:  0.06558199226856232\n",
            "epoch:  387  loss:  0.06557495146989822\n",
            "epoch:  388  loss:  0.06556791812181473\n",
            "epoch:  389  loss:  0.06556088477373123\n",
            "epoch:  390  loss:  0.06555385142564774\n",
            "epoch:  391  loss:  0.06554682552814484\n",
            "epoch:  392  loss:  0.06553980708122253\n",
            "epoch:  393  loss:  0.06553279608488083\n",
            "epoch:  394  loss:  0.06552578508853912\n",
            "epoch:  395  loss:  0.06551878154277802\n",
            "epoch:  396  loss:  0.0655117854475975\n",
            "epoch:  397  loss:  0.0655047819018364\n",
            "epoch:  398  loss:  0.06549779325723648\n",
            "epoch:  399  loss:  0.06549080461263657\n",
            "epoch:  400  loss:  0.06548382341861725\n",
            "epoch:  401  loss:  0.06547684967517853\n",
            "epoch:  402  loss:  0.06546986848115921\n",
            "epoch:  403  loss:  0.06546290218830109\n",
            "epoch:  404  loss:  0.06545593589544296\n",
            "epoch:  405  loss:  0.06544899195432663\n",
            "epoch:  406  loss:  0.0654420331120491\n",
            "epoch:  407  loss:  0.06543507426977158\n",
            "epoch:  408  loss:  0.06542813032865524\n",
            "epoch:  409  loss:  0.06542118638753891\n",
            "epoch:  410  loss:  0.06541424989700317\n",
            "epoch:  411  loss:  0.06540732085704803\n",
            "epoch:  412  loss:  0.0654003918170929\n",
            "epoch:  413  loss:  0.06539346277713776\n",
            "epoch:  414  loss:  0.06538654863834381\n",
            "epoch:  415  loss:  0.06537963449954987\n",
            "epoch:  416  loss:  0.06537272781133652\n",
            "epoch:  417  loss:  0.06536581367254257\n",
            "epoch:  418  loss:  0.06535890698432922\n",
            "epoch:  419  loss:  0.06535201519727707\n",
            "epoch:  420  loss:  0.06534513086080551\n",
            "epoch:  421  loss:  0.06533823162317276\n",
            "epoch:  422  loss:  0.0653313472867012\n",
            "epoch:  423  loss:  0.06532447785139084\n",
            "epoch:  424  loss:  0.06531759351491928\n",
            "epoch:  425  loss:  0.06531072407960892\n",
            "epoch:  426  loss:  0.06530385464429855\n",
            "epoch:  427  loss:  0.06529700011014938\n",
            "epoch:  428  loss:  0.06529013812541962\n",
            "epoch:  429  loss:  0.06528327614068985\n",
            "epoch:  430  loss:  0.06527644395828247\n",
            "epoch:  431  loss:  0.0652695894241333\n",
            "epoch:  432  loss:  0.06526275724172592\n",
            "epoch:  433  loss:  0.06525591760873795\n",
            "epoch:  434  loss:  0.06524908542633057\n",
            "epoch:  435  loss:  0.06524226069450378\n",
            "epoch:  436  loss:  0.0652354434132576\n",
            "epoch:  437  loss:  0.06522862613201141\n",
            "epoch:  438  loss:  0.06522180140018463\n",
            "epoch:  439  loss:  0.06521499902009964\n",
            "epoch:  440  loss:  0.06520818918943405\n",
            "epoch:  441  loss:  0.06520140171051025\n",
            "epoch:  442  loss:  0.06519459187984467\n",
            "epoch:  443  loss:  0.06518780440092087\n",
            "epoch:  444  loss:  0.06518101692199707\n",
            "epoch:  445  loss:  0.06517423689365387\n",
            "epoch:  446  loss:  0.06516745686531067\n",
            "epoch:  447  loss:  0.06516068428754807\n",
            "epoch:  448  loss:  0.06515391170978546\n",
            "epoch:  449  loss:  0.06514714658260345\n",
            "epoch:  450  loss:  0.06514038145542145\n",
            "epoch:  451  loss:  0.06513363122940063\n",
            "epoch:  452  loss:  0.06512686610221863\n",
            "epoch:  453  loss:  0.06512013077735901\n",
            "epoch:  454  loss:  0.06511338800191879\n",
            "epoch:  455  loss:  0.06510664522647858\n",
            "epoch:  456  loss:  0.06509991735219955\n",
            "epoch:  457  loss:  0.06509318202733994\n",
            "epoch:  458  loss:  0.06508644670248032\n",
            "epoch:  459  loss:  0.06507972627878189\n",
            "epoch:  460  loss:  0.06507301330566406\n",
            "epoch:  461  loss:  0.06506630033254623\n",
            "epoch:  462  loss:  0.0650595873594284\n",
            "epoch:  463  loss:  0.06505288928747177\n",
            "epoch:  464  loss:  0.06504619121551514\n",
            "epoch:  465  loss:  0.0650394856929779\n",
            "epoch:  466  loss:  0.06503279507160187\n",
            "epoch:  467  loss:  0.06502610445022583\n",
            "epoch:  468  loss:  0.06501942127943039\n",
            "epoch:  469  loss:  0.06501274555921555\n",
            "epoch:  470  loss:  0.0650060623884201\n",
            "epoch:  471  loss:  0.06499939411878586\n",
            "epoch:  472  loss:  0.0649927407503128\n",
            "epoch:  473  loss:  0.06498606503009796\n",
            "epoch:  474  loss:  0.0649794191122055\n",
            "epoch:  475  loss:  0.06497276574373245\n",
            "epoch:  476  loss:  0.0649661123752594\n",
            "epoch:  477  loss:  0.06495946645736694\n",
            "epoch:  478  loss:  0.06495282799005508\n",
            "epoch:  479  loss:  0.06494618952274323\n",
            "epoch:  480  loss:  0.06493955850601196\n",
            "epoch:  481  loss:  0.0649329274892807\n",
            "epoch:  482  loss:  0.06492631137371063\n",
            "epoch:  483  loss:  0.06491969525814056\n",
            "epoch:  484  loss:  0.0649130716919899\n",
            "epoch:  485  loss:  0.06490646302700043\n",
            "epoch:  486  loss:  0.06489985436201096\n",
            "epoch:  487  loss:  0.06489325314760208\n",
            "epoch:  488  loss:  0.0648866593837738\n",
            "epoch:  489  loss:  0.06488005816936493\n",
            "epoch:  490  loss:  0.06487347185611725\n",
            "epoch:  491  loss:  0.06486688554286957\n",
            "epoch:  492  loss:  0.06486030668020248\n",
            "epoch:  493  loss:  0.0648537278175354\n",
            "epoch:  494  loss:  0.06484714150428772\n",
            "epoch:  495  loss:  0.06484058499336243\n",
            "epoch:  496  loss:  0.06483401358127594\n",
            "epoch:  497  loss:  0.06482746452093124\n",
            "epoch:  498  loss:  0.06482090801000595\n",
            "epoch:  499  loss:  0.06481435149908066\n",
            "epoch:  500  loss:  0.06480780988931656\n",
            "epoch:  501  loss:  0.06480126082897186\n",
            "epoch:  502  loss:  0.06479472666978836\n",
            "epoch:  503  loss:  0.06478818506002426\n",
            "epoch:  504  loss:  0.06478165090084076\n",
            "epoch:  505  loss:  0.06477513164281845\n",
            "epoch:  506  loss:  0.06476859748363495\n",
            "epoch:  507  loss:  0.06476208567619324\n",
            "epoch:  508  loss:  0.06475556641817093\n",
            "epoch:  509  loss:  0.06474906206130981\n",
            "epoch:  510  loss:  0.0647425502538681\n",
            "epoch:  511  loss:  0.06473605334758759\n",
            "epoch:  512  loss:  0.06472955644130707\n",
            "epoch:  513  loss:  0.06472305208444595\n",
            "epoch:  514  loss:  0.06471657007932663\n",
            "epoch:  515  loss:  0.06471008062362671\n",
            "epoch:  516  loss:  0.06470360606908798\n",
            "epoch:  517  loss:  0.06469711661338806\n",
            "epoch:  518  loss:  0.06469064205884933\n",
            "epoch:  519  loss:  0.0646841824054718\n",
            "epoch:  520  loss:  0.06467770785093307\n",
            "epoch:  521  loss:  0.06467124819755554\n",
            "epoch:  522  loss:  0.0646647959947586\n",
            "epoch:  523  loss:  0.06465834379196167\n",
            "epoch:  524  loss:  0.06465189158916473\n",
            "epoch:  525  loss:  0.0646454468369484\n",
            "epoch:  526  loss:  0.06463900208473206\n",
            "epoch:  527  loss:  0.06463256478309631\n",
            "epoch:  528  loss:  0.06462613493204117\n",
            "epoch:  529  loss:  0.06461970508098602\n",
            "epoch:  530  loss:  0.06461327522993088\n",
            "epoch:  531  loss:  0.06460686773061752\n",
            "epoch:  532  loss:  0.06460043042898178\n",
            "epoch:  533  loss:  0.06459403038024902\n",
            "epoch:  534  loss:  0.06458762288093567\n",
            "epoch:  535  loss:  0.06458121538162231\n",
            "epoch:  536  loss:  0.06457481533288956\n",
            "epoch:  537  loss:  0.0645684227347374\n",
            "epoch:  538  loss:  0.06456203013658524\n",
            "epoch:  539  loss:  0.06455563008785248\n",
            "epoch:  540  loss:  0.06454925239086151\n",
            "epoch:  541  loss:  0.06454286724328995\n",
            "epoch:  542  loss:  0.06453649699687958\n",
            "epoch:  543  loss:  0.06453012675046921\n",
            "epoch:  544  loss:  0.06452374905347824\n",
            "epoch:  545  loss:  0.06451739370822906\n",
            "epoch:  546  loss:  0.06451103091239929\n",
            "epoch:  547  loss:  0.06450467556715012\n",
            "epoch:  548  loss:  0.06449832022190094\n",
            "epoch:  549  loss:  0.06449197977781296\n",
            "epoch:  550  loss:  0.06448563188314438\n",
            "epoch:  551  loss:  0.0644792914390564\n",
            "epoch:  552  loss:  0.06447295099496841\n",
            "epoch:  553  loss:  0.06446662545204163\n",
            "epoch:  554  loss:  0.06446029990911484\n",
            "epoch:  555  loss:  0.06445397436618805\n",
            "epoch:  556  loss:  0.06444764882326126\n",
            "epoch:  557  loss:  0.06444133073091507\n",
            "epoch:  558  loss:  0.06443502008914948\n",
            "epoch:  559  loss:  0.06442870944738388\n",
            "epoch:  560  loss:  0.06442240625619888\n",
            "epoch:  561  loss:  0.06441611051559448\n",
            "epoch:  562  loss:  0.06440981477499008\n",
            "epoch:  563  loss:  0.06440351903438568\n",
            "epoch:  564  loss:  0.06439722329378128\n",
            "epoch:  565  loss:  0.06439094245433807\n",
            "epoch:  566  loss:  0.06438466161489487\n",
            "epoch:  567  loss:  0.06437838822603226\n",
            "epoch:  568  loss:  0.06437212228775024\n",
            "epoch:  569  loss:  0.06436584144830704\n",
            "epoch:  570  loss:  0.06435959041118622\n",
            "epoch:  571  loss:  0.0643533244729042\n",
            "epoch:  572  loss:  0.06434706598520279\n",
            "epoch:  573  loss:  0.06434081494808197\n",
            "epoch:  574  loss:  0.06433456391096115\n",
            "epoch:  575  loss:  0.06432832032442093\n",
            "epoch:  576  loss:  0.0643220767378807\n",
            "epoch:  577  loss:  0.06431584060192108\n",
            "epoch:  578  loss:  0.06430961936712265\n",
            "epoch:  579  loss:  0.06430337578058243\n",
            "epoch:  580  loss:  0.0642971470952034\n",
            "epoch:  581  loss:  0.06429093331098557\n",
            "epoch:  582  loss:  0.06428471207618713\n",
            "epoch:  583  loss:  0.0642784982919693\n",
            "epoch:  584  loss:  0.06427229195833206\n",
            "epoch:  585  loss:  0.06426608562469482\n",
            "epoch:  586  loss:  0.06425988674163818\n",
            "epoch:  587  loss:  0.06425368785858154\n",
            "epoch:  588  loss:  0.0642474815249443\n",
            "epoch:  589  loss:  0.06424129754304886\n",
            "epoch:  590  loss:  0.06423511356115341\n",
            "epoch:  591  loss:  0.06422892957925797\n",
            "epoch:  592  loss:  0.06422274559736252\n",
            "epoch:  593  loss:  0.06421657651662827\n",
            "epoch:  594  loss:  0.06421039998531342\n",
            "epoch:  595  loss:  0.06420423090457916\n",
            "epoch:  596  loss:  0.0641980767250061\n",
            "epoch:  597  loss:  0.06419190764427185\n",
            "epoch:  598  loss:  0.06418576091527939\n",
            "epoch:  599  loss:  0.06417960673570633\n",
            "epoch:  600  loss:  0.06417345255613327\n",
            "epoch:  601  loss:  0.0641673132777214\n",
            "epoch:  602  loss:  0.06416117399930954\n",
            "epoch:  603  loss:  0.06415503472089767\n",
            "epoch:  604  loss:  0.0641489028930664\n",
            "epoch:  605  loss:  0.06414277106523514\n",
            "epoch:  606  loss:  0.06413663923740387\n",
            "epoch:  607  loss:  0.06413052976131439\n",
            "epoch:  608  loss:  0.06412440538406372\n",
            "epoch:  609  loss:  0.06411829590797424\n",
            "epoch:  610  loss:  0.06411217898130417\n",
            "epoch:  611  loss:  0.06410607695579529\n",
            "epoch:  612  loss:  0.06409997493028641\n",
            "epoch:  613  loss:  0.06409387290477753\n",
            "epoch:  614  loss:  0.06408777832984924\n",
            "epoch:  615  loss:  0.06408169865608215\n",
            "epoch:  616  loss:  0.06407559663057327\n",
            "epoch:  617  loss:  0.06406951695680618\n",
            "epoch:  618  loss:  0.06406344473361969\n",
            "epoch:  619  loss:  0.0640573650598526\n",
            "epoch:  620  loss:  0.06405129283666611\n",
            "epoch:  621  loss:  0.06404522806406021\n",
            "epoch:  622  loss:  0.06403916329145432\n",
            "epoch:  623  loss:  0.06403309851884842\n",
            "epoch:  624  loss:  0.06402704864740372\n",
            "epoch:  625  loss:  0.06402099877595901\n",
            "epoch:  626  loss:  0.06401494145393372\n",
            "epoch:  627  loss:  0.06400889903306961\n",
            "epoch:  628  loss:  0.0640028566122055\n",
            "epoch:  629  loss:  0.063996821641922\n",
            "epoch:  630  loss:  0.06399078667163849\n",
            "epoch:  631  loss:  0.06398475915193558\n",
            "epoch:  632  loss:  0.06397873163223267\n",
            "epoch:  633  loss:  0.06397270411252975\n",
            "epoch:  634  loss:  0.06396668404340744\n",
            "epoch:  635  loss:  0.06396066397428513\n",
            "epoch:  636  loss:  0.063954658806324\n",
            "epoch:  637  loss:  0.06394864618778229\n",
            "epoch:  638  loss:  0.06394264101982117\n",
            "epoch:  639  loss:  0.06393664330244064\n",
            "epoch:  640  loss:  0.06393064558506012\n",
            "epoch:  641  loss:  0.06392465531826019\n",
            "epoch:  642  loss:  0.06391866505146027\n",
            "epoch:  643  loss:  0.06391268223524094\n",
            "epoch:  644  loss:  0.0639066994190216\n",
            "epoch:  645  loss:  0.06390072405338287\n",
            "epoch:  646  loss:  0.06389474123716354\n",
            "epoch:  647  loss:  0.06388877332210541\n",
            "epoch:  648  loss:  0.06388280540704727\n",
            "epoch:  649  loss:  0.06387684494256973\n",
            "epoch:  650  loss:  0.0638708844780922\n",
            "epoch:  651  loss:  0.06386493146419525\n",
            "epoch:  652  loss:  0.06385897845029831\n",
            "epoch:  653  loss:  0.06385302543640137\n",
            "epoch:  654  loss:  0.06384707987308502\n",
            "epoch:  655  loss:  0.06384113430976868\n",
            "epoch:  656  loss:  0.06383521109819412\n",
            "epoch:  657  loss:  0.06382927298545837\n",
            "epoch:  658  loss:  0.06382334232330322\n",
            "epoch:  659  loss:  0.06381741911172867\n",
            "epoch:  660  loss:  0.06381148844957352\n",
            "epoch:  661  loss:  0.06380557268857956\n",
            "epoch:  662  loss:  0.0637996569275856\n",
            "epoch:  663  loss:  0.06379374861717224\n",
            "epoch:  664  loss:  0.06378783285617828\n",
            "epoch:  665  loss:  0.06378192454576492\n",
            "epoch:  666  loss:  0.06377603858709335\n",
            "epoch:  667  loss:  0.06377013772726059\n",
            "epoch:  668  loss:  0.06376424431800842\n",
            "epoch:  669  loss:  0.06375835090875626\n",
            "epoch:  670  loss:  0.06375246495008469\n",
            "epoch:  671  loss:  0.06374657899141312\n",
            "epoch:  672  loss:  0.06374070048332214\n",
            "epoch:  673  loss:  0.06373482942581177\n",
            "epoch:  674  loss:  0.06372895836830139\n",
            "epoch:  675  loss:  0.06372308731079102\n",
            "epoch:  676  loss:  0.06371723115444183\n",
            "epoch:  677  loss:  0.06371136754751205\n",
            "epoch:  678  loss:  0.06370550394058228\n",
            "epoch:  679  loss:  0.06369965523481369\n",
            "epoch:  680  loss:  0.0636938065290451\n",
            "epoch:  681  loss:  0.06368795782327652\n",
            "epoch:  682  loss:  0.06368211656808853\n",
            "epoch:  683  loss:  0.06367626786231995\n",
            "epoch:  684  loss:  0.06367044150829315\n",
            "epoch:  685  loss:  0.06366460770368576\n",
            "epoch:  686  loss:  0.06365877389907837\n",
            "epoch:  687  loss:  0.06365294754505157\n",
            "epoch:  688  loss:  0.06364712864160538\n",
            "epoch:  689  loss:  0.06364130228757858\n",
            "epoch:  690  loss:  0.06363549828529358\n",
            "epoch:  691  loss:  0.06362968683242798\n",
            "epoch:  692  loss:  0.06362387537956238\n",
            "epoch:  693  loss:  0.06361807137727737\n",
            "epoch:  694  loss:  0.06361227482557297\n",
            "epoch:  695  loss:  0.06360647827386856\n",
            "epoch:  696  loss:  0.06360068917274475\n",
            "epoch:  697  loss:  0.06359490007162094\n",
            "epoch:  698  loss:  0.06358911097049713\n",
            "epoch:  699  loss:  0.06358332931995392\n",
            "epoch:  700  loss:  0.0635775476694107\n",
            "epoch:  701  loss:  0.06357177346944809\n",
            "epoch:  702  loss:  0.06356599926948547\n",
            "epoch:  703  loss:  0.06356023252010345\n",
            "epoch:  704  loss:  0.06355447322130203\n",
            "epoch:  705  loss:  0.06354870647192001\n",
            "epoch:  706  loss:  0.06354295462369919\n",
            "epoch:  707  loss:  0.06353719532489777\n",
            "epoch:  708  loss:  0.06353145092725754\n",
            "epoch:  709  loss:  0.06352570652961731\n",
            "epoch:  710  loss:  0.06351996213197708\n",
            "epoch:  711  loss:  0.06351421773433685\n",
            "epoch:  712  loss:  0.06350848078727722\n",
            "epoch:  713  loss:  0.06350274384021759\n",
            "epoch:  714  loss:  0.06349702179431915\n",
            "epoch:  715  loss:  0.06349129229784012\n",
            "epoch:  716  loss:  0.06348557025194168\n",
            "epoch:  717  loss:  0.06347984820604324\n",
            "epoch:  718  loss:  0.0634741336107254\n",
            "epoch:  719  loss:  0.06346842646598816\n",
            "epoch:  720  loss:  0.06346271932125092\n",
            "epoch:  721  loss:  0.06345701217651367\n",
            "epoch:  722  loss:  0.06345131248235703\n",
            "epoch:  723  loss:  0.06344561278820038\n",
            "epoch:  724  loss:  0.06343992799520493\n",
            "epoch:  725  loss:  0.06343422830104828\n",
            "epoch:  726  loss:  0.06342854350805283\n",
            "epoch:  727  loss:  0.06342285871505737\n",
            "epoch:  728  loss:  0.06341718137264252\n",
            "epoch:  729  loss:  0.06341150403022766\n",
            "epoch:  730  loss:  0.0634058266878128\n",
            "epoch:  731  loss:  0.06340015679597855\n",
            "epoch:  732  loss:  0.06339449435472488\n",
            "epoch:  733  loss:  0.06338883191347122\n",
            "epoch:  734  loss:  0.06338316947221756\n",
            "epoch:  735  loss:  0.0633775070309639\n",
            "epoch:  736  loss:  0.06337185949087143\n",
            "epoch:  737  loss:  0.06336621195077896\n",
            "epoch:  738  loss:  0.06336056441068649\n",
            "epoch:  739  loss:  0.06335492432117462\n",
            "epoch:  740  loss:  0.06334928423166275\n",
            "epoch:  741  loss:  0.06334364414215088\n",
            "epoch:  742  loss:  0.0633380115032196\n",
            "epoch:  743  loss:  0.06333238631486893\n",
            "epoch:  744  loss:  0.06332676112651825\n",
            "epoch:  745  loss:  0.06332113593816757\n",
            "epoch:  746  loss:  0.06331551820039749\n",
            "epoch:  747  loss:  0.06330990791320801\n",
            "epoch:  748  loss:  0.06330429017543793\n",
            "epoch:  749  loss:  0.06329867988824844\n",
            "epoch:  750  loss:  0.06329307705163956\n",
            "epoch:  751  loss:  0.06328748166561127\n",
            "epoch:  752  loss:  0.06328187882900238\n",
            "epoch:  753  loss:  0.06327628344297409\n",
            "epoch:  754  loss:  0.0632706955075264\n",
            "epoch:  755  loss:  0.0632651075720787\n",
            "epoch:  756  loss:  0.06325951963663101\n",
            "epoch:  757  loss:  0.06325393915176392\n",
            "epoch:  758  loss:  0.06324836611747742\n",
            "epoch:  759  loss:  0.06324279308319092\n",
            "epoch:  760  loss:  0.06323722004890442\n",
            "epoch:  761  loss:  0.06323164701461792\n",
            "epoch:  762  loss:  0.06322608888149261\n",
            "epoch:  763  loss:  0.06322053074836731\n",
            "epoch:  764  loss:  0.063214972615242\n",
            "epoch:  765  loss:  0.0632094144821167\n",
            "epoch:  766  loss:  0.06320386379957199\n",
            "epoch:  767  loss:  0.06319832056760788\n",
            "epoch:  768  loss:  0.06319277733564377\n",
            "epoch:  769  loss:  0.06318723410367966\n",
            "epoch:  770  loss:  0.06318169087171555\n",
            "epoch:  771  loss:  0.06317616999149323\n",
            "epoch:  772  loss:  0.06317062675952911\n",
            "epoch:  773  loss:  0.0631651058793068\n",
            "epoch:  774  loss:  0.06315957754850388\n",
            "epoch:  775  loss:  0.06315405666828156\n",
            "epoch:  776  loss:  0.06314854323863983\n",
            "epoch:  777  loss:  0.06314302980899811\n",
            "epoch:  778  loss:  0.06313751637935638\n",
            "epoch:  779  loss:  0.06313201785087585\n",
            "epoch:  780  loss:  0.06312651187181473\n",
            "epoch:  781  loss:  0.0631210058927536\n",
            "epoch:  782  loss:  0.06311550736427307\n",
            "epoch:  783  loss:  0.06311001628637314\n",
            "epoch:  784  loss:  0.0631045252084732\n",
            "epoch:  785  loss:  0.06309903413057327\n",
            "epoch:  786  loss:  0.06309355050325394\n",
            "epoch:  787  loss:  0.0630880743265152\n",
            "epoch:  788  loss:  0.06308259814977646\n",
            "epoch:  789  loss:  0.06307712197303772\n",
            "epoch:  790  loss:  0.06307165324687958\n",
            "epoch:  791  loss:  0.06306619197130203\n",
            "epoch:  792  loss:  0.06306072324514389\n",
            "epoch:  793  loss:  0.06305525451898575\n",
            "epoch:  794  loss:  0.0630498081445694\n",
            "epoch:  795  loss:  0.06304434686899185\n",
            "epoch:  796  loss:  0.0630389004945755\n",
            "epoch:  797  loss:  0.06303345412015915\n",
            "epoch:  798  loss:  0.0630280077457428\n",
            "epoch:  799  loss:  0.06302256137132645\n",
            "epoch:  800  loss:  0.06301712989807129\n",
            "epoch:  801  loss:  0.06301169097423553\n",
            "epoch:  802  loss:  0.06300627440214157\n",
            "epoch:  803  loss:  0.06300084292888641\n",
            "epoch:  804  loss:  0.06299541145563126\n",
            "epoch:  805  loss:  0.0629899874329567\n",
            "epoch:  806  loss:  0.06298457831144333\n",
            "epoch:  807  loss:  0.06297916173934937\n",
            "epoch:  808  loss:  0.062973752617836\n",
            "epoch:  809  loss:  0.06296833604574203\n",
            "epoch:  810  loss:  0.06296293437480927\n",
            "epoch:  811  loss:  0.0629575327038765\n",
            "epoch:  812  loss:  0.06295213103294373\n",
            "epoch:  813  loss:  0.06294674426317215\n",
            "epoch:  814  loss:  0.06294134259223938\n",
            "epoch:  815  loss:  0.0629359558224678\n",
            "epoch:  816  loss:  0.06293057650327682\n",
            "epoch:  817  loss:  0.06292519718408585\n",
            "epoch:  818  loss:  0.06291981786489487\n",
            "epoch:  819  loss:  0.06291444599628448\n",
            "epoch:  820  loss:  0.0629090741276741\n",
            "epoch:  821  loss:  0.06290370225906372\n",
            "epoch:  822  loss:  0.06289833784103394\n",
            "epoch:  823  loss:  0.06289297342300415\n",
            "epoch:  824  loss:  0.06288761645555496\n",
            "epoch:  825  loss:  0.06288225948810577\n",
            "epoch:  826  loss:  0.06287690252065659\n",
            "epoch:  827  loss:  0.062871553003788\n",
            "epoch:  828  loss:  0.0628662034869194\n",
            "epoch:  829  loss:  0.062860868871212\n",
            "epoch:  830  loss:  0.06285552680492401\n",
            "epoch:  831  loss:  0.06285019218921661\n",
            "epoch:  832  loss:  0.06284485012292862\n",
            "epoch:  833  loss:  0.06283952295780182\n",
            "epoch:  834  loss:  0.06283419579267502\n",
            "epoch:  835  loss:  0.06282887607812881\n",
            "epoch:  836  loss:  0.06282355636358261\n",
            "epoch:  837  loss:  0.06281823664903641\n",
            "epoch:  838  loss:  0.0628129169344902\n",
            "epoch:  839  loss:  0.0628076121211052\n",
            "epoch:  840  loss:  0.06280229985713959\n",
            "epoch:  841  loss:  0.06279699504375458\n",
            "epoch:  842  loss:  0.06279169768095016\n",
            "epoch:  843  loss:  0.06278640031814575\n",
            "epoch:  844  loss:  0.06278110295534134\n",
            "epoch:  845  loss:  0.06277580559253693\n",
            "epoch:  846  loss:  0.06277052313089371\n",
            "epoch:  847  loss:  0.06276523321866989\n",
            "epoch:  848  loss:  0.06275995820760727\n",
            "epoch:  849  loss:  0.06275466829538345\n",
            "epoch:  850  loss:  0.06274940073490143\n",
            "epoch:  851  loss:  0.0627441257238388\n",
            "epoch:  852  loss:  0.06273885816335678\n",
            "epoch:  853  loss:  0.06273359060287476\n",
            "epoch:  854  loss:  0.06272832304239273\n",
            "epoch:  855  loss:  0.0627230703830719\n",
            "epoch:  856  loss:  0.06271780282258987\n",
            "epoch:  857  loss:  0.06271255016326904\n",
            "epoch:  858  loss:  0.06270729750394821\n",
            "epoch:  859  loss:  0.06270205229520798\n",
            "epoch:  860  loss:  0.06269680708646774\n",
            "epoch:  861  loss:  0.0626915693283081\n",
            "epoch:  862  loss:  0.06268633157014847\n",
            "epoch:  863  loss:  0.06268109381198883\n",
            "epoch:  864  loss:  0.06267586350440979\n",
            "epoch:  865  loss:  0.06267063319683075\n",
            "epoch:  866  loss:  0.06266540288925171\n",
            "epoch:  867  loss:  0.06266018003225327\n",
            "epoch:  868  loss:  0.06265495717525482\n",
            "epoch:  869  loss:  0.06264974921941757\n",
            "epoch:  870  loss:  0.06264453381299973\n",
            "epoch:  871  loss:  0.06263931840658188\n",
            "epoch:  872  loss:  0.06263411045074463\n",
            "epoch:  873  loss:  0.06262890994548798\n",
            "epoch:  874  loss:  0.06262370944023132\n",
            "epoch:  875  loss:  0.06261850893497467\n",
            "epoch:  876  loss:  0.06261331588029861\n",
            "epoch:  877  loss:  0.06260812282562256\n",
            "epoch:  878  loss:  0.0626029297709465\n",
            "epoch:  879  loss:  0.06259774416685104\n",
            "epoch:  880  loss:  0.06259256601333618\n",
            "epoch:  881  loss:  0.06258738785982132\n",
            "epoch:  882  loss:  0.06258220970630646\n",
            "epoch:  883  loss:  0.06257703900337219\n",
            "epoch:  884  loss:  0.06257186830043793\n",
            "epoch:  885  loss:  0.06256670504808426\n",
            "epoch:  886  loss:  0.06256153434515\n",
            "epoch:  887  loss:  0.06255637854337692\n",
            "epoch:  888  loss:  0.06255121529102325\n",
            "epoch:  889  loss:  0.06254605948925018\n",
            "epoch:  890  loss:  0.06254090368747711\n",
            "epoch:  891  loss:  0.06253575533628464\n",
            "epoch:  892  loss:  0.06253060698509216\n",
            "epoch:  893  loss:  0.06252547353506088\n",
            "epoch:  894  loss:  0.062520332634449\n",
            "epoch:  895  loss:  0.06251518428325653\n",
            "epoch:  896  loss:  0.06251005828380585\n",
            "epoch:  897  loss:  0.06250493228435516\n",
            "epoch:  898  loss:  0.06249980255961418\n",
            "epoch:  899  loss:  0.0624946728348732\n",
            "epoch:  900  loss:  0.062489550560712814\n",
            "epoch:  901  loss:  0.062484435737133026\n",
            "epoch:  902  loss:  0.06247931718826294\n",
            "epoch:  903  loss:  0.06247420608997345\n",
            "epoch:  904  loss:  0.06246909871697426\n",
            "epoch:  905  loss:  0.06246398761868477\n",
            "epoch:  906  loss:  0.062458883970975876\n",
            "epoch:  907  loss:  0.06245378777384758\n",
            "epoch:  908  loss:  0.062448691576719284\n",
            "epoch:  909  loss:  0.06244359537959099\n",
            "epoch:  910  loss:  0.06243850663304329\n",
            "epoch:  911  loss:  0.06243341416120529\n",
            "epoch:  912  loss:  0.06242832913994789\n",
            "epoch:  913  loss:  0.06242324784398079\n",
            "epoch:  914  loss:  0.06241816654801369\n",
            "epoch:  915  loss:  0.062413088977336884\n",
            "epoch:  916  loss:  0.06240801885724068\n",
            "epoch:  917  loss:  0.06240294873714447\n",
            "epoch:  918  loss:  0.06239788234233856\n",
            "epoch:  919  loss:  0.06239281967282295\n",
            "epoch:  920  loss:  0.062387753278017044\n",
            "epoch:  921  loss:  0.062382690608501434\n",
            "epoch:  922  loss:  0.06237763911485672\n",
            "epoch:  923  loss:  0.06237258389592171\n",
            "epoch:  924  loss:  0.06236753985285759\n",
            "epoch:  925  loss:  0.062362492084503174\n",
            "epoch:  926  loss:  0.062357448041439056\n",
            "epoch:  927  loss:  0.06235240399837494\n",
            "epoch:  928  loss:  0.06234737113118172\n",
            "epoch:  929  loss:  0.0623423308134079\n",
            "epoch:  930  loss:  0.06233730539679527\n",
            "epoch:  931  loss:  0.06233227625489235\n",
            "epoch:  932  loss:  0.062327247112989426\n",
            "epoch:  933  loss:  0.0623222254216671\n",
            "epoch:  934  loss:  0.06231720745563507\n",
            "epoch:  935  loss:  0.06231218948960304\n",
            "epoch:  936  loss:  0.06230717897415161\n",
            "epoch:  937  loss:  0.062302153557538986\n",
            "epoch:  938  loss:  0.06229715049266815\n",
            "epoch:  939  loss:  0.06229214370250702\n",
            "epoch:  940  loss:  0.06228714436292648\n",
            "epoch:  941  loss:  0.062282148748636246\n",
            "epoch:  942  loss:  0.06227714568376541\n",
            "epoch:  943  loss:  0.06227215752005577\n",
            "epoch:  944  loss:  0.06226716935634613\n",
            "epoch:  945  loss:  0.06226218119263649\n",
            "epoch:  946  loss:  0.06225719675421715\n",
            "epoch:  947  loss:  0.062252212315797806\n",
            "epoch:  948  loss:  0.06224723532795906\n",
            "epoch:  949  loss:  0.062242258340120316\n",
            "epoch:  950  loss:  0.06223727762699127\n",
            "epoch:  951  loss:  0.06223231554031372\n",
            "epoch:  952  loss:  0.06222734972834587\n",
            "epoch:  953  loss:  0.06222238019108772\n",
            "epoch:  954  loss:  0.06221741810441017\n",
            "epoch:  955  loss:  0.062212467193603516\n",
            "epoch:  956  loss:  0.062207505106925964\n",
            "epoch:  957  loss:  0.062202561646699905\n",
            "epoch:  958  loss:  0.06219761073589325\n",
            "epoch:  959  loss:  0.062192656099796295\n",
            "epoch:  960  loss:  0.062187716364860535\n",
            "epoch:  961  loss:  0.062182776629924774\n",
            "epoch:  962  loss:  0.062177836894989014\n",
            "epoch:  963  loss:  0.06217289716005325\n",
            "epoch:  964  loss:  0.06216796487569809\n",
            "epoch:  965  loss:  0.062163036316633224\n",
            "epoch:  966  loss:  0.062158115208148956\n",
            "epoch:  967  loss:  0.06215318664908409\n",
            "epoch:  968  loss:  0.06214826554059982\n",
            "epoch:  969  loss:  0.06214334815740585\n",
            "epoch:  970  loss:  0.06213843449950218\n",
            "epoch:  971  loss:  0.06213352084159851\n",
            "epoch:  972  loss:  0.06212861090898514\n",
            "epoch:  973  loss:  0.062123704701662064\n",
            "epoch:  974  loss:  0.06211879849433899\n",
            "epoch:  975  loss:  0.06211389973759651\n",
            "epoch:  976  loss:  0.062109000980854034\n",
            "epoch:  977  loss:  0.06210410222411156\n",
            "epoch:  978  loss:  0.062099210917949677\n",
            "epoch:  979  loss:  0.062094323337078094\n",
            "epoch:  980  loss:  0.06208943948149681\n",
            "epoch:  981  loss:  0.062084559351205826\n",
            "epoch:  982  loss:  0.06207967549562454\n",
            "epoch:  983  loss:  0.06207479536533356\n",
            "epoch:  984  loss:  0.06206992268562317\n",
            "epoch:  985  loss:  0.06206505373120308\n",
            "epoch:  986  loss:  0.06206018105149269\n",
            "epoch:  987  loss:  0.0620553083717823\n",
            "epoch:  988  loss:  0.06205045059323311\n",
            "epoch:  989  loss:  0.06204558163881302\n",
            "epoch:  990  loss:  0.06204073131084442\n",
            "epoch:  991  loss:  0.06203587353229523\n",
            "epoch:  992  loss:  0.06203101947903633\n",
            "epoch:  993  loss:  0.06202617660164833\n",
            "epoch:  994  loss:  0.06202132627367973\n",
            "epoch:  995  loss:  0.062016479671001434\n",
            "epoch:  996  loss:  0.06201164424419403\n",
            "epoch:  997  loss:  0.06200679764151573\n",
            "epoch:  998  loss:  0.06200196593999863\n",
            "epoch:  999  loss:  0.061997123062610626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKN-vIwjvwRK",
        "outputId": "602cba46-19eb-468f-f5d7-9015be158161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.16416653990745544\n",
            "epoch:  1  loss:  0.16391539573669434\n",
            "epoch:  2  loss:  0.16366639733314514\n",
            "epoch:  3  loss:  0.16341951489448547\n",
            "epoch:  4  loss:  0.16317471861839294\n",
            "epoch:  5  loss:  0.16293200850486755\n",
            "epoch:  6  loss:  0.16269133985042572\n",
            "epoch:  7  loss:  0.16245266795158386\n",
            "epoch:  8  loss:  0.16221600770950317\n",
            "epoch:  9  loss:  0.16198131442070007\n",
            "epoch:  10  loss:  0.16174854338169098\n",
            "epoch:  11  loss:  0.16151772439479828\n",
            "epoch:  12  loss:  0.16128875315189362\n",
            "epoch:  13  loss:  0.16106167435646057\n",
            "epoch:  14  loss:  0.16083644330501556\n",
            "epoch:  15  loss:  0.160613015294075\n",
            "epoch:  16  loss:  0.16039139032363892\n",
            "epoch:  17  loss:  0.16017155349254608\n",
            "epoch:  18  loss:  0.15995343029499054\n",
            "epoch:  19  loss:  0.15973706543445587\n",
            "epoch:  20  loss:  0.1595223844051361\n",
            "epoch:  21  loss:  0.15930940210819244\n",
            "epoch:  22  loss:  0.1590980887413025\n",
            "epoch:  23  loss:  0.15888839960098267\n",
            "epoch:  24  loss:  0.15868034958839417\n",
            "epoch:  25  loss:  0.1584739089012146\n",
            "epoch:  26  loss:  0.1582690328359604\n",
            "epoch:  27  loss:  0.15806572139263153\n",
            "epoch:  28  loss:  0.15786395967006683\n",
            "epoch:  29  loss:  0.1576637327671051\n",
            "epoch:  30  loss:  0.15746496617794037\n",
            "epoch:  31  loss:  0.1572677344083786\n",
            "epoch:  32  loss:  0.15707196295261383\n",
            "epoch:  33  loss:  0.15687760710716248\n",
            "epoch:  34  loss:  0.1566847413778305\n",
            "epoch:  35  loss:  0.15649323165416718\n",
            "epoch:  36  loss:  0.15630313754081726\n",
            "epoch:  37  loss:  0.15611442923545837\n",
            "epoch:  38  loss:  0.15592706203460693\n",
            "epoch:  39  loss:  0.15574108064174652\n",
            "epoch:  40  loss:  0.15555638074874878\n",
            "epoch:  41  loss:  0.15537302196025848\n",
            "epoch:  42  loss:  0.15519095957279205\n",
            "epoch:  43  loss:  0.1550101637840271\n",
            "epoch:  44  loss:  0.154830664396286\n",
            "epoch:  45  loss:  0.15465234220027924\n",
            "epoch:  46  loss:  0.15447533130645752\n",
            "epoch:  47  loss:  0.15429949760437012\n",
            "epoch:  48  loss:  0.1541248857975006\n",
            "epoch:  49  loss:  0.1539514809846878\n",
            "epoch:  50  loss:  0.15377923846244812\n",
            "epoch:  51  loss:  0.15360815823078156\n",
            "epoch:  52  loss:  0.15343819558620453\n",
            "epoch:  53  loss:  0.153269425034523\n",
            "epoch:  54  loss:  0.15310172736644745\n",
            "epoch:  55  loss:  0.15293514728546143\n",
            "epoch:  56  loss:  0.15276968479156494\n",
            "epoch:  57  loss:  0.1526053100824356\n",
            "epoch:  58  loss:  0.15244202315807343\n",
            "epoch:  59  loss:  0.15227973461151123\n",
            "epoch:  60  loss:  0.1521185338497162\n",
            "epoch:  61  loss:  0.15195834636688232\n",
            "epoch:  62  loss:  0.15179918706417084\n",
            "epoch:  63  loss:  0.15164104104042053\n",
            "epoch:  64  loss:  0.15148389339447021\n",
            "epoch:  65  loss:  0.1513277292251587\n",
            "epoch:  66  loss:  0.15117253363132477\n",
            "epoch:  67  loss:  0.15101830661296844\n",
            "epoch:  68  loss:  0.15086506307125092\n",
            "epoch:  69  loss:  0.1507127285003662\n",
            "epoch:  70  loss:  0.1505613476037979\n",
            "epoch:  71  loss:  0.15041087567806244\n",
            "epoch:  72  loss:  0.1502612978219986\n",
            "epoch:  73  loss:  0.15011264383792877\n",
            "epoch:  74  loss:  0.14996486902236938\n",
            "epoch:  75  loss:  0.14981798827648163\n",
            "epoch:  76  loss:  0.1496719866991043\n",
            "epoch:  77  loss:  0.14952683448791504\n",
            "epoch:  78  loss:  0.14938253164291382\n",
            "epoch:  79  loss:  0.14923909306526184\n",
            "epoch:  80  loss:  0.14909644424915314\n",
            "epoch:  81  loss:  0.14895464479923248\n",
            "epoch:  82  loss:  0.1488136649131775\n",
            "epoch:  83  loss:  0.14867347478866577\n",
            "epoch:  84  loss:  0.14853410422801971\n",
            "epoch:  85  loss:  0.14839550852775574\n",
            "epoch:  86  loss:  0.14825768768787384\n",
            "epoch:  87  loss:  0.14812065660953522\n",
            "epoch:  88  loss:  0.1479843556880951\n",
            "epoch:  89  loss:  0.14784884452819824\n",
            "epoch:  90  loss:  0.1477140635251999\n",
            "epoch:  91  loss:  0.14758002758026123\n",
            "epoch:  92  loss:  0.14744673669338226\n",
            "epoch:  93  loss:  0.1473141461610794\n",
            "epoch:  94  loss:  0.14718228578567505\n",
            "epoch:  95  loss:  0.1470511257648468\n",
            "epoch:  96  loss:  0.14692066609859467\n",
            "epoch:  97  loss:  0.14679090678691864\n",
            "epoch:  98  loss:  0.14666183292865753\n",
            "epoch:  99  loss:  0.14653342962265015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, avg_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "id": "ReENqF2bwJBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325f34ab-3919-4802-cf26-2b465ccb53a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.3487427234649658\n",
            "epoch:  1  loss:  0.34613001346588135\n",
            "epoch:  2  loss:  0.3435584008693695\n",
            "epoch:  3  loss:  0.34102705121040344\n",
            "epoch:  4  loss:  0.33853504061698914\n",
            "epoch:  5  loss:  0.3360815942287445\n",
            "epoch:  6  loss:  0.33366599678993225\n",
            "epoch:  7  loss:  0.33128732442855835\n",
            "epoch:  8  loss:  0.3289449214935303\n",
            "epoch:  9  loss:  0.32663804292678833\n",
            "epoch:  10  loss:  0.3243659734725952\n",
            "epoch:  11  loss:  0.32212796807289124\n",
            "epoch:  12  loss:  0.319923460483551\n",
            "epoch:  13  loss:  0.3177516758441925\n",
            "epoch:  14  loss:  0.3156120777130127\n",
            "epoch:  15  loss:  0.31350401043891907\n",
            "epoch:  16  loss:  0.3114267885684967\n",
            "epoch:  17  loss:  0.3093799650669098\n",
            "epoch:  18  loss:  0.3073629140853882\n",
            "epoch:  19  loss:  0.30537500977516174\n",
            "epoch:  20  loss:  0.30341580510139465\n",
            "epoch:  21  loss:  0.30148473381996155\n",
            "epoch:  22  loss:  0.29958122968673706\n",
            "epoch:  23  loss:  0.29770487546920776\n",
            "epoch:  24  loss:  0.2958551049232483\n",
            "epoch:  25  loss:  0.2940314710140228\n",
            "epoch:  26  loss:  0.29223349690437317\n",
            "epoch:  27  loss:  0.2904607355594635\n",
            "epoch:  28  loss:  0.288712739944458\n",
            "epoch:  29  loss:  0.2869890630245209\n",
            "epoch:  30  loss:  0.28528931736946106\n",
            "epoch:  31  loss:  0.283612996339798\n",
            "epoch:  32  loss:  0.2819598317146301\n",
            "epoch:  33  loss:  0.28032931685447693\n",
            "epoch:  34  loss:  0.2787210941314697\n",
            "epoch:  35  loss:  0.27713483572006226\n",
            "epoch:  36  loss:  0.2755700647830963\n",
            "epoch:  37  loss:  0.2740265130996704\n",
            "epoch:  38  loss:  0.2725037932395935\n",
            "epoch:  39  loss:  0.27100154757499695\n",
            "epoch:  40  loss:  0.26951947808265686\n",
            "epoch:  41  loss:  0.2680572271347046\n",
            "epoch:  42  loss:  0.2666144371032715\n",
            "epoch:  43  loss:  0.26519086956977844\n",
            "epoch:  44  loss:  0.26378610730171204\n",
            "epoch:  45  loss:  0.26239994168281555\n",
            "epoch:  46  loss:  0.2610320448875427\n",
            "epoch:  47  loss:  0.2596821188926697\n",
            "epoch:  48  loss:  0.25834986567497253\n",
            "epoch:  49  loss:  0.2570350468158722\n",
            "epoch:  50  loss:  0.2557373046875\n",
            "epoch:  51  loss:  0.25445646047592163\n",
            "epoch:  52  loss:  0.2531921863555908\n",
            "epoch:  53  loss:  0.25194430351257324\n",
            "epoch:  54  loss:  0.25071248412132263\n",
            "epoch:  55  loss:  0.24949650466442108\n",
            "epoch:  56  loss:  0.2482960969209671\n",
            "epoch:  57  loss:  0.24711106717586517\n",
            "epoch:  58  loss:  0.24594111740589142\n",
            "epoch:  59  loss:  0.2447860836982727\n",
            "epoch:  60  loss:  0.24364574253559113\n",
            "epoch:  61  loss:  0.2425197958946228\n",
            "epoch:  62  loss:  0.241408109664917\n",
            "epoch:  63  loss:  0.240310400724411\n",
            "epoch:  64  loss:  0.2392265349626541\n",
            "epoch:  65  loss:  0.23815621435642242\n",
            "epoch:  66  loss:  0.23709933459758759\n",
            "epoch:  67  loss:  0.2360556274652481\n",
            "epoch:  68  loss:  0.23502492904663086\n",
            "epoch:  69  loss:  0.23400703072547913\n",
            "epoch:  70  loss:  0.23300176858901978\n",
            "epoch:  71  loss:  0.23200896382331848\n",
            "epoch:  72  loss:  0.23102836310863495\n",
            "epoch:  73  loss:  0.23005987703800201\n",
            "epoch:  74  loss:  0.22910325229167938\n",
            "epoch:  75  loss:  0.2281583994626999\n",
            "epoch:  76  loss:  0.22722506523132324\n",
            "epoch:  77  loss:  0.22630316019058228\n",
            "epoch:  78  loss:  0.2253924310207367\n",
            "epoch:  79  loss:  0.22449283301830292\n",
            "epoch:  80  loss:  0.22360408306121826\n",
            "epoch:  81  loss:  0.22272612154483795\n",
            "epoch:  82  loss:  0.22185873985290527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXMrI4XKtZDp",
        "outputId": "e1f85632-f950-4b9a-fc27-dfa854905610"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 4096])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FynnOhGItLo8",
        "outputId": "3ace85d4-31b5-46bd-c965-a436a5fb70c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.569\n",
            "accuracy 0.659\n",
            "accuracy 0.701\n",
            "accuracy 0.768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(1000):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, new_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-CtI340iem0",
        "outputId": "519ecad3-f250-4d76-9f5e-40129a5a2f5a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.24873816967010498\n",
            "epoch:  1  loss:  0.24798521399497986\n",
            "epoch:  2  loss:  0.24723653495311737\n",
            "epoch:  3  loss:  0.24649202823638916\n",
            "epoch:  4  loss:  0.24575172364711761\n",
            "epoch:  5  loss:  0.24501553177833557\n",
            "epoch:  6  loss:  0.24428340792655945\n",
            "epoch:  7  loss:  0.24355530738830566\n",
            "epoch:  8  loss:  0.2428312599658966\n",
            "epoch:  9  loss:  0.24211116135120392\n",
            "epoch:  10  loss:  0.24139496684074402\n",
            "epoch:  11  loss:  0.2406826615333557\n",
            "epoch:  12  loss:  0.239974245429039\n",
            "epoch:  13  loss:  0.23926961421966553\n",
            "epoch:  14  loss:  0.2385687530040741\n",
            "epoch:  15  loss:  0.23787164688110352\n",
            "epoch:  16  loss:  0.2371782511472702\n",
            "epoch:  17  loss:  0.236488476395607\n",
            "epoch:  18  loss:  0.23580241203308105\n",
            "epoch:  19  loss:  0.23511993885040283\n",
            "epoch:  20  loss:  0.23444101214408875\n",
            "epoch:  21  loss:  0.2337656468153\n",
            "epoch:  22  loss:  0.23309379816055298\n",
            "epoch:  23  loss:  0.23242536187171936\n",
            "epoch:  24  loss:  0.2317604273557663\n",
            "epoch:  25  loss:  0.23109890520572662\n",
            "epoch:  26  loss:  0.23044075071811676\n",
            "epoch:  27  loss:  0.2297859638929367\n",
            "epoch:  28  loss:  0.2291344553232193\n",
            "epoch:  29  loss:  0.2284862995147705\n",
            "epoch:  30  loss:  0.2278413623571396\n",
            "epoch:  31  loss:  0.22719968855381012\n",
            "epoch:  32  loss:  0.22656121850013733\n",
            "epoch:  33  loss:  0.2259259670972824\n",
            "epoch:  34  loss:  0.225293830037117\n",
            "epoch:  35  loss:  0.2246648222208023\n",
            "epoch:  36  loss:  0.22403891384601593\n",
            "epoch:  37  loss:  0.22341609001159668\n",
            "epoch:  38  loss:  0.22279633581638336\n",
            "epoch:  39  loss:  0.22217957675457\n",
            "epoch:  40  loss:  0.221565842628479\n",
            "epoch:  41  loss:  0.2209550440311432\n",
            "epoch:  42  loss:  0.22034722566604614\n",
            "epoch:  43  loss:  0.2197423279285431\n",
            "epoch:  44  loss:  0.21914033591747284\n",
            "epoch:  45  loss:  0.2185412347316742\n",
            "epoch:  46  loss:  0.21794497966766357\n",
            "epoch:  47  loss:  0.2173515409231186\n",
            "epoch:  48  loss:  0.21676093339920044\n",
            "epoch:  49  loss:  0.21617312729358673\n",
            "epoch:  50  loss:  0.21558809280395508\n",
            "epoch:  51  loss:  0.2150057852268219\n",
            "epoch:  52  loss:  0.214426189661026\n",
            "epoch:  53  loss:  0.21384933590888977\n",
            "epoch:  54  loss:  0.21327516436576843\n",
            "epoch:  55  loss:  0.2127036303281784\n",
            "epoch:  56  loss:  0.21213476359844208\n",
            "epoch:  57  loss:  0.21156853437423706\n",
            "epoch:  58  loss:  0.21100486814975739\n",
            "epoch:  59  loss:  0.21044379472732544\n",
            "epoch:  60  loss:  0.20988531410694122\n",
            "epoch:  61  loss:  0.20932938158512115\n",
            "epoch:  62  loss:  0.20877596735954285\n",
            "epoch:  63  loss:  0.2082250565290451\n",
            "epoch:  64  loss:  0.20767666399478912\n",
            "epoch:  65  loss:  0.20713071525096893\n",
            "epoch:  66  loss:  0.20658725500106812\n",
            "epoch:  67  loss:  0.2060462087392807\n",
            "epoch:  68  loss:  0.20550760626792908\n",
            "epoch:  69  loss:  0.20497137308120728\n",
            "epoch:  70  loss:  0.20443759858608246\n",
            "epoch:  71  loss:  0.2039061337709427\n",
            "epoch:  72  loss:  0.20337703824043274\n",
            "epoch:  73  loss:  0.2028503119945526\n",
            "epoch:  74  loss:  0.20232591032981873\n",
            "epoch:  75  loss:  0.2018037885427475\n",
            "epoch:  76  loss:  0.2012839913368225\n",
            "epoch:  77  loss:  0.2007664442062378\n",
            "epoch:  78  loss:  0.20025119185447693\n",
            "epoch:  79  loss:  0.19973818957805634\n",
            "epoch:  80  loss:  0.19922742247581482\n",
            "epoch:  81  loss:  0.1987188756465912\n",
            "epoch:  82  loss:  0.19821251928806305\n",
            "epoch:  83  loss:  0.1977083683013916\n",
            "epoch:  84  loss:  0.19720639288425446\n",
            "epoch:  85  loss:  0.1967065930366516\n",
            "epoch:  86  loss:  0.1962089240550995\n",
            "epoch:  87  loss:  0.19571341574192047\n",
            "epoch:  88  loss:  0.19522003829479218\n",
            "epoch:  89  loss:  0.1947287619113922\n",
            "epoch:  90  loss:  0.1942395716905594\n",
            "epoch:  91  loss:  0.1937524974346161\n",
            "epoch:  92  loss:  0.19326747953891754\n",
            "epoch:  93  loss:  0.19278451800346375\n",
            "epoch:  94  loss:  0.1923036128282547\n",
            "epoch:  95  loss:  0.1918247491121292\n",
            "epoch:  96  loss:  0.1913479119539261\n",
            "epoch:  97  loss:  0.19087310135364532\n",
            "epoch:  98  loss:  0.19040028750896454\n",
            "epoch:  99  loss:  0.18992947041988373\n",
            "epoch:  100  loss:  0.1894606351852417\n",
            "epoch:  101  loss:  0.18899375200271606\n",
            "epoch:  102  loss:  0.18852882087230682\n",
            "epoch:  103  loss:  0.18806585669517517\n",
            "epoch:  104  loss:  0.18760481476783752\n",
            "epoch:  105  loss:  0.18714572489261627\n",
            "epoch:  106  loss:  0.18668852746486664\n",
            "epoch:  107  loss:  0.1862332820892334\n",
            "epoch:  108  loss:  0.1857798546552658\n",
            "epoch:  109  loss:  0.18532833456993103\n",
            "epoch:  110  loss:  0.18487872183322906\n",
            "epoch:  111  loss:  0.18443097174167633\n",
            "epoch:  112  loss:  0.18398502469062805\n",
            "epoch:  113  loss:  0.1835409551858902\n",
            "epoch:  114  loss:  0.1830987185239792\n",
            "epoch:  115  loss:  0.1826583296060562\n",
            "epoch:  116  loss:  0.1822197288274765\n",
            "epoch:  117  loss:  0.18178294599056244\n",
            "epoch:  118  loss:  0.18134798109531403\n",
            "epoch:  119  loss:  0.18091478943824768\n",
            "epoch:  120  loss:  0.1804833710193634\n",
            "epoch:  121  loss:  0.1800537258386612\n",
            "epoch:  122  loss:  0.17962586879730225\n",
            "epoch:  123  loss:  0.17919974029064178\n",
            "epoch:  124  loss:  0.178775355219841\n",
            "epoch:  125  loss:  0.1783527433872223\n",
            "epoch:  126  loss:  0.17793181538581848\n",
            "epoch:  127  loss:  0.17751263082027435\n",
            "epoch:  128  loss:  0.1770951896905899\n",
            "epoch:  129  loss:  0.17667940258979797\n",
            "epoch:  130  loss:  0.17626532912254333\n",
            "epoch:  131  loss:  0.175852969288826\n",
            "epoch:  132  loss:  0.17544226348400116\n",
            "epoch:  133  loss:  0.17503324151039124\n",
            "epoch:  134  loss:  0.17462588846683502\n",
            "epoch:  135  loss:  0.17422018945217133\n",
            "epoch:  136  loss:  0.17381614446640015\n",
            "epoch:  137  loss:  0.1734137237071991\n",
            "epoch:  138  loss:  0.17301297187805176\n",
            "epoch:  139  loss:  0.17261382937431335\n",
            "epoch:  140  loss:  0.17221631109714508\n",
            "epoch:  141  loss:  0.17182041704654694\n",
            "epoch:  142  loss:  0.17142613232135773\n",
            "epoch:  143  loss:  0.17103342711925507\n",
            "epoch:  144  loss:  0.17064233124256134\n",
            "epoch:  145  loss:  0.17025281488895416\n",
            "epoch:  146  loss:  0.16986489295959473\n",
            "epoch:  147  loss:  0.16947853565216064\n",
            "epoch:  148  loss:  0.1690937578678131\n",
            "epoch:  149  loss:  0.16871054470539093\n",
            "epoch:  150  loss:  0.16832885146141052\n",
            "epoch:  151  loss:  0.16794873774051666\n",
            "epoch:  152  loss:  0.16757015883922577\n",
            "epoch:  153  loss:  0.16719311475753784\n",
            "epoch:  154  loss:  0.16681762039661407\n",
            "epoch:  155  loss:  0.1664436310529709\n",
            "epoch:  156  loss:  0.16607114672660828\n",
            "epoch:  157  loss:  0.16570021212100983\n",
            "epoch:  158  loss:  0.16533075273036957\n",
            "epoch:  159  loss:  0.1649627983570099\n",
            "epoch:  160  loss:  0.1645963340997696\n",
            "epoch:  161  loss:  0.16423137485980988\n",
            "epoch:  162  loss:  0.16386787593364716\n",
            "epoch:  163  loss:  0.16350588202476501\n",
            "epoch:  164  loss:  0.16314533352851868\n",
            "epoch:  165  loss:  0.16278627514839172\n",
            "epoch:  166  loss:  0.16242864727973938\n",
            "epoch:  167  loss:  0.16207250952720642\n",
            "epoch:  168  loss:  0.16171780228614807\n",
            "epoch:  169  loss:  0.1613645702600479\n",
            "epoch:  170  loss:  0.16101275384426117\n",
            "epoch:  171  loss:  0.16066235303878784\n",
            "epoch:  172  loss:  0.16031338274478912\n",
            "epoch:  173  loss:  0.1599658578634262\n",
            "epoch:  174  loss:  0.1596197634935379\n",
            "epoch:  175  loss:  0.15927504003047943\n",
            "epoch:  176  loss:  0.15893174707889557\n",
            "epoch:  177  loss:  0.15858985483646393\n",
            "epoch:  178  loss:  0.1582493633031845\n",
            "epoch:  179  loss:  0.15791025757789612\n",
            "epoch:  180  loss:  0.15757255256175995\n",
            "epoch:  181  loss:  0.1572362333536148\n",
            "epoch:  182  loss:  0.1569012701511383\n",
            "epoch:  183  loss:  0.15656772255897522\n",
            "epoch:  184  loss:  0.1562355011701584\n",
            "epoch:  185  loss:  0.15590466558933258\n",
            "epoch:  186  loss:  0.1555752009153366\n",
            "epoch:  187  loss:  0.1552470624446869\n",
            "epoch:  188  loss:  0.154920294880867\n",
            "epoch:  189  loss:  0.15459485352039337\n",
            "epoch:  190  loss:  0.15427078306674957\n",
            "epoch:  191  loss:  0.15394802391529083\n",
            "epoch:  192  loss:  0.15362660586833954\n",
            "epoch:  193  loss:  0.1533065140247345\n",
            "epoch:  194  loss:  0.1529877483844757\n",
            "epoch:  195  loss:  0.15267030894756317\n",
            "epoch:  196  loss:  0.1523541659116745\n",
            "epoch:  197  loss:  0.15203934907913208\n",
            "epoch:  198  loss:  0.15172582864761353\n",
            "epoch:  199  loss:  0.15141360461711884\n",
            "epoch:  200  loss:  0.151102676987648\n",
            "epoch:  201  loss:  0.15079306066036224\n",
            "epoch:  202  loss:  0.15048472583293915\n",
            "epoch:  203  loss:  0.15017764270305634\n",
            "epoch:  204  loss:  0.14987188577651978\n",
            "epoch:  205  loss:  0.1495673656463623\n",
            "epoch:  206  loss:  0.1492641419172287\n",
            "epoch:  207  loss:  0.14896221458911896\n",
            "epoch:  208  loss:  0.14866149425506592\n",
            "epoch:  209  loss:  0.14836207032203674\n",
            "epoch:  210  loss:  0.14806388318538666\n",
            "epoch:  211  loss:  0.14776696264743805\n",
            "epoch:  212  loss:  0.14747129380702972\n",
            "epoch:  213  loss:  0.1471768468618393\n",
            "epoch:  214  loss:  0.14688363671302795\n",
            "epoch:  215  loss:  0.1465916931629181\n",
            "epoch:  216  loss:  0.14630094170570374\n",
            "epoch:  217  loss:  0.14601145684719086\n",
            "epoch:  218  loss:  0.14572317898273468\n",
            "epoch:  219  loss:  0.1454361081123352\n",
            "epoch:  220  loss:  0.14515027403831482\n",
            "epoch:  221  loss:  0.14486566185951233\n",
            "epoch:  222  loss:  0.14458221197128296\n",
            "epoch:  223  loss:  0.14429999887943268\n",
            "epoch:  224  loss:  0.1440189927816391\n",
            "epoch:  225  loss:  0.14373917877674103\n",
            "epoch:  226  loss:  0.14346055686473846\n",
            "epoch:  227  loss:  0.1431831270456314\n",
            "epoch:  228  loss:  0.14290685951709747\n",
            "epoch:  229  loss:  0.14263179898262024\n",
            "epoch:  230  loss:  0.1423579305410385\n",
            "epoch:  231  loss:  0.1420852094888687\n",
            "epoch:  232  loss:  0.14181366562843323\n",
            "epoch:  233  loss:  0.14154328405857086\n",
            "epoch:  234  loss:  0.141274094581604\n",
            "epoch:  235  loss:  0.1410060077905655\n",
            "epoch:  236  loss:  0.14073914289474487\n",
            "epoch:  237  loss:  0.140473410487175\n",
            "epoch:  238  loss:  0.14020879566669464\n",
            "epoch:  239  loss:  0.1399453729391098\n",
            "epoch:  240  loss:  0.1396830677986145\n",
            "epoch:  241  loss:  0.13942191004753113\n",
            "epoch:  242  loss:  0.1391618698835373\n",
            "epoch:  243  loss:  0.13890299201011658\n",
            "epoch:  244  loss:  0.138645201921463\n",
            "epoch:  245  loss:  0.13838854432106018\n",
            "epoch:  246  loss:  0.13813301920890808\n",
            "epoch:  247  loss:  0.1378786265850067\n",
            "epoch:  248  loss:  0.1376253366470337\n",
            "epoch:  249  loss:  0.137373149394989\n",
            "epoch:  250  loss:  0.13712206482887268\n",
            "epoch:  251  loss:  0.1368720978498459\n",
            "epoch:  252  loss:  0.13662323355674744\n",
            "epoch:  253  loss:  0.13637547194957733\n",
            "epoch:  254  loss:  0.13612878322601318\n",
            "epoch:  255  loss:  0.13588319718837738\n",
            "epoch:  256  loss:  0.13563869893550873\n",
            "epoch:  257  loss:  0.13539528846740723\n",
            "epoch:  258  loss:  0.13515295088291168\n",
            "epoch:  259  loss:  0.1349117010831833\n",
            "epoch:  260  loss:  0.13467149436473846\n",
            "epoch:  261  loss:  0.13443240523338318\n",
            "epoch:  262  loss:  0.13419434428215027\n",
            "epoch:  263  loss:  0.1339573711156845\n",
            "epoch:  264  loss:  0.13372144103050232\n",
            "epoch:  265  loss:  0.1334865689277649\n",
            "epoch:  266  loss:  0.13325275480747223\n",
            "epoch:  267  loss:  0.13301998376846313\n",
            "epoch:  268  loss:  0.1327882707118988\n",
            "epoch:  269  loss:  0.13255757093429565\n",
            "epoch:  270  loss:  0.13232795894145966\n",
            "epoch:  271  loss:  0.13209933042526245\n",
            "epoch:  272  loss:  0.13187175989151\n",
            "epoch:  273  loss:  0.13164521753787994\n",
            "epoch:  274  loss:  0.13141968846321106\n",
            "epoch:  275  loss:  0.13119520246982574\n",
            "epoch:  276  loss:  0.1309717446565628\n",
            "epoch:  277  loss:  0.13074927031993866\n",
            "epoch:  278  loss:  0.13052783906459808\n",
            "epoch:  279  loss:  0.1303073763847351\n",
            "epoch:  280  loss:  0.1300879567861557\n",
            "epoch:  281  loss:  0.1298695057630539\n",
            "epoch:  282  loss:  0.12965208292007446\n",
            "epoch:  283  loss:  0.12943565845489502\n",
            "epoch:  284  loss:  0.12922021746635437\n",
            "epoch:  285  loss:  0.1290057748556137\n",
            "epoch:  286  loss:  0.12879228591918945\n",
            "epoch:  287  loss:  0.12857979536056519\n",
            "epoch:  288  loss:  0.1283683031797409\n",
            "epoch:  289  loss:  0.12815776467323303\n",
            "epoch:  290  loss:  0.12794820964336395\n",
            "epoch:  291  loss:  0.12773959338665009\n",
            "epoch:  292  loss:  0.127531960606575\n",
            "epoch:  293  loss:  0.12732531130313873\n",
            "epoch:  294  loss:  0.12711960077285767\n",
            "epoch:  295  loss:  0.126914843916893\n",
            "epoch:  296  loss:  0.12671104073524475\n",
            "epoch:  297  loss:  0.1265082061290741\n",
            "epoch:  298  loss:  0.12630631029605865\n",
            "epoch:  299  loss:  0.12610535323619843\n",
            "epoch:  300  loss:  0.12590532004833221\n",
            "epoch:  301  loss:  0.1257062405347824\n",
            "epoch:  302  loss:  0.12550809979438782\n",
            "epoch:  303  loss:  0.12531088292598724\n",
            "epoch:  304  loss:  0.1251145750284195\n",
            "epoch:  305  loss:  0.12491921335458755\n",
            "epoch:  306  loss:  0.12472476065158844\n",
            "epoch:  307  loss:  0.12453123182058334\n",
            "epoch:  308  loss:  0.12433860450983047\n",
            "epoch:  309  loss:  0.12414690852165222\n",
            "epoch:  310  loss:  0.1239561215043068\n",
            "epoch:  311  loss:  0.1237662136554718\n",
            "epoch:  312  loss:  0.12357722222805023\n",
            "epoch:  313  loss:  0.12338912487030029\n",
            "epoch:  314  loss:  0.12320192158222198\n",
            "epoch:  315  loss:  0.1230156272649765\n",
            "epoch:  316  loss:  0.12283018231391907\n",
            "epoch:  317  loss:  0.12264565378427505\n",
            "epoch:  318  loss:  0.12246199697256088\n",
            "epoch:  319  loss:  0.12227921932935715\n",
            "epoch:  320  loss:  0.12209732085466385\n",
            "epoch:  321  loss:  0.1219162791967392\n",
            "epoch:  322  loss:  0.12173611670732498\n",
            "epoch:  323  loss:  0.1215568259358406\n",
            "epoch:  324  loss:  0.12137837707996368\n",
            "epoch:  325  loss:  0.121200792491436\n",
            "epoch:  326  loss:  0.12102406471967697\n",
            "epoch:  327  loss:  0.12084820121526718\n",
            "epoch:  328  loss:  0.12067317217588425\n",
            "epoch:  329  loss:  0.12049897760152817\n",
            "epoch:  330  loss:  0.12032563984394073\n",
            "epoch:  331  loss:  0.12015314400196075\n",
            "epoch:  332  loss:  0.11998149007558823\n",
            "epoch:  333  loss:  0.11981064826250076\n",
            "epoch:  334  loss:  0.11964064836502075\n",
            "epoch:  335  loss:  0.1194714829325676\n",
            "epoch:  336  loss:  0.11930312216281891\n",
            "epoch:  337  loss:  0.11913558095693588\n",
            "epoch:  338  loss:  0.11896887421607971\n",
            "epoch:  339  loss:  0.1188029795885086\n",
            "epoch:  340  loss:  0.11863787472248077\n",
            "epoch:  341  loss:  0.1184736043214798\n",
            "epoch:  342  loss:  0.1183101162314415\n",
            "epoch:  343  loss:  0.11814744770526886\n",
            "epoch:  344  loss:  0.1179855614900589\n",
            "epoch:  345  loss:  0.11782447248697281\n",
            "epoch:  346  loss:  0.1176641583442688\n",
            "epoch:  347  loss:  0.11750467121601105\n",
            "epoch:  348  loss:  0.11734593659639359\n",
            "epoch:  349  loss:  0.1171879842877388\n",
            "epoch:  350  loss:  0.11703082174062729\n",
            "epoch:  351  loss:  0.11687443405389786\n",
            "epoch:  352  loss:  0.11671880632638931\n",
            "epoch:  353  loss:  0.11656394600868225\n",
            "epoch:  354  loss:  0.11640987545251846\n",
            "epoch:  355  loss:  0.11625654250383377\n",
            "epoch:  356  loss:  0.11610397696495056\n",
            "epoch:  357  loss:  0.11595217138528824\n",
            "epoch:  358  loss:  0.1158011183142662\n",
            "epoch:  359  loss:  0.11565080285072327\n",
            "epoch:  360  loss:  0.11550123989582062\n",
            "epoch:  361  loss:  0.11535242199897766\n",
            "epoch:  362  loss:  0.1152043491601944\n",
            "epoch:  363  loss:  0.11505699902772903\n",
            "epoch:  364  loss:  0.11491038650274277\n",
            "epoch:  365  loss:  0.11476451903581619\n",
            "epoch:  366  loss:  0.11461936682462692\n",
            "epoch:  367  loss:  0.11447494477033615\n",
            "epoch:  368  loss:  0.11433123797178268\n",
            "epoch:  369  loss:  0.11418826133012772\n",
            "epoch:  370  loss:  0.11404598504304886\n",
            "epoch:  371  loss:  0.1139044240117073\n",
            "epoch:  372  loss:  0.11376356333494186\n",
            "epoch:  373  loss:  0.11362341046333313\n",
            "epoch:  374  loss:  0.1134839877486229\n",
            "epoch:  375  loss:  0.11334522813558578\n",
            "epoch:  376  loss:  0.11320717632770538\n",
            "epoch:  377  loss:  0.11306982487440109\n",
            "epoch:  378  loss:  0.11293315887451172\n",
            "epoch:  379  loss:  0.11279717832803726\n",
            "epoch:  380  loss:  0.11266189068555832\n",
            "epoch:  381  loss:  0.1125272661447525\n",
            "epoch:  382  loss:  0.1123933494091034\n",
            "epoch:  383  loss:  0.11226008087396622\n",
            "epoch:  384  loss:  0.11212749779224396\n",
            "epoch:  385  loss:  0.11199557781219482\n",
            "epoch:  386  loss:  0.11186433583498001\n",
            "epoch:  387  loss:  0.11173373460769653\n",
            "epoch:  388  loss:  0.11160381138324738\n",
            "epoch:  389  loss:  0.11147453635931015\n",
            "epoch:  390  loss:  0.11134591698646545\n",
            "epoch:  391  loss:  0.11121794581413269\n",
            "epoch:  392  loss:  0.11109063029289246\n",
            "epoch:  393  loss:  0.11096394062042236\n",
            "epoch:  394  loss:  0.1108379065990448\n",
            "epoch:  395  loss:  0.11071249097585678\n",
            "epoch:  396  loss:  0.11058773845434189\n",
            "epoch:  397  loss:  0.11046359688043594\n",
            "epoch:  398  loss:  0.11034010350704193\n",
            "epoch:  399  loss:  0.11021722108125687\n",
            "epoch:  400  loss:  0.11009496450424194\n",
            "epoch:  401  loss:  0.10997333377599716\n",
            "epoch:  402  loss:  0.10985230654478073\n",
            "epoch:  403  loss:  0.10973192006349564\n",
            "epoch:  404  loss:  0.1096121221780777\n",
            "epoch:  405  loss:  0.10949293524026871\n",
            "epoch:  406  loss:  0.10937435925006866\n",
            "epoch:  407  loss:  0.10925637930631638\n",
            "epoch:  408  loss:  0.10913901031017303\n",
            "epoch:  409  loss:  0.10902222245931625\n",
            "epoch:  410  loss:  0.10890603810548782\n",
            "epoch:  411  loss:  0.10879044979810715\n",
            "epoch:  412  loss:  0.10867542773485184\n",
            "epoch:  413  loss:  0.10856100171804428\n",
            "epoch:  414  loss:  0.10844717174768448\n",
            "epoch:  415  loss:  0.10833390802145004\n",
            "epoch:  416  loss:  0.10822122544050217\n",
            "epoch:  417  loss:  0.10810911655426025\n",
            "epoch:  418  loss:  0.10799757391214371\n",
            "epoch:  419  loss:  0.10788659751415253\n",
            "epoch:  420  loss:  0.10777617245912552\n",
            "epoch:  421  loss:  0.10766632854938507\n",
            "epoch:  422  loss:  0.1075570359826088\n",
            "epoch:  423  loss:  0.10744830220937729\n",
            "epoch:  424  loss:  0.10734011977910995\n",
            "epoch:  425  loss:  0.1072324886918068\n",
            "epoch:  426  loss:  0.10712539404630661\n",
            "epoch:  427  loss:  0.1070188581943512\n",
            "epoch:  428  loss:  0.10691286623477936\n",
            "epoch:  429  loss:  0.1068074032664299\n",
            "epoch:  430  loss:  0.10670248419046402\n",
            "epoch:  431  loss:  0.10659808665513992\n",
            "epoch:  432  loss:  0.1064942330121994\n",
            "epoch:  433  loss:  0.10639090090990067\n",
            "epoch:  434  loss:  0.10628809779882431\n",
            "epoch:  435  loss:  0.10618582367897034\n",
            "epoch:  436  loss:  0.10608405619859695\n",
            "epoch:  437  loss:  0.10598280280828476\n",
            "epoch:  438  loss:  0.10588207095861435\n",
            "epoch:  439  loss:  0.10578186810016632\n",
            "epoch:  440  loss:  0.10568214952945709\n",
            "epoch:  441  loss:  0.10558294504880905\n",
            "epoch:  442  loss:  0.1054842546582222\n",
            "epoch:  443  loss:  0.10538604855537415\n",
            "epoch:  444  loss:  0.10528835654258728\n",
            "epoch:  445  loss:  0.10519114881753922\n",
            "epoch:  446  loss:  0.10509444773197174\n",
            "epoch:  447  loss:  0.10499821603298187\n",
            "epoch:  448  loss:  0.1049024760723114\n",
            "epoch:  449  loss:  0.10480724275112152\n",
            "epoch:  450  loss:  0.10471247881650925\n",
            "epoch:  451  loss:  0.10461817681789398\n",
            "epoch:  452  loss:  0.10452436655759811\n",
            "epoch:  453  loss:  0.10443104058504105\n",
            "epoch:  454  loss:  0.1043381616473198\n",
            "epoch:  455  loss:  0.10424578189849854\n",
            "epoch:  456  loss:  0.10415385663509369\n",
            "epoch:  457  loss:  0.10406239330768585\n",
            "epoch:  458  loss:  0.10397138446569443\n",
            "epoch:  459  loss:  0.10388083755970001\n",
            "epoch:  460  loss:  0.1037907525897026\n",
            "epoch:  461  loss:  0.10370112210512161\n",
            "epoch:  462  loss:  0.10361193865537643\n",
            "epoch:  463  loss:  0.10352320224046707\n",
            "epoch:  464  loss:  0.10343490540981293\n",
            "epoch:  465  loss:  0.10334707796573639\n",
            "epoch:  466  loss:  0.10325967520475388\n",
            "epoch:  467  loss:  0.10317270457744598\n",
            "epoch:  468  loss:  0.1030861884355545\n",
            "epoch:  469  loss:  0.10300008952617645\n",
            "epoch:  470  loss:  0.10291444510221481\n",
            "epoch:  471  loss:  0.10282920300960541\n",
            "epoch:  472  loss:  0.10274440050125122\n",
            "epoch:  473  loss:  0.10266001522541046\n",
            "epoch:  474  loss:  0.10257606208324432\n",
            "epoch:  475  loss:  0.10249252617359161\n",
            "epoch:  476  loss:  0.10240939259529114\n",
            "epoch:  477  loss:  0.10232669860124588\n",
            "epoch:  478  loss:  0.10224439203739166\n",
            "epoch:  479  loss:  0.10216250270605087\n",
            "epoch:  480  loss:  0.10208103805780411\n",
            "epoch:  481  loss:  0.10199995338916779\n",
            "epoch:  482  loss:  0.1019192710518837\n",
            "epoch:  483  loss:  0.10183900594711304\n",
            "epoch:  484  loss:  0.10175912827253342\n",
            "epoch:  485  loss:  0.10167963802814484\n",
            "epoch:  486  loss:  0.10160055756568909\n",
            "epoch:  487  loss:  0.10152186453342438\n",
            "epoch:  488  loss:  0.10144355893135071\n",
            "epoch:  489  loss:  0.10136563330888748\n",
            "epoch:  490  loss:  0.10128810256719589\n",
            "epoch:  491  loss:  0.10121094435453415\n",
            "epoch:  492  loss:  0.10113417357206345\n",
            "epoch:  493  loss:  0.10105777531862259\n",
            "epoch:  494  loss:  0.10098174959421158\n",
            "epoch:  495  loss:  0.10090609639883041\n",
            "epoch:  496  loss:  0.1008308082818985\n",
            "epoch:  497  loss:  0.10075590759515762\n",
            "epoch:  498  loss:  0.1006813570857048\n",
            "epoch:  499  loss:  0.10060717910528183\n",
            "epoch:  500  loss:  0.10053335875272751\n",
            "epoch:  501  loss:  0.10045988857746124\n",
            "epoch:  502  loss:  0.10038679093122482\n",
            "epoch:  503  loss:  0.10031404346227646\n",
            "epoch:  504  loss:  0.10024164617061615\n",
            "epoch:  505  loss:  0.1001695990562439\n",
            "epoch:  506  loss:  0.1000979021191597\n",
            "epoch:  507  loss:  0.10002655535936356\n",
            "epoch:  508  loss:  0.09995554387569427\n",
            "epoch:  509  loss:  0.09988489001989365\n",
            "epoch:  510  loss:  0.09981455653905869\n",
            "epoch:  511  loss:  0.09974457323551178\n",
            "epoch:  512  loss:  0.09967492520809174\n",
            "epoch:  513  loss:  0.09960561245679855\n",
            "epoch:  514  loss:  0.09953662753105164\n",
            "epoch:  515  loss:  0.09946797043085098\n",
            "epoch:  516  loss:  0.0993996411561966\n",
            "epoch:  517  loss:  0.09933164715766907\n",
            "epoch:  518  loss:  0.0992639809846878\n",
            "epoch:  519  loss:  0.09919662773609161\n",
            "epoch:  520  loss:  0.0991295874118805\n",
            "epoch:  521  loss:  0.09906287491321564\n",
            "epoch:  522  loss:  0.09899647533893585\n",
            "epoch:  523  loss:  0.09893039613962173\n",
            "epoch:  524  loss:  0.09886462241411209\n",
            "epoch:  525  loss:  0.09879915416240692\n",
            "epoch:  526  loss:  0.09873399883508682\n",
            "epoch:  527  loss:  0.0986691489815712\n",
            "epoch:  528  loss:  0.09860461205244064\n",
            "epoch:  529  loss:  0.09854035824537277\n",
            "epoch:  530  loss:  0.09847642481327057\n",
            "epoch:  531  loss:  0.09841278195381165\n",
            "epoch:  532  loss:  0.0983494445681572\n",
            "epoch:  533  loss:  0.09828639030456543\n",
            "epoch:  534  loss:  0.09822363406419754\n",
            "epoch:  535  loss:  0.09816116094589233\n",
            "epoch:  536  loss:  0.0980989933013916\n",
            "epoch:  537  loss:  0.09803709387779236\n",
            "epoch:  538  loss:  0.09797549992799759\n",
            "epoch:  539  loss:  0.09791417419910431\n",
            "epoch:  540  loss:  0.09785313904285431\n",
            "epoch:  541  loss:  0.0977923721075058\n",
            "epoch:  542  loss:  0.09773189574480057\n",
            "epoch:  543  loss:  0.09767169505357742\n",
            "epoch:  544  loss:  0.09761175513267517\n",
            "epoch:  545  loss:  0.0975521057844162\n",
            "epoch:  546  loss:  0.09749272465705872\n",
            "epoch:  547  loss:  0.09743359684944153\n",
            "epoch:  548  loss:  0.09737475216388702\n",
            "epoch:  549  loss:  0.09731616824865341\n",
            "epoch:  550  loss:  0.09725785255432129\n",
            "epoch:  551  loss:  0.09719979763031006\n",
            "epoch:  552  loss:  0.09714199602603912\n",
            "epoch:  553  loss:  0.09708446264266968\n",
            "epoch:  554  loss:  0.09702719002962112\n",
            "epoch:  555  loss:  0.09697017073631287\n",
            "epoch:  556  loss:  0.0969133973121643\n",
            "epoch:  557  loss:  0.09685688465833664\n",
            "epoch:  558  loss:  0.09680061787366867\n",
            "epoch:  559  loss:  0.0967445969581604\n",
            "epoch:  560  loss:  0.09668883681297302\n",
            "epoch:  561  loss:  0.09663331508636475\n",
            "epoch:  562  loss:  0.09657803177833557\n",
            "epoch:  563  loss:  0.0965229943394661\n",
            "epoch:  564  loss:  0.09646820276975632\n",
            "epoch:  565  loss:  0.09641364961862564\n",
            "epoch:  566  loss:  0.09635934233665466\n",
            "epoch:  567  loss:  0.096305251121521\n",
            "epoch:  568  loss:  0.09625142067670822\n",
            "epoch:  569  loss:  0.09619780629873276\n",
            "epoch:  570  loss:  0.0961444303393364\n",
            "epoch:  571  loss:  0.09609128534793854\n",
            "epoch:  572  loss:  0.09603837132453918\n",
            "epoch:  573  loss:  0.09598567336797714\n",
            "epoch:  574  loss:  0.0959332063794136\n",
            "epoch:  575  loss:  0.09588096290826797\n",
            "epoch:  576  loss:  0.09582895785570145\n",
            "epoch:  577  loss:  0.09577716141939163\n",
            "epoch:  578  loss:  0.09572558104991913\n",
            "epoch:  579  loss:  0.09567422419786453\n",
            "epoch:  580  loss:  0.09562308341264725\n",
            "epoch:  581  loss:  0.09557215124368668\n",
            "epoch:  582  loss:  0.09552144259214401\n",
            "epoch:  583  loss:  0.09547095000743866\n",
            "epoch:  584  loss:  0.09542066603899002\n",
            "epoch:  585  loss:  0.0953705832362175\n",
            "epoch:  586  loss:  0.09532071650028229\n",
            "epoch:  587  loss:  0.0952710509300232\n",
            "epoch:  588  loss:  0.09522160142660141\n",
            "epoch:  589  loss:  0.09517234563827515\n",
            "epoch:  590  loss:  0.0951232984662056\n",
            "epoch:  591  loss:  0.09507445245981216\n",
            "epoch:  592  loss:  0.09502580016851425\n",
            "epoch:  593  loss:  0.09497735649347305\n",
            "epoch:  594  loss:  0.09492911398410797\n",
            "epoch:  595  loss:  0.09488106518983841\n",
            "epoch:  596  loss:  0.09483318775892258\n",
            "epoch:  597  loss:  0.09478552639484406\n",
            "epoch:  598  loss:  0.09473805874586105\n",
            "epoch:  599  loss:  0.09469078481197357\n",
            "epoch:  600  loss:  0.09464368969202042\n",
            "epoch:  601  loss:  0.09459678828716278\n",
            "epoch:  602  loss:  0.09455007314682007\n",
            "epoch:  603  loss:  0.09450353682041168\n",
            "epoch:  604  loss:  0.09445719420909882\n",
            "epoch:  605  loss:  0.09441103041172028\n",
            "epoch:  606  loss:  0.09436505287885666\n",
            "epoch:  607  loss:  0.09431926161050797\n",
            "epoch:  608  loss:  0.094273641705513\n",
            "epoch:  609  loss:  0.09422820806503296\n",
            "epoch:  610  loss:  0.09418295323848724\n",
            "epoch:  611  loss:  0.09413786232471466\n",
            "epoch:  612  loss:  0.0940929502248764\n",
            "epoch:  613  loss:  0.09404821693897247\n",
            "epoch:  614  loss:  0.09400364756584167\n",
            "epoch:  615  loss:  0.0939592570066452\n",
            "epoch:  616  loss:  0.09391503781080246\n",
            "epoch:  617  loss:  0.09387098252773285\n",
            "epoch:  618  loss:  0.09382711350917816\n",
            "epoch:  619  loss:  0.09378337860107422\n",
            "epoch:  620  loss:  0.0937398299574852\n",
            "epoch:  621  loss:  0.09369644522666931\n",
            "epoch:  622  loss:  0.09365322440862656\n",
            "epoch:  623  loss:  0.09361015260219574\n",
            "epoch:  624  loss:  0.09356724470853806\n",
            "epoch:  625  loss:  0.0935245081782341\n",
            "epoch:  626  loss:  0.09348192065954208\n",
            "epoch:  627  loss:  0.0934394970536232\n",
            "epoch:  628  loss:  0.09339722990989685\n",
            "epoch:  629  loss:  0.09335511922836304\n",
            "epoch:  630  loss:  0.09331317245960236\n",
            "epoch:  631  loss:  0.09327135235071182\n",
            "epoch:  632  loss:  0.09322971105575562\n",
            "epoch:  633  loss:  0.09318821132183075\n",
            "epoch:  634  loss:  0.09314686805009842\n",
            "epoch:  635  loss:  0.09310566633939743\n",
            "epoch:  636  loss:  0.09306462109088898\n",
            "epoch:  637  loss:  0.09302371740341187\n",
            "epoch:  638  loss:  0.0929829552769661\n",
            "epoch:  639  loss:  0.09294234216213226\n",
            "epoch:  640  loss:  0.09290188550949097\n",
            "epoch:  641  loss:  0.09286156296730042\n",
            "epoch:  642  loss:  0.0928213819861412\n",
            "epoch:  643  loss:  0.09278134256601334\n",
            "epoch:  644  loss:  0.09274143725633621\n",
            "epoch:  645  loss:  0.09270168840885162\n",
            "epoch:  646  loss:  0.09266206622123718\n",
            "epoch:  647  loss:  0.09262258559465408\n",
            "epoch:  648  loss:  0.09258324652910233\n",
            "epoch:  649  loss:  0.09254402667284012\n",
            "epoch:  650  loss:  0.09250496327877045\n",
            "epoch:  651  loss:  0.09246601164340973\n",
            "epoch:  652  loss:  0.09242720901966095\n",
            "epoch:  653  loss:  0.09238853305578232\n",
            "epoch:  654  loss:  0.09234999120235443\n",
            "epoch:  655  loss:  0.09231159090995789\n",
            "epoch:  656  loss:  0.0922733023762703\n",
            "epoch:  657  loss:  0.09223514795303345\n",
            "epoch:  658  loss:  0.09219712018966675\n",
            "epoch:  659  loss:  0.0921592265367508\n",
            "epoch:  660  loss:  0.09212145209312439\n",
            "epoch:  661  loss:  0.09208380430936813\n",
            "epoch:  662  loss:  0.09204628318548203\n",
            "epoch:  663  loss:  0.09200888127088547\n",
            "epoch:  664  loss:  0.09197161346673965\n",
            "epoch:  665  loss:  0.0919344574213028\n",
            "epoch:  666  loss:  0.09189741313457489\n",
            "epoch:  667  loss:  0.09186051040887833\n",
            "epoch:  668  loss:  0.09182370454072952\n",
            "epoch:  669  loss:  0.09178704768419266\n",
            "epoch:  670  loss:  0.09175048768520355\n",
            "epoch:  671  loss:  0.0917140319943428\n",
            "epoch:  672  loss:  0.0916777178645134\n",
            "epoch:  673  loss:  0.09164150804281235\n",
            "epoch:  674  loss:  0.09160542488098145\n",
            "epoch:  675  loss:  0.0915694385766983\n",
            "epoch:  676  loss:  0.09153357148170471\n",
            "epoch:  677  loss:  0.09149781614542007\n",
            "epoch:  678  loss:  0.09146217256784439\n",
            "epoch:  679  loss:  0.09142664074897766\n",
            "epoch:  680  loss:  0.09139122813940048\n",
            "epoch:  681  loss:  0.09135590493679047\n",
            "epoch:  682  loss:  0.09132070094347\n",
            "epoch:  683  loss:  0.09128560870885849\n",
            "epoch:  684  loss:  0.09125062078237534\n",
            "epoch:  685  loss:  0.09121573716402054\n",
            "epoch:  686  loss:  0.0911809653043747\n",
            "epoch:  687  loss:  0.09114628285169601\n",
            "epoch:  688  loss:  0.09111172705888748\n",
            "epoch:  689  loss:  0.09107725322246552\n",
            "epoch:  690  loss:  0.0910428985953331\n",
            "epoch:  691  loss:  0.09100864082574844\n",
            "epoch:  692  loss:  0.09097447246313095\n",
            "epoch:  693  loss:  0.0909404307603836\n",
            "epoch:  694  loss:  0.09090647846460342\n",
            "epoch:  695  loss:  0.0908726155757904\n",
            "epoch:  696  loss:  0.09083886444568634\n",
            "epoch:  697  loss:  0.09080518782138824\n",
            "epoch:  698  loss:  0.0907716378569603\n",
            "epoch:  699  loss:  0.09073816239833832\n",
            "epoch:  700  loss:  0.09070480614900589\n",
            "epoch:  701  loss:  0.09067153185606003\n",
            "epoch:  702  loss:  0.09063835442066193\n",
            "epoch:  703  loss:  0.09060527384281158\n",
            "epoch:  704  loss:  0.09057227522134781\n",
            "epoch:  705  loss:  0.09053938090801239\n",
            "epoch:  706  loss:  0.09050657600164413\n",
            "epoch:  707  loss:  0.09047386795282364\n",
            "epoch:  708  loss:  0.09044124186038971\n",
            "epoch:  709  loss:  0.09040870517492294\n",
            "epoch:  710  loss:  0.09037626534700394\n",
            "epoch:  711  loss:  0.0903439149260521\n",
            "epoch:  712  loss:  0.09031166136264801\n",
            "epoch:  713  loss:  0.0902794823050499\n",
            "epoch:  714  loss:  0.09024739265441895\n",
            "epoch:  715  loss:  0.09021539241075516\n",
            "epoch:  716  loss:  0.09018347412347794\n",
            "epoch:  717  loss:  0.09015166014432907\n",
            "epoch:  718  loss:  0.09011992067098618\n",
            "epoch:  719  loss:  0.09008825570344925\n",
            "epoch:  720  loss:  0.09005668014287949\n",
            "epoch:  721  loss:  0.09002520143985748\n",
            "epoch:  722  loss:  0.08999378979206085\n",
            "epoch:  723  loss:  0.08996245265007019\n",
            "epoch:  724  loss:  0.08993122726678848\n",
            "epoch:  725  loss:  0.08990006148815155\n",
            "epoch:  726  loss:  0.08986897766590118\n",
            "epoch:  727  loss:  0.08983798325061798\n",
            "epoch:  728  loss:  0.08980707824230194\n",
            "epoch:  729  loss:  0.08977623283863068\n",
            "epoch:  730  loss:  0.08974547684192657\n",
            "epoch:  731  loss:  0.08971479535102844\n",
            "epoch:  732  loss:  0.08968418836593628\n",
            "epoch:  733  loss:  0.08965366333723068\n",
            "epoch:  734  loss:  0.08962321281433105\n",
            "epoch:  735  loss:  0.089592844247818\n",
            "epoch:  736  loss:  0.0895625427365303\n",
            "epoch:  737  loss:  0.08953232318162918\n",
            "epoch:  738  loss:  0.08950218558311462\n",
            "epoch:  739  loss:  0.08947210758924484\n",
            "epoch:  740  loss:  0.08944210410118103\n",
            "epoch:  741  loss:  0.08941217511892319\n",
            "epoch:  742  loss:  0.08938232064247131\n",
            "epoch:  743  loss:  0.0893525555729866\n",
            "epoch:  744  loss:  0.08932283520698547\n",
            "epoch:  745  loss:  0.08929320424795151\n",
            "epoch:  746  loss:  0.08926364034414291\n",
            "epoch:  747  loss:  0.0892341360449791\n",
            "epoch:  748  loss:  0.08920472115278244\n",
            "epoch:  749  loss:  0.08917535841464996\n",
            "epoch:  750  loss:  0.08914607763290405\n",
            "epoch:  751  loss:  0.08911684900522232\n",
            "epoch:  752  loss:  0.08908770233392715\n",
            "epoch:  753  loss:  0.08905862271785736\n",
            "epoch:  754  loss:  0.08902961015701294\n",
            "epoch:  755  loss:  0.08900067210197449\n",
            "epoch:  756  loss:  0.08897180110216141\n",
            "epoch:  757  loss:  0.08894297480583191\n",
            "epoch:  758  loss:  0.08891423046588898\n",
            "epoch:  759  loss:  0.08888555318117142\n",
            "epoch:  760  loss:  0.08885692805051804\n",
            "epoch:  761  loss:  0.08882838487625122\n",
            "epoch:  762  loss:  0.08879990130662918\n",
            "epoch:  763  loss:  0.08877147734165192\n",
            "epoch:  764  loss:  0.08874311298131943\n",
            "epoch:  765  loss:  0.08871480822563171\n",
            "epoch:  766  loss:  0.08868658542633057\n",
            "epoch:  767  loss:  0.088658407330513\n",
            "epoch:  768  loss:  0.0886302962899208\n",
            "epoch:  769  loss:  0.08860225230455399\n",
            "epoch:  770  loss:  0.08857426047325134\n",
            "epoch:  771  loss:  0.08854633569717407\n",
            "epoch:  772  loss:  0.08851846307516098\n",
            "epoch:  773  loss:  0.08849065750837326\n",
            "epoch:  774  loss:  0.08846291154623032\n",
            "epoch:  775  loss:  0.08843522518873215\n",
            "epoch:  776  loss:  0.08840759843587875\n",
            "epoch:  777  loss:  0.08838003128767014\n",
            "epoch:  778  loss:  0.0883525162935257\n",
            "epoch:  779  loss:  0.08832506835460663\n",
            "epoch:  780  loss:  0.08829766511917114\n",
            "epoch:  781  loss:  0.08827032893896103\n",
            "epoch:  782  loss:  0.08824305236339569\n",
            "epoch:  783  loss:  0.08821581304073334\n",
            "epoch:  784  loss:  0.08818866312503815\n",
            "epoch:  785  loss:  0.08816153556108475\n",
            "epoch:  786  loss:  0.08813447505235672\n",
            "epoch:  787  loss:  0.08810748159885406\n",
            "epoch:  788  loss:  0.0880805253982544\n",
            "epoch:  789  loss:  0.0880536288022995\n",
            "epoch:  790  loss:  0.08802679181098938\n",
            "epoch:  791  loss:  0.08800000697374344\n",
            "epoch:  792  loss:  0.08797328174114227\n",
            "epoch:  793  loss:  0.08794659376144409\n",
            "epoch:  794  loss:  0.08791995793581009\n",
            "epoch:  795  loss:  0.08789340406656265\n",
            "epoch:  796  loss:  0.087866872549057\n",
            "epoch:  797  loss:  0.08784040808677673\n",
            "epoch:  798  loss:  0.08781398832798004\n",
            "epoch:  799  loss:  0.08778762072324753\n",
            "epoch:  800  loss:  0.08776131272315979\n",
            "epoch:  801  loss:  0.08773505687713623\n",
            "epoch:  802  loss:  0.08770883083343506\n",
            "epoch:  803  loss:  0.08768267184495926\n",
            "epoch:  804  loss:  0.08765655010938644\n",
            "epoch:  805  loss:  0.0876304879784584\n",
            "epoch:  806  loss:  0.08760447055101395\n",
            "epoch:  807  loss:  0.08757849782705307\n",
            "epoch:  808  loss:  0.08755259215831757\n",
            "epoch:  809  loss:  0.08752671629190445\n",
            "epoch:  810  loss:  0.08750090003013611\n",
            "epoch:  811  loss:  0.08747512847185135\n",
            "epoch:  812  loss:  0.08744940161705017\n",
            "epoch:  813  loss:  0.08742371946573257\n",
            "epoch:  814  loss:  0.08739809691905975\n",
            "epoch:  815  loss:  0.08737250417470932\n",
            "epoch:  816  loss:  0.08734697103500366\n",
            "epoch:  817  loss:  0.08732147514820099\n",
            "epoch:  818  loss:  0.08729604631662369\n",
            "epoch:  819  loss:  0.08727063983678818\n",
            "epoch:  820  loss:  0.08724529296159744\n",
            "epoch:  821  loss:  0.0872199609875679\n",
            "epoch:  822  loss:  0.08719471842050552\n",
            "epoch:  823  loss:  0.08716950565576553\n",
            "epoch:  824  loss:  0.08714433014392853\n",
            "epoch:  825  loss:  0.08711918443441391\n",
            "epoch:  826  loss:  0.08709410578012466\n",
            "epoch:  827  loss:  0.0870690643787384\n",
            "epoch:  828  loss:  0.08704407513141632\n",
            "epoch:  829  loss:  0.08701910078525543\n",
            "epoch:  830  loss:  0.08699420094490051\n",
            "epoch:  831  loss:  0.08696933090686798\n",
            "epoch:  832  loss:  0.08694450557231903\n",
            "epoch:  833  loss:  0.08691972494125366\n",
            "epoch:  834  loss:  0.08689498156309128\n",
            "epoch:  835  loss:  0.08687026798725128\n",
            "epoch:  836  loss:  0.08684562146663666\n",
            "epoch:  837  loss:  0.08682099729776382\n",
            "epoch:  838  loss:  0.08679642528295517\n",
            "epoch:  839  loss:  0.0867718979716301\n",
            "epoch:  840  loss:  0.08674740046262741\n",
            "epoch:  841  loss:  0.0867229476571083\n",
            "epoch:  842  loss:  0.08669853210449219\n",
            "epoch:  843  loss:  0.08667415380477905\n",
            "epoch:  844  loss:  0.0866498202085495\n",
            "epoch:  845  loss:  0.08662552386522293\n",
            "epoch:  846  loss:  0.08660127222537994\n",
            "epoch:  847  loss:  0.08657706528902054\n",
            "epoch:  848  loss:  0.08655288815498352\n",
            "epoch:  849  loss:  0.08652874827384949\n",
            "epoch:  850  loss:  0.08650465309619904\n",
            "epoch:  851  loss:  0.08648058772087097\n",
            "epoch:  852  loss:  0.08645657449960709\n",
            "epoch:  853  loss:  0.08643259108066559\n",
            "epoch:  854  loss:  0.08640863746404648\n",
            "epoch:  855  loss:  0.08638474345207214\n",
            "epoch:  856  loss:  0.0863608717918396\n",
            "epoch:  857  loss:  0.08633704483509064\n",
            "epoch:  858  loss:  0.08631324023008347\n",
            "epoch:  859  loss:  0.08628948032855988\n",
            "epoch:  860  loss:  0.08626576513051987\n",
            "epoch:  861  loss:  0.08624207228422165\n",
            "epoch:  862  loss:  0.0862184390425682\n",
            "epoch:  863  loss:  0.08619482070207596\n",
            "epoch:  864  loss:  0.0861712396144867\n",
            "epoch:  865  loss:  0.08614770323038101\n",
            "epoch:  866  loss:  0.08612420409917831\n",
            "epoch:  867  loss:  0.0861007422208786\n",
            "epoch:  868  loss:  0.08607731014490128\n",
            "epoch:  869  loss:  0.08605390042066574\n",
            "epoch:  870  loss:  0.08603054285049438\n",
            "epoch:  871  loss:  0.08600720763206482\n",
            "epoch:  872  loss:  0.08598391711711884\n",
            "epoch:  873  loss:  0.08596066385507584\n",
            "epoch:  874  loss:  0.08593743294477463\n",
            "epoch:  875  loss:  0.085914246737957\n",
            "epoch:  876  loss:  0.08589109033346176\n",
            "epoch:  877  loss:  0.0858679711818695\n",
            "epoch:  878  loss:  0.08584488183259964\n",
            "epoch:  879  loss:  0.08582182228565216\n",
            "epoch:  880  loss:  0.08579880744218826\n",
            "epoch:  881  loss:  0.08577581495046616\n",
            "epoch:  882  loss:  0.08575286716222763\n",
            "epoch:  883  loss:  0.08572994917631149\n",
            "epoch:  884  loss:  0.08570704609155655\n",
            "epoch:  885  loss:  0.08568419516086578\n",
            "epoch:  886  loss:  0.08566135913133621\n",
            "epoch:  887  loss:  0.08563857525587082\n",
            "epoch:  888  loss:  0.08561581373214722\n",
            "epoch:  889  loss:  0.0855930969119072\n",
            "epoch:  890  loss:  0.08557038754224777\n",
            "epoch:  891  loss:  0.08554773032665253\n",
            "epoch:  892  loss:  0.08552508801221848\n",
            "epoch:  893  loss:  0.0855024978518486\n",
            "epoch:  894  loss:  0.08547992259263992\n",
            "epoch:  895  loss:  0.08545738458633423\n",
            "epoch:  896  loss:  0.08543487638235092\n",
            "epoch:  897  loss:  0.08541239798069\n",
            "epoch:  898  loss:  0.08538996428251266\n",
            "epoch:  899  loss:  0.08536753803491592\n",
            "epoch:  900  loss:  0.08534515649080276\n",
            "epoch:  901  loss:  0.0853227972984314\n",
            "epoch:  902  loss:  0.08530047535896301\n",
            "epoch:  903  loss:  0.08527818322181702\n",
            "epoch:  904  loss:  0.085255928337574\n",
            "epoch:  905  loss:  0.08523368835449219\n",
            "epoch:  906  loss:  0.08521147817373276\n",
            "epoch:  907  loss:  0.08518930524587631\n",
            "epoch:  908  loss:  0.08516716957092285\n",
            "epoch:  909  loss:  0.08514504134654999\n",
            "epoch:  910  loss:  0.0851229578256607\n",
            "epoch:  911  loss:  0.08510089665651321\n",
            "epoch:  912  loss:  0.0850788801908493\n",
            "epoch:  913  loss:  0.08505687117576599\n",
            "epoch:  914  loss:  0.08503489941358566\n",
            "epoch:  915  loss:  0.08501296490430832\n",
            "epoch:  916  loss:  0.08499104529619217\n",
            "epoch:  917  loss:  0.08496914803981781\n",
            "epoch:  918  loss:  0.08494729548692703\n",
            "epoch:  919  loss:  0.08492547273635864\n",
            "epoch:  920  loss:  0.08490366488695145\n",
            "epoch:  921  loss:  0.08488188683986664\n",
            "epoch:  922  loss:  0.08486015349626541\n",
            "epoch:  923  loss:  0.08483843505382538\n",
            "epoch:  924  loss:  0.08481673151254654\n",
            "epoch:  925  loss:  0.08479506522417068\n",
            "epoch:  926  loss:  0.08477343618869781\n",
            "epoch:  927  loss:  0.08475182950496674\n",
            "epoch:  928  loss:  0.08473024517297745\n",
            "epoch:  929  loss:  0.08470868319272995\n",
            "epoch:  930  loss:  0.08468716591596603\n",
            "epoch:  931  loss:  0.08466566354036331\n",
            "epoch:  932  loss:  0.08464419841766357\n",
            "epoch:  933  loss:  0.08462274074554443\n",
            "epoch:  934  loss:  0.08460132032632828\n",
            "epoch:  935  loss:  0.08457992225885391\n",
            "epoch:  936  loss:  0.08455855399370193\n",
            "epoch:  937  loss:  0.08453720808029175\n",
            "epoch:  938  loss:  0.08451589196920395\n",
            "epoch:  939  loss:  0.08449461311101913\n",
            "epoch:  940  loss:  0.08447334915399551\n",
            "epoch:  941  loss:  0.08445210754871368\n",
            "epoch:  942  loss:  0.08443089574575424\n",
            "epoch:  943  loss:  0.08440970629453659\n",
            "epoch:  944  loss:  0.08438853919506073\n",
            "epoch:  945  loss:  0.08436741679906845\n",
            "epoch:  946  loss:  0.08434630185365677\n",
            "epoch:  947  loss:  0.08432520925998688\n",
            "epoch:  948  loss:  0.08430415391921997\n",
            "epoch:  949  loss:  0.08428311347961426\n",
            "epoch:  950  loss:  0.08426210284233093\n",
            "epoch:  951  loss:  0.08424112200737\n",
            "epoch:  952  loss:  0.08422014862298965\n",
            "epoch:  953  loss:  0.0841992199420929\n",
            "epoch:  954  loss:  0.08417830616235733\n",
            "epoch:  955  loss:  0.08415742218494415\n",
            "epoch:  956  loss:  0.08413656055927277\n",
            "epoch:  957  loss:  0.08411570638418198\n",
            "epoch:  958  loss:  0.08409489691257477\n",
            "epoch:  959  loss:  0.08407410234212875\n",
            "epoch:  960  loss:  0.08405334502458572\n",
            "epoch:  961  loss:  0.08403259515762329\n",
            "epoch:  962  loss:  0.08401188999414444\n",
            "epoch:  963  loss:  0.08399117738008499\n",
            "epoch:  964  loss:  0.08397050946950912\n",
            "epoch:  965  loss:  0.08394986391067505\n",
            "epoch:  966  loss:  0.08392924070358276\n",
            "epoch:  967  loss:  0.08390864729881287\n",
            "epoch:  968  loss:  0.08388806134462357\n",
            "epoch:  969  loss:  0.08386750519275665\n",
            "epoch:  970  loss:  0.08384697139263153\n",
            "epoch:  971  loss:  0.0838264673948288\n",
            "epoch:  972  loss:  0.08380598574876785\n",
            "epoch:  973  loss:  0.0837855190038681\n",
            "epoch:  974  loss:  0.08376507461071014\n",
            "epoch:  975  loss:  0.08374466747045517\n",
            "epoch:  976  loss:  0.08372427523136139\n",
            "epoch:  977  loss:  0.0837038978934288\n",
            "epoch:  978  loss:  0.0836835503578186\n",
            "epoch:  979  loss:  0.08366323262453079\n",
            "epoch:  980  loss:  0.08364291489124298\n",
            "epoch:  981  loss:  0.08362264186143875\n",
            "epoch:  982  loss:  0.08360237628221512\n",
            "epoch:  983  loss:  0.08358215540647507\n",
            "epoch:  984  loss:  0.08356193453073502\n",
            "epoch:  985  loss:  0.08354173600673676\n",
            "epoch:  986  loss:  0.08352155983448029\n",
            "epoch:  987  loss:  0.0835014134645462\n",
            "epoch:  988  loss:  0.08348128944635391\n",
            "epoch:  989  loss:  0.08346118777990341\n",
            "epoch:  990  loss:  0.08344109356403351\n",
            "epoch:  991  loss:  0.08342104405164719\n",
            "epoch:  992  loss:  0.08340100198984146\n",
            "epoch:  993  loss:  0.08338098227977753\n",
            "epoch:  994  loss:  0.08336098492145538\n",
            "epoch:  995  loss:  0.08334100246429443\n",
            "epoch:  996  loss:  0.08332104980945587\n",
            "epoch:  997  loss:  0.0833011195063591\n",
            "epoch:  998  loss:  0.08328121155500412\n",
            "epoch:  999  loss:  0.08326131850481033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSpFHCwFk10L",
        "outputId": "f0900934-e400-4124-d0bf-99d3768f345c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.547\n",
            "accuracy 0.631\n",
            "accuracy 0.674\n",
            "accuracy 0.752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, new_image_feature)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUbHavt0lP-e",
        "outputId": "d80d721c-6528-4721-dfbf-4a1a42befb3b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.08324144780635834\n",
            "epoch:  1  loss:  0.08322161436080933\n",
            "epoch:  2  loss:  0.08320177346467972\n",
            "epoch:  3  loss:  0.0831819623708725\n",
            "epoch:  4  loss:  0.08316218852996826\n",
            "epoch:  5  loss:  0.08314241468906403\n",
            "epoch:  6  loss:  0.08312266319990158\n",
            "epoch:  7  loss:  0.08310294151306152\n",
            "epoch:  8  loss:  0.08308324217796326\n",
            "epoch:  9  loss:  0.08306356519460678\n",
            "epoch:  10  loss:  0.0830438882112503\n",
            "epoch:  11  loss:  0.08302426338195801\n",
            "epoch:  12  loss:  0.08300463110208511\n",
            "epoch:  13  loss:  0.0829850286245346\n",
            "epoch:  14  loss:  0.08296544849872589\n",
            "epoch:  15  loss:  0.08294589072465897\n",
            "epoch:  16  loss:  0.08292634785175323\n",
            "epoch:  17  loss:  0.08290683478116989\n",
            "epoch:  18  loss:  0.08288732916116714\n",
            "epoch:  19  loss:  0.08286784589290619\n",
            "epoch:  20  loss:  0.08284838497638702\n",
            "epoch:  21  loss:  0.08282893896102905\n",
            "epoch:  22  loss:  0.08280952274799347\n",
            "epoch:  23  loss:  0.08279011398553848\n",
            "epoch:  24  loss:  0.08277073502540588\n",
            "epoch:  25  loss:  0.08275137096643448\n",
            "epoch:  26  loss:  0.08273203670978546\n",
            "epoch:  27  loss:  0.08271271735429764\n",
            "epoch:  28  loss:  0.08269340544939041\n",
            "epoch:  29  loss:  0.08267411589622498\n",
            "epoch:  30  loss:  0.08265486359596252\n",
            "epoch:  31  loss:  0.08263560384511948\n",
            "epoch:  32  loss:  0.08261638134717941\n",
            "epoch:  33  loss:  0.08259717375040054\n",
            "epoch:  34  loss:  0.08257798850536346\n",
            "epoch:  35  loss:  0.08255881816148758\n",
            "epoch:  36  loss:  0.08253966271877289\n",
            "epoch:  37  loss:  0.08252052962779999\n",
            "epoch:  38  loss:  0.08250142633914948\n",
            "epoch:  39  loss:  0.08248233795166016\n",
            "epoch:  40  loss:  0.08246325701475143\n",
            "epoch:  41  loss:  0.0824441984295845\n",
            "epoch:  42  loss:  0.08242516219615936\n",
            "epoch:  43  loss:  0.08240614086389542\n",
            "epoch:  44  loss:  0.08238715678453445\n",
            "epoch:  45  loss:  0.0823681652545929\n",
            "epoch:  46  loss:  0.08234920352697372\n",
            "epoch:  47  loss:  0.08233026415109634\n",
            "epoch:  48  loss:  0.08231133967638016\n",
            "epoch:  49  loss:  0.08229243010282516\n",
            "epoch:  50  loss:  0.08227354288101196\n",
            "epoch:  51  loss:  0.08225467801094055\n",
            "epoch:  52  loss:  0.08223582804203033\n",
            "epoch:  53  loss:  0.08221699297428131\n",
            "epoch:  54  loss:  0.08219817280769348\n",
            "epoch:  55  loss:  0.08217937499284744\n",
            "epoch:  56  loss:  0.0821605995297432\n",
            "epoch:  57  loss:  0.08214183151721954\n",
            "epoch:  58  loss:  0.08212309330701828\n",
            "epoch:  59  loss:  0.08210436999797821\n",
            "epoch:  60  loss:  0.08208565413951874\n",
            "epoch:  61  loss:  0.08206696808338165\n",
            "epoch:  62  loss:  0.08204829692840576\n",
            "epoch:  63  loss:  0.08202964812517166\n",
            "epoch:  64  loss:  0.08201100677251816\n",
            "epoch:  65  loss:  0.08199239522218704\n",
            "epoch:  66  loss:  0.08197378367185593\n",
            "epoch:  67  loss:  0.0819552093744278\n",
            "epoch:  68  loss:  0.08193664252758026\n",
            "epoch:  69  loss:  0.08191809803247452\n",
            "epoch:  70  loss:  0.08189956843852997\n",
            "epoch:  71  loss:  0.08188106119632721\n",
            "epoch:  72  loss:  0.08186256140470505\n",
            "epoch:  73  loss:  0.08184408396482468\n",
            "epoch:  74  loss:  0.0818256288766861\n",
            "epoch:  75  loss:  0.08180718868970871\n",
            "epoch:  76  loss:  0.08178875595331192\n",
            "epoch:  77  loss:  0.08177034556865692\n",
            "epoch:  78  loss:  0.08175195753574371\n",
            "epoch:  79  loss:  0.0817335844039917\n",
            "epoch:  80  loss:  0.08171523362398148\n",
            "epoch:  81  loss:  0.08169689774513245\n",
            "epoch:  82  loss:  0.08167856931686401\n",
            "epoch:  83  loss:  0.08166026324033737\n",
            "epoch:  84  loss:  0.08164197206497192\n",
            "epoch:  85  loss:  0.08162370324134827\n",
            "epoch:  86  loss:  0.0816054567694664\n",
            "epoch:  87  loss:  0.08158720284700394\n",
            "epoch:  88  loss:  0.08156898617744446\n",
            "epoch:  89  loss:  0.08155079185962677\n",
            "epoch:  90  loss:  0.08153259754180908\n",
            "epoch:  91  loss:  0.08151441812515259\n",
            "epoch:  92  loss:  0.08149627596139908\n",
            "epoch:  93  loss:  0.08147813379764557\n",
            "epoch:  94  loss:  0.08146002888679504\n",
            "epoch:  95  loss:  0.08144192397594452\n",
            "epoch:  96  loss:  0.08142382651567459\n",
            "epoch:  97  loss:  0.08140575885772705\n",
            "epoch:  98  loss:  0.0813877135515213\n",
            "epoch:  99  loss:  0.08136967569589615\n",
            "epoch:  100  loss:  0.08135164529085159\n",
            "epoch:  101  loss:  0.08133365213871002\n",
            "epoch:  102  loss:  0.08131566643714905\n",
            "epoch:  103  loss:  0.08129768073558807\n",
            "epoch:  104  loss:  0.08127973228693008\n",
            "epoch:  105  loss:  0.08126179873943329\n",
            "epoch:  106  loss:  0.08124387264251709\n",
            "epoch:  107  loss:  0.08122596889734268\n",
            "epoch:  108  loss:  0.08120808005332947\n",
            "epoch:  109  loss:  0.08119019865989685\n",
            "epoch:  110  loss:  0.08117233961820602\n",
            "epoch:  111  loss:  0.08115450292825699\n",
            "epoch:  112  loss:  0.08113668113946915\n",
            "epoch:  113  loss:  0.0811188668012619\n",
            "epoch:  114  loss:  0.08110107481479645\n",
            "epoch:  115  loss:  0.08108329772949219\n",
            "epoch:  116  loss:  0.08106553554534912\n",
            "epoch:  117  loss:  0.08104778826236725\n",
            "epoch:  118  loss:  0.08103006333112717\n",
            "epoch:  119  loss:  0.08101234585046768\n",
            "epoch:  120  loss:  0.08099465072154999\n",
            "epoch:  121  loss:  0.08097697049379349\n",
            "epoch:  122  loss:  0.08095929771661758\n",
            "epoch:  123  loss:  0.08094163984060287\n",
            "epoch:  124  loss:  0.08092401921749115\n",
            "epoch:  125  loss:  0.08090638369321823\n",
            "epoch:  126  loss:  0.0808887854218483\n",
            "epoch:  127  loss:  0.08087120205163956\n",
            "epoch:  128  loss:  0.08085363358259201\n",
            "epoch:  129  loss:  0.08083607256412506\n",
            "epoch:  130  loss:  0.0808185264468193\n",
            "epoch:  131  loss:  0.08080100268125534\n",
            "epoch:  132  loss:  0.08078349381685257\n",
            "epoch:  133  loss:  0.08076600730419159\n",
            "epoch:  134  loss:  0.08074851334095001\n",
            "epoch:  135  loss:  0.08073105663061142\n",
            "epoch:  136  loss:  0.08071360737085342\n",
            "epoch:  137  loss:  0.08069617301225662\n",
            "epoch:  138  loss:  0.08067875355482101\n",
            "epoch:  139  loss:  0.0806613489985466\n",
            "epoch:  140  loss:  0.08064396679401398\n",
            "epoch:  141  loss:  0.08062659204006195\n",
            "epoch:  142  loss:  0.08060923218727112\n",
            "epoch:  143  loss:  0.08059189468622208\n",
            "epoch:  144  loss:  0.08057456463575363\n",
            "epoch:  145  loss:  0.08055725693702698\n",
            "epoch:  146  loss:  0.08053995668888092\n",
            "epoch:  147  loss:  0.08052267134189606\n",
            "epoch:  148  loss:  0.08050540834665298\n",
            "epoch:  149  loss:  0.0804881677031517\n",
            "epoch:  150  loss:  0.08047091960906982\n",
            "epoch:  151  loss:  0.08045370131731033\n",
            "epoch:  152  loss:  0.08043649047613144\n",
            "epoch:  153  loss:  0.08041930943727493\n",
            "epoch:  154  loss:  0.08040212839841843\n",
            "epoch:  155  loss:  0.08038496971130371\n",
            "epoch:  156  loss:  0.08036781847476959\n",
            "epoch:  157  loss:  0.08035068958997726\n",
            "epoch:  158  loss:  0.08033357560634613\n",
            "epoch:  159  loss:  0.08031647652387619\n",
            "epoch:  160  loss:  0.08029938489198685\n",
            "epoch:  161  loss:  0.0802823081612587\n",
            "epoch:  162  loss:  0.08026526123285294\n",
            "epoch:  163  loss:  0.08024821430444717\n",
            "epoch:  164  loss:  0.0802311822772026\n",
            "epoch:  165  loss:  0.08021417260169983\n",
            "epoch:  166  loss:  0.08019717037677765\n",
            "epoch:  167  loss:  0.08018018305301666\n",
            "epoch:  168  loss:  0.08016322553157806\n",
            "epoch:  169  loss:  0.08014626055955887\n",
            "epoch:  170  loss:  0.08012931793928146\n",
            "epoch:  171  loss:  0.08011239767074585\n",
            "epoch:  172  loss:  0.08009547740221024\n",
            "epoch:  173  loss:  0.08007858693599701\n",
            "epoch:  174  loss:  0.08006169646978378\n",
            "epoch:  175  loss:  0.08004482835531235\n",
            "epoch:  176  loss:  0.0800279825925827\n",
            "epoch:  177  loss:  0.08001113682985306\n",
            "epoch:  178  loss:  0.0799943134188652\n",
            "epoch:  179  loss:  0.07997749745845795\n",
            "epoch:  180  loss:  0.07996071130037308\n",
            "epoch:  181  loss:  0.0799439325928688\n",
            "epoch:  182  loss:  0.07992715388536453\n",
            "epoch:  183  loss:  0.07991039752960205\n",
            "epoch:  184  loss:  0.07989366352558136\n",
            "epoch:  185  loss:  0.07987693697214127\n",
            "epoch:  186  loss:  0.07986023277044296\n",
            "epoch:  187  loss:  0.07984352856874466\n",
            "epoch:  188  loss:  0.07982684671878815\n",
            "epoch:  189  loss:  0.07981017231941223\n",
            "epoch:  190  loss:  0.0797935277223587\n",
            "epoch:  191  loss:  0.07977687567472458\n",
            "epoch:  192  loss:  0.07976024597883224\n",
            "epoch:  193  loss:  0.0797436460852623\n",
            "epoch:  194  loss:  0.07972703874111176\n",
            "epoch:  195  loss:  0.079710453748703\n",
            "epoch:  196  loss:  0.07969387620687485\n",
            "epoch:  197  loss:  0.07967732846736908\n",
            "epoch:  198  loss:  0.07966078817844391\n",
            "epoch:  199  loss:  0.07964425534009933\n",
            "epoch:  200  loss:  0.07962773740291595\n",
            "epoch:  201  loss:  0.07961124181747437\n",
            "epoch:  202  loss:  0.07959474623203278\n",
            "epoch:  203  loss:  0.07957828044891357\n",
            "epoch:  204  loss:  0.07956182211637497\n",
            "epoch:  205  loss:  0.07954537123441696\n",
            "epoch:  206  loss:  0.07952893525362015\n",
            "epoch:  207  loss:  0.07951252907514572\n",
            "epoch:  208  loss:  0.0794961154460907\n",
            "epoch:  209  loss:  0.07947972416877747\n",
            "epoch:  210  loss:  0.07946334779262543\n",
            "epoch:  211  loss:  0.07944698631763458\n",
            "epoch:  212  loss:  0.07943063229322433\n",
            "epoch:  213  loss:  0.07941430807113647\n",
            "epoch:  214  loss:  0.07939797639846802\n",
            "epoch:  215  loss:  0.07938167452812195\n",
            "epoch:  216  loss:  0.07936537265777588\n",
            "epoch:  217  loss:  0.0793490931391716\n",
            "epoch:  218  loss:  0.07933282107114792\n",
            "epoch:  219  loss:  0.07931656390428543\n",
            "epoch:  220  loss:  0.07930031418800354\n",
            "epoch:  221  loss:  0.07928409427404404\n",
            "epoch:  222  loss:  0.07926787436008453\n",
            "epoch:  223  loss:  0.07925167679786682\n",
            "epoch:  224  loss:  0.0792354866862297\n",
            "epoch:  225  loss:  0.07921931147575378\n",
            "epoch:  226  loss:  0.07920315116643906\n",
            "epoch:  227  loss:  0.07918700575828552\n",
            "epoch:  228  loss:  0.07917086780071259\n",
            "epoch:  229  loss:  0.07915474474430084\n",
            "epoch:  230  loss:  0.07913864403963089\n",
            "epoch:  231  loss:  0.07912255078554153\n",
            "epoch:  232  loss:  0.07910647243261337\n",
            "epoch:  233  loss:  0.07909039407968521\n",
            "epoch:  234  loss:  0.07907434552907944\n",
            "epoch:  235  loss:  0.07905829697847366\n",
            "epoch:  236  loss:  0.07904227077960968\n",
            "epoch:  237  loss:  0.0790262520313263\n",
            "epoch:  238  loss:  0.0790102556347847\n",
            "epoch:  239  loss:  0.0789942592382431\n",
            "epoch:  240  loss:  0.0789782851934433\n",
            "epoch:  241  loss:  0.07896232604980469\n",
            "epoch:  242  loss:  0.07894638180732727\n",
            "epoch:  243  loss:  0.07893044501543045\n",
            "epoch:  244  loss:  0.07891451567411423\n",
            "epoch:  245  loss:  0.0788986086845398\n",
            "epoch:  246  loss:  0.07888271659612656\n",
            "epoch:  247  loss:  0.07886683195829391\n",
            "epoch:  248  loss:  0.07885095477104187\n",
            "epoch:  249  loss:  0.07883509993553162\n",
            "epoch:  250  loss:  0.07881925255060196\n",
            "epoch:  251  loss:  0.0788034200668335\n",
            "epoch:  252  loss:  0.07878759503364563\n",
            "epoch:  253  loss:  0.07877179235219955\n",
            "epoch:  254  loss:  0.07875599712133408\n",
            "epoch:  255  loss:  0.0787402093410492\n",
            "epoch:  256  loss:  0.0787244513630867\n",
            "epoch:  257  loss:  0.07870868593454361\n",
            "epoch:  258  loss:  0.0786929503083229\n",
            "epoch:  259  loss:  0.0786772146821022\n",
            "epoch:  260  loss:  0.07866150885820389\n",
            "epoch:  261  loss:  0.07864579558372498\n",
            "epoch:  262  loss:  0.07863011211156845\n",
            "epoch:  263  loss:  0.07861442863941193\n",
            "epoch:  264  loss:  0.07859876751899719\n",
            "epoch:  265  loss:  0.07858311384916306\n",
            "epoch:  266  loss:  0.07856746762990952\n",
            "epoch:  267  loss:  0.07855185121297836\n",
            "epoch:  268  loss:  0.07853623479604721\n",
            "epoch:  269  loss:  0.07852063328027725\n",
            "epoch:  270  loss:  0.0785050317645073\n",
            "epoch:  271  loss:  0.07848947495222092\n",
            "epoch:  272  loss:  0.07847390323877335\n",
            "epoch:  273  loss:  0.07845834642648697\n",
            "epoch:  274  loss:  0.07844281196594238\n",
            "epoch:  275  loss:  0.0784272775053978\n",
            "epoch:  276  loss:  0.0784117579460144\n",
            "epoch:  277  loss:  0.0783962607383728\n",
            "epoch:  278  loss:  0.0783807784318924\n",
            "epoch:  279  loss:  0.07836530357599258\n",
            "epoch:  280  loss:  0.07834983617067337\n",
            "epoch:  281  loss:  0.07833439111709595\n",
            "epoch:  282  loss:  0.07831893861293793\n",
            "epoch:  283  loss:  0.0783035159111023\n",
            "epoch:  284  loss:  0.07828810065984726\n",
            "epoch:  285  loss:  0.07827270776033401\n",
            "epoch:  286  loss:  0.07825731486082077\n",
            "epoch:  287  loss:  0.07824193686246872\n",
            "epoch:  288  loss:  0.07822657376527786\n",
            "epoch:  289  loss:  0.0782112181186676\n",
            "epoch:  290  loss:  0.07819588482379913\n",
            "epoch:  291  loss:  0.07818055152893066\n",
            "epoch:  292  loss:  0.07816524058580399\n",
            "epoch:  293  loss:  0.07814992964267731\n",
            "epoch:  294  loss:  0.07813464105129242\n",
            "epoch:  295  loss:  0.07811937481164932\n",
            "epoch:  296  loss:  0.07810409367084503\n",
            "epoch:  297  loss:  0.07808884233236313\n",
            "epoch:  298  loss:  0.07807359844446182\n",
            "epoch:  299  loss:  0.0780583843588829\n",
            "epoch:  300  loss:  0.07804315537214279\n",
            "epoch:  301  loss:  0.07802795618772507\n",
            "epoch:  302  loss:  0.07801276445388794\n",
            "epoch:  303  loss:  0.07799757272005081\n",
            "epoch:  304  loss:  0.07798240333795547\n",
            "epoch:  305  loss:  0.07796725630760193\n",
            "epoch:  306  loss:  0.07795210927724838\n",
            "epoch:  307  loss:  0.07793698459863663\n",
            "epoch:  308  loss:  0.07792185246944427\n",
            "epoch:  309  loss:  0.07790674269199371\n",
            "epoch:  310  loss:  0.07789165526628494\n",
            "epoch:  311  loss:  0.07787657529115677\n",
            "epoch:  312  loss:  0.0778614953160286\n",
            "epoch:  313  loss:  0.07784643769264221\n",
            "epoch:  314  loss:  0.07783138006925583\n",
            "epoch:  315  loss:  0.07781635224819183\n",
            "epoch:  316  loss:  0.07780133187770844\n",
            "epoch:  317  loss:  0.07778631895780563\n",
            "epoch:  318  loss:  0.07777130603790283\n",
            "epoch:  319  loss:  0.07775631546974182\n",
            "epoch:  320  loss:  0.077741339802742\n",
            "epoch:  321  loss:  0.07772637903690338\n",
            "epoch:  322  loss:  0.07771142572164536\n",
            "epoch:  323  loss:  0.07769647985696793\n",
            "epoch:  324  loss:  0.07768155634403229\n",
            "epoch:  325  loss:  0.07766664028167725\n",
            "epoch:  326  loss:  0.0776517316699028\n",
            "epoch:  327  loss:  0.07763683795928955\n",
            "epoch:  328  loss:  0.0776219591498375\n",
            "epoch:  329  loss:  0.07760708779096603\n",
            "epoch:  330  loss:  0.07759223133325577\n",
            "epoch:  331  loss:  0.0775773823261261\n",
            "epoch:  332  loss:  0.07756254822015762\n",
            "epoch:  333  loss:  0.07754772901535034\n",
            "epoch:  334  loss:  0.07753290981054306\n",
            "epoch:  335  loss:  0.07751811295747757\n",
            "epoch:  336  loss:  0.07750332355499268\n",
            "epoch:  337  loss:  0.07748855650424957\n",
            "epoch:  338  loss:  0.07747378200292587\n",
            "epoch:  339  loss:  0.07745902985334396\n",
            "epoch:  340  loss:  0.07744429260492325\n",
            "epoch:  341  loss:  0.07742956280708313\n",
            "epoch:  342  loss:  0.07741483300924301\n",
            "epoch:  343  loss:  0.07740014046430588\n",
            "epoch:  344  loss:  0.07738544046878815\n",
            "epoch:  345  loss:  0.07737075537443161\n",
            "epoch:  346  loss:  0.07735608518123627\n",
            "epoch:  347  loss:  0.07734142243862152\n",
            "epoch:  348  loss:  0.07732677459716797\n",
            "epoch:  349  loss:  0.07731214165687561\n",
            "epoch:  350  loss:  0.07729751616716385\n",
            "epoch:  351  loss:  0.07728289812803268\n",
            "epoch:  352  loss:  0.07726829499006271\n",
            "epoch:  353  loss:  0.07725370675325394\n",
            "epoch:  354  loss:  0.07723912596702576\n",
            "epoch:  355  loss:  0.07722455263137817\n",
            "epoch:  356  loss:  0.07721000164747238\n",
            "epoch:  357  loss:  0.07719545811414719\n",
            "epoch:  358  loss:  0.07718092203140259\n",
            "epoch:  359  loss:  0.07716640830039978\n",
            "epoch:  360  loss:  0.07715188711881638\n",
            "epoch:  361  loss:  0.07713738828897476\n",
            "epoch:  362  loss:  0.07712289690971375\n",
            "epoch:  363  loss:  0.07710842043161392\n",
            "epoch:  364  loss:  0.07709396630525589\n",
            "epoch:  365  loss:  0.07707951217889786\n",
            "epoch:  366  loss:  0.07706506550312042\n",
            "epoch:  367  loss:  0.07705063372850418\n",
            "epoch:  368  loss:  0.07703621685504913\n",
            "epoch:  369  loss:  0.07702180743217468\n",
            "epoch:  370  loss:  0.07700740545988083\n",
            "epoch:  371  loss:  0.07699301838874817\n",
            "epoch:  372  loss:  0.0769786462187767\n",
            "epoch:  373  loss:  0.07696428894996643\n",
            "epoch:  374  loss:  0.07694993913173676\n",
            "epoch:  375  loss:  0.07693558186292648\n",
            "epoch:  376  loss:  0.0769212618470192\n",
            "epoch:  377  loss:  0.07690694183111191\n",
            "epoch:  378  loss:  0.07689263671636581\n",
            "epoch:  379  loss:  0.07687833160161972\n",
            "epoch:  380  loss:  0.07686404883861542\n",
            "epoch:  381  loss:  0.07684977352619171\n",
            "epoch:  382  loss:  0.0768355205655098\n",
            "epoch:  383  loss:  0.07682126015424728\n",
            "epoch:  384  loss:  0.07680702209472656\n",
            "epoch:  385  loss:  0.07679279893636703\n",
            "epoch:  386  loss:  0.07677857577800751\n",
            "epoch:  387  loss:  0.07676436007022858\n",
            "epoch:  388  loss:  0.07675016671419144\n",
            "epoch:  389  loss:  0.0767359808087349\n",
            "epoch:  390  loss:  0.07672180980443954\n",
            "epoch:  391  loss:  0.0767076388001442\n",
            "epoch:  392  loss:  0.07669349014759064\n",
            "epoch:  393  loss:  0.07667934149503708\n",
            "epoch:  394  loss:  0.07666521519422531\n",
            "epoch:  395  loss:  0.07665108889341354\n",
            "epoch:  396  loss:  0.07663699239492416\n",
            "epoch:  397  loss:  0.07662289589643478\n",
            "epoch:  398  loss:  0.076608806848526\n",
            "epoch:  399  loss:  0.07659472525119781\n",
            "epoch:  400  loss:  0.07658066600561142\n",
            "epoch:  401  loss:  0.07656662166118622\n",
            "epoch:  402  loss:  0.07655257731676102\n",
            "epoch:  403  loss:  0.07653854042291641\n",
            "epoch:  404  loss:  0.0765245109796524\n",
            "epoch:  405  loss:  0.07651051133871078\n",
            "epoch:  406  loss:  0.07649650424718857\n",
            "epoch:  407  loss:  0.07648251950740814\n",
            "epoch:  408  loss:  0.07646854966878891\n",
            "epoch:  409  loss:  0.07645457983016968\n",
            "epoch:  410  loss:  0.07644060999155045\n",
            "epoch:  411  loss:  0.0764266774058342\n",
            "epoch:  412  loss:  0.07641274482011795\n",
            "epoch:  413  loss:  0.0763988271355629\n",
            "epoch:  414  loss:  0.07638490200042725\n",
            "epoch:  415  loss:  0.07637100666761398\n",
            "epoch:  416  loss:  0.07635711133480072\n",
            "epoch:  417  loss:  0.07634323835372925\n",
            "epoch:  418  loss:  0.07632935792207718\n",
            "epoch:  419  loss:  0.0763155072927475\n",
            "epoch:  420  loss:  0.07630165666341782\n",
            "epoch:  421  loss:  0.07628782838582993\n",
            "epoch:  422  loss:  0.07627399265766144\n",
            "epoch:  423  loss:  0.07626018673181534\n",
            "epoch:  424  loss:  0.07624637335538864\n",
            "epoch:  425  loss:  0.07623257488012314\n",
            "epoch:  426  loss:  0.07621879875659943\n",
            "epoch:  427  loss:  0.07620501518249512\n",
            "epoch:  428  loss:  0.07619126886129379\n",
            "epoch:  429  loss:  0.07617750018835068\n",
            "epoch:  430  loss:  0.07616376131772995\n",
            "epoch:  431  loss:  0.07615002989768982\n",
            "epoch:  432  loss:  0.07613632082939148\n",
            "epoch:  433  loss:  0.07612261176109314\n",
            "epoch:  434  loss:  0.0761089026927948\n",
            "epoch:  435  loss:  0.07609521597623825\n",
            "epoch:  436  loss:  0.0760815367102623\n",
            "epoch:  437  loss:  0.07606787234544754\n",
            "epoch:  438  loss:  0.07605421543121338\n",
            "epoch:  439  loss:  0.07604057341814041\n",
            "epoch:  440  loss:  0.07602692395448685\n",
            "epoch:  441  loss:  0.07601330429315567\n",
            "epoch:  442  loss:  0.07599969208240509\n",
            "epoch:  443  loss:  0.07598608732223511\n",
            "epoch:  444  loss:  0.07597249746322632\n",
            "epoch:  445  loss:  0.07595890760421753\n",
            "epoch:  446  loss:  0.07594533264636993\n",
            "epoch:  447  loss:  0.07593177258968353\n",
            "epoch:  448  loss:  0.07591821253299713\n",
            "epoch:  449  loss:  0.07590467482805252\n",
            "epoch:  450  loss:  0.0758911520242691\n",
            "epoch:  451  loss:  0.07587762176990509\n",
            "epoch:  452  loss:  0.07586412131786346\n",
            "epoch:  453  loss:  0.07585061341524124\n",
            "epoch:  454  loss:  0.07583712786436081\n",
            "epoch:  455  loss:  0.07582364976406097\n",
            "epoch:  456  loss:  0.07581017911434174\n",
            "epoch:  457  loss:  0.07579672336578369\n",
            "epoch:  458  loss:  0.07578326761722565\n",
            "epoch:  459  loss:  0.0757698342204094\n",
            "epoch:  460  loss:  0.07575640082359314\n",
            "epoch:  461  loss:  0.07574298232793808\n",
            "epoch:  462  loss:  0.07572957873344421\n",
            "epoch:  463  loss:  0.07571619004011154\n",
            "epoch:  464  loss:  0.07570279389619827\n",
            "epoch:  465  loss:  0.0756894201040268\n",
            "epoch:  466  loss:  0.07567605376243591\n",
            "epoch:  467  loss:  0.07566269487142563\n",
            "epoch:  468  loss:  0.07564935088157654\n",
            "epoch:  469  loss:  0.07563601434230804\n",
            "epoch:  470  loss:  0.07562268525362015\n",
            "epoch:  471  loss:  0.07560937851667404\n",
            "epoch:  472  loss:  0.07559607177972794\n",
            "epoch:  473  loss:  0.07558277249336243\n",
            "epoch:  474  loss:  0.07556949555873871\n",
            "epoch:  475  loss:  0.07555621862411499\n",
            "epoch:  476  loss:  0.07554295659065247\n",
            "epoch:  477  loss:  0.07552969455718994\n",
            "epoch:  478  loss:  0.07551645487546921\n",
            "epoch:  479  loss:  0.07550322264432907\n",
            "epoch:  480  loss:  0.07548999041318893\n",
            "epoch:  481  loss:  0.07547677308320999\n",
            "epoch:  482  loss:  0.07546357810497284\n",
            "epoch:  483  loss:  0.07545037567615509\n",
            "epoch:  484  loss:  0.07543720304965973\n",
            "epoch:  485  loss:  0.07542403042316437\n",
            "epoch:  486  loss:  0.0754108652472496\n",
            "epoch:  487  loss:  0.07539771497249603\n",
            "epoch:  488  loss:  0.07538456469774246\n",
            "epoch:  489  loss:  0.07537143677473068\n",
            "epoch:  490  loss:  0.0753583088517189\n",
            "epoch:  491  loss:  0.07534520328044891\n",
            "epoch:  492  loss:  0.07533209770917892\n",
            "epoch:  493  loss:  0.07531899213790894\n",
            "epoch:  494  loss:  0.07530592381954193\n",
            "epoch:  495  loss:  0.07529284060001373\n",
            "epoch:  496  loss:  0.07527977973222733\n",
            "epoch:  497  loss:  0.07526672631502151\n",
            "epoch:  498  loss:  0.0752536803483963\n",
            "epoch:  499  loss:  0.07524064928293228\n",
            "epoch:  500  loss:  0.07522762566804886\n",
            "epoch:  501  loss:  0.07521460950374603\n",
            "epoch:  502  loss:  0.0752016007900238\n",
            "epoch:  503  loss:  0.07518861442804337\n",
            "epoch:  504  loss:  0.07517562061548233\n",
            "epoch:  505  loss:  0.07516264915466309\n",
            "epoch:  506  loss:  0.07514968514442444\n",
            "epoch:  507  loss:  0.07513672858476639\n",
            "epoch:  508  loss:  0.07512379437685013\n",
            "epoch:  509  loss:  0.07511085271835327\n",
            "epoch:  510  loss:  0.0750979334115982\n",
            "epoch:  511  loss:  0.07508501410484314\n",
            "epoch:  512  loss:  0.07507210224866867\n",
            "epoch:  513  loss:  0.07505921274423599\n",
            "epoch:  514  loss:  0.07504632323980331\n",
            "epoch:  515  loss:  0.07503344863653183\n",
            "epoch:  516  loss:  0.07502058148384094\n",
            "epoch:  517  loss:  0.07500772178173065\n",
            "epoch:  518  loss:  0.07499486953020096\n",
            "epoch:  519  loss:  0.07498203963041306\n",
            "epoch:  520  loss:  0.07496920973062515\n",
            "epoch:  521  loss:  0.07495639473199844\n",
            "epoch:  522  loss:  0.07494358718395233\n",
            "epoch:  523  loss:  0.07493079453706741\n",
            "epoch:  524  loss:  0.0749180018901825\n",
            "epoch:  525  loss:  0.07490522414445877\n",
            "epoch:  526  loss:  0.07489244639873505\n",
            "epoch:  527  loss:  0.07487969100475311\n",
            "epoch:  528  loss:  0.07486694306135178\n",
            "epoch:  529  loss:  0.07485420256853104\n",
            "epoch:  530  loss:  0.0748414695262909\n",
            "epoch:  531  loss:  0.07482874393463135\n",
            "epoch:  532  loss:  0.07481604069471359\n",
            "epoch:  533  loss:  0.07480333000421524\n",
            "epoch:  534  loss:  0.07479064166545868\n",
            "epoch:  535  loss:  0.07477796077728271\n",
            "epoch:  536  loss:  0.07476529479026794\n",
            "epoch:  537  loss:  0.07475262135267258\n",
            "epoch:  538  loss:  0.074739970266819\n",
            "epoch:  539  loss:  0.07472732663154602\n",
            "epoch:  540  loss:  0.07471469044685364\n",
            "epoch:  541  loss:  0.07470206916332245\n",
            "epoch:  542  loss:  0.07468945533037186\n",
            "epoch:  543  loss:  0.07467684149742126\n",
            "epoch:  544  loss:  0.07466425001621246\n",
            "epoch:  545  loss:  0.07465166598558426\n",
            "epoch:  546  loss:  0.07463908195495605\n",
            "epoch:  547  loss:  0.07462651282548904\n",
            "epoch:  548  loss:  0.07461395114660263\n",
            "epoch:  549  loss:  0.07460141181945801\n",
            "epoch:  550  loss:  0.07458886504173279\n",
            "epoch:  551  loss:  0.07457634061574936\n",
            "epoch:  552  loss:  0.07456381618976593\n",
            "epoch:  553  loss:  0.0745512992143631\n",
            "epoch:  554  loss:  0.07453880459070206\n",
            "epoch:  555  loss:  0.07452631741762161\n",
            "epoch:  556  loss:  0.07451383024454117\n",
            "epoch:  557  loss:  0.07450135797262192\n",
            "epoch:  558  loss:  0.07448889315128326\n",
            "epoch:  559  loss:  0.0744764432311058\n",
            "epoch:  560  loss:  0.07446398586034775\n",
            "epoch:  561  loss:  0.07445155829191208\n",
            "epoch:  562  loss:  0.07443912327289581\n",
            "epoch:  563  loss:  0.07442671060562134\n",
            "epoch:  564  loss:  0.07441430538892746\n",
            "epoch:  565  loss:  0.07440190762281418\n",
            "epoch:  566  loss:  0.0743895098567009\n",
            "epoch:  567  loss:  0.0743771344423294\n",
            "epoch:  568  loss:  0.07436475902795792\n",
            "epoch:  569  loss:  0.07435239851474762\n",
            "epoch:  570  loss:  0.07434005290269852\n",
            "epoch:  571  loss:  0.07432771474123001\n",
            "epoch:  572  loss:  0.07431536912918091\n",
            "epoch:  573  loss:  0.0743030458688736\n",
            "epoch:  574  loss:  0.07429073005914688\n",
            "epoch:  575  loss:  0.07427842915058136\n",
            "epoch:  576  loss:  0.07426613569259644\n",
            "epoch:  577  loss:  0.07425384223461151\n",
            "epoch:  578  loss:  0.07424157112836838\n",
            "epoch:  579  loss:  0.07422930002212524\n",
            "epoch:  580  loss:  0.07421703636646271\n",
            "epoch:  581  loss:  0.07420478016138077\n",
            "epoch:  582  loss:  0.07419254630804062\n",
            "epoch:  583  loss:  0.07418031245470047\n",
            "epoch:  584  loss:  0.07416809350252151\n",
            "epoch:  585  loss:  0.07415586709976196\n",
            "epoch:  586  loss:  0.0741436779499054\n",
            "epoch:  587  loss:  0.07413147389888763\n",
            "epoch:  588  loss:  0.07411929219961166\n",
            "epoch:  589  loss:  0.07410711795091629\n",
            "epoch:  590  loss:  0.07409495115280151\n",
            "epoch:  591  loss:  0.07408279180526733\n",
            "epoch:  592  loss:  0.07407063245773315\n",
            "epoch:  593  loss:  0.07405849546194077\n",
            "epoch:  594  loss:  0.07404637336730957\n",
            "epoch:  595  loss:  0.07403423637151718\n",
            "epoch:  596  loss:  0.07402212917804718\n",
            "epoch:  597  loss:  0.07401002943515778\n",
            "epoch:  598  loss:  0.07399792969226837\n",
            "epoch:  599  loss:  0.07398584485054016\n",
            "epoch:  600  loss:  0.07397376000881195\n",
            "epoch:  601  loss:  0.07396169751882553\n",
            "epoch:  602  loss:  0.07394963502883911\n",
            "epoch:  603  loss:  0.07393759489059448\n",
            "epoch:  604  loss:  0.07392555475234985\n",
            "epoch:  605  loss:  0.07391351461410522\n",
            "epoch:  606  loss:  0.07390149682760239\n",
            "epoch:  607  loss:  0.07388947904109955\n",
            "epoch:  608  loss:  0.07387746870517731\n",
            "epoch:  609  loss:  0.07386548072099686\n",
            "epoch:  610  loss:  0.073853500187397\n",
            "epoch:  611  loss:  0.07384150475263596\n",
            "epoch:  612  loss:  0.07382955402135849\n",
            "epoch:  613  loss:  0.07381758838891983\n",
            "epoch:  614  loss:  0.07380563020706177\n",
            "epoch:  615  loss:  0.0737936943769455\n",
            "epoch:  616  loss:  0.07378175109624863\n",
            "epoch:  617  loss:  0.07376982271671295\n",
            "epoch:  618  loss:  0.07375790923833847\n",
            "epoch:  619  loss:  0.07374600321054459\n",
            "epoch:  620  loss:  0.0737341046333313\n",
            "epoch:  621  loss:  0.07372221350669861\n",
            "epoch:  622  loss:  0.07371034473180771\n",
            "epoch:  623  loss:  0.07369846850633621\n",
            "epoch:  624  loss:  0.07368660718202591\n",
            "epoch:  625  loss:  0.0736747607588768\n",
            "epoch:  626  loss:  0.0736628994345665\n",
            "epoch:  627  loss:  0.07365106791257858\n",
            "epoch:  628  loss:  0.07363924384117126\n",
            "epoch:  629  loss:  0.07362742722034454\n",
            "epoch:  630  loss:  0.07361561805009842\n",
            "epoch:  631  loss:  0.07360382378101349\n",
            "epoch:  632  loss:  0.07359202206134796\n",
            "epoch:  633  loss:  0.07358023524284363\n",
            "epoch:  634  loss:  0.07356846332550049\n",
            "epoch:  635  loss:  0.07355669140815735\n",
            "epoch:  636  loss:  0.0735449343919754\n",
            "epoch:  637  loss:  0.07353319227695465\n",
            "epoch:  638  loss:  0.0735214501619339\n",
            "epoch:  639  loss:  0.07350971549749374\n",
            "epoch:  640  loss:  0.07349800318479538\n",
            "epoch:  641  loss:  0.07348627597093582\n",
            "epoch:  642  loss:  0.07347457855939865\n",
            "epoch:  643  loss:  0.07346287369728088\n",
            "epoch:  644  loss:  0.07345118373632431\n",
            "epoch:  645  loss:  0.07343951612710953\n",
            "epoch:  646  loss:  0.07342784106731415\n",
            "epoch:  647  loss:  0.07341617345809937\n",
            "epoch:  648  loss:  0.07340452820062637\n",
            "epoch:  649  loss:  0.07339287549257278\n",
            "epoch:  650  loss:  0.07338123768568039\n",
            "epoch:  651  loss:  0.07336962223052979\n",
            "epoch:  652  loss:  0.07335799932479858\n",
            "epoch:  653  loss:  0.07334639132022858\n",
            "epoch:  654  loss:  0.07333479076623917\n",
            "epoch:  655  loss:  0.07332319766283035\n",
            "epoch:  656  loss:  0.07331160455942154\n",
            "epoch:  657  loss:  0.07330003380775452\n",
            "epoch:  658  loss:  0.07328847795724869\n",
            "epoch:  659  loss:  0.07327690720558167\n",
            "epoch:  660  loss:  0.07326535880565643\n",
            "epoch:  661  loss:  0.0732538253068924\n",
            "epoch:  662  loss:  0.07324228435754776\n",
            "epoch:  663  loss:  0.07323075830936432\n",
            "epoch:  664  loss:  0.07321924716234207\n",
            "epoch:  665  loss:  0.07320774346590042\n",
            "epoch:  666  loss:  0.07319623976945877\n",
            "epoch:  667  loss:  0.07318475097417831\n",
            "epoch:  668  loss:  0.07317326962947845\n",
            "epoch:  669  loss:  0.0731617882847786\n",
            "epoch:  670  loss:  0.07315032929182053\n",
            "epoch:  671  loss:  0.07313887774944305\n",
            "epoch:  672  loss:  0.07312742620706558\n",
            "epoch:  673  loss:  0.0731159895658493\n",
            "epoch:  674  loss:  0.07310455292463303\n",
            "epoch:  675  loss:  0.07309313863515854\n",
            "epoch:  676  loss:  0.07308172434568405\n",
            "epoch:  677  loss:  0.07307031750679016\n",
            "epoch:  678  loss:  0.07305891811847687\n",
            "epoch:  679  loss:  0.07304753363132477\n",
            "epoch:  680  loss:  0.07303616404533386\n",
            "epoch:  681  loss:  0.07302478700876236\n",
            "epoch:  682  loss:  0.07301341742277145\n",
            "epoch:  683  loss:  0.07300207018852234\n",
            "epoch:  684  loss:  0.07299072295427322\n",
            "epoch:  685  loss:  0.0729793831706047\n",
            "epoch:  686  loss:  0.07296805828809738\n",
            "epoch:  687  loss:  0.07295673340559006\n",
            "epoch:  688  loss:  0.07294541597366333\n",
            "epoch:  689  loss:  0.0729341134428978\n",
            "epoch:  690  loss:  0.07292281836271286\n",
            "epoch:  691  loss:  0.07291153073310852\n",
            "epoch:  692  loss:  0.07290025055408478\n",
            "epoch:  693  loss:  0.07288898527622223\n",
            "epoch:  694  loss:  0.07287771254777908\n",
            "epoch:  695  loss:  0.07286646217107773\n",
            "epoch:  696  loss:  0.07285522669553757\n",
            "epoch:  697  loss:  0.07284398376941681\n",
            "epoch:  698  loss:  0.07283274829387665\n",
            "epoch:  699  loss:  0.07282153517007828\n",
            "epoch:  700  loss:  0.07281032204627991\n",
            "epoch:  701  loss:  0.07279911637306213\n",
            "epoch:  702  loss:  0.07278791815042496\n",
            "epoch:  703  loss:  0.07277673482894897\n",
            "epoch:  704  loss:  0.07276555895805359\n",
            "epoch:  705  loss:  0.0727543756365776\n",
            "epoch:  706  loss:  0.07274321466684341\n",
            "epoch:  707  loss:  0.07273206114768982\n",
            "epoch:  708  loss:  0.07272090762853622\n",
            "epoch:  709  loss:  0.07270977646112442\n",
            "epoch:  710  loss:  0.07269863784313202\n",
            "epoch:  711  loss:  0.07268752157688141\n",
            "epoch:  712  loss:  0.0726764127612114\n",
            "epoch:  713  loss:  0.07266531139612198\n",
            "epoch:  714  loss:  0.07265420258045197\n",
            "epoch:  715  loss:  0.07264311611652374\n",
            "epoch:  716  loss:  0.07263204455375671\n",
            "epoch:  717  loss:  0.07262096554040909\n",
            "epoch:  718  loss:  0.07260990142822266\n",
            "epoch:  719  loss:  0.07259884476661682\n",
            "epoch:  720  loss:  0.07258779555559158\n",
            "epoch:  721  loss:  0.07257676124572754\n",
            "epoch:  722  loss:  0.0725657269358635\n",
            "epoch:  723  loss:  0.07255470007658005\n",
            "epoch:  724  loss:  0.0725436881184578\n",
            "epoch:  725  loss:  0.07253267616033554\n",
            "epoch:  726  loss:  0.07252167165279388\n",
            "epoch:  727  loss:  0.07251068204641342\n",
            "epoch:  728  loss:  0.07249969244003296\n",
            "epoch:  729  loss:  0.07248872518539429\n",
            "epoch:  730  loss:  0.07247775048017502\n",
            "epoch:  731  loss:  0.07246679812669754\n",
            "epoch:  732  loss:  0.07245584577322006\n",
            "epoch:  733  loss:  0.07244490832090378\n",
            "epoch:  734  loss:  0.0724339708685875\n",
            "epoch:  735  loss:  0.07242303341627121\n",
            "epoch:  736  loss:  0.07241212576627731\n",
            "epoch:  737  loss:  0.07240121066570282\n",
            "epoch:  738  loss:  0.07239030301570892\n",
            "epoch:  739  loss:  0.07237941026687622\n",
            "epoch:  740  loss:  0.07236852496862411\n",
            "epoch:  741  loss:  0.07235763221979141\n",
            "epoch:  742  loss:  0.0723467692732811\n",
            "epoch:  743  loss:  0.07233590632677078\n",
            "epoch:  744  loss:  0.07232505828142166\n",
            "epoch:  745  loss:  0.07231420278549194\n",
            "epoch:  746  loss:  0.07230336964130402\n",
            "epoch:  747  loss:  0.07229253649711609\n",
            "epoch:  748  loss:  0.07228171825408936\n",
            "epoch:  749  loss:  0.07227089256048203\n",
            "epoch:  750  loss:  0.07226008176803589\n",
            "epoch:  751  loss:  0.07224929332733154\n",
            "epoch:  752  loss:  0.0722384974360466\n",
            "epoch:  753  loss:  0.07222771644592285\n",
            "epoch:  754  loss:  0.0722169280052185\n",
            "epoch:  755  loss:  0.07220616191625595\n",
            "epoch:  756  loss:  0.07219540327787399\n",
            "epoch:  757  loss:  0.07218464463949203\n",
            "epoch:  758  loss:  0.07217390090227127\n",
            "epoch:  759  loss:  0.0721631720662117\n",
            "epoch:  760  loss:  0.07215244323015213\n",
            "epoch:  761  loss:  0.07214172184467316\n",
            "epoch:  762  loss:  0.07213100790977478\n",
            "epoch:  763  loss:  0.072120301425457\n",
            "epoch:  764  loss:  0.07210959494113922\n",
            "epoch:  765  loss:  0.07209891825914383\n",
            "epoch:  766  loss:  0.07208822667598724\n",
            "epoch:  767  loss:  0.07207755744457245\n",
            "epoch:  768  loss:  0.07206688821315765\n",
            "epoch:  769  loss:  0.07205623388290405\n",
            "epoch:  770  loss:  0.07204558700323105\n",
            "epoch:  771  loss:  0.07203493267297745\n",
            "epoch:  772  loss:  0.07202430814504623\n",
            "epoch:  773  loss:  0.07201366871595383\n",
            "epoch:  774  loss:  0.07200305163860321\n",
            "epoch:  775  loss:  0.07199244946241379\n",
            "epoch:  776  loss:  0.07198183238506317\n",
            "epoch:  777  loss:  0.07197123765945435\n",
            "epoch:  778  loss:  0.07196065038442612\n",
            "epoch:  779  loss:  0.07195007055997849\n",
            "epoch:  780  loss:  0.07193949073553085\n",
            "epoch:  781  loss:  0.07192892581224442\n",
            "epoch:  782  loss:  0.07191836833953857\n",
            "epoch:  783  loss:  0.07190781831741333\n",
            "epoch:  784  loss:  0.07189727574586868\n",
            "epoch:  785  loss:  0.07188673317432404\n",
            "epoch:  786  loss:  0.07187622040510178\n",
            "epoch:  787  loss:  0.07186569273471832\n",
            "epoch:  788  loss:  0.07185518741607666\n",
            "epoch:  789  loss:  0.071844682097435\n",
            "epoch:  790  loss:  0.07183418422937393\n",
            "epoch:  791  loss:  0.07182369381189346\n",
            "epoch:  792  loss:  0.07181321084499359\n",
            "epoch:  793  loss:  0.07180273532867432\n",
            "epoch:  794  loss:  0.07179226726293564\n",
            "epoch:  795  loss:  0.07178181409835815\n",
            "epoch:  796  loss:  0.07177136093378067\n",
            "epoch:  797  loss:  0.07176091521978378\n",
            "epoch:  798  loss:  0.07175047695636749\n",
            "epoch:  799  loss:  0.0717400535941124\n",
            "epoch:  800  loss:  0.0717296302318573\n",
            "epoch:  801  loss:  0.0717192217707634\n",
            "epoch:  802  loss:  0.0717088133096695\n",
            "epoch:  803  loss:  0.07169841229915619\n",
            "epoch:  804  loss:  0.07168802618980408\n",
            "epoch:  805  loss:  0.07167763262987137\n",
            "epoch:  806  loss:  0.07166726887226105\n",
            "epoch:  807  loss:  0.07165689766407013\n",
            "epoch:  808  loss:  0.0716465413570404\n",
            "epoch:  809  loss:  0.07163618505001068\n",
            "epoch:  810  loss:  0.07162584364414215\n",
            "epoch:  811  loss:  0.07161550968885422\n",
            "epoch:  812  loss:  0.07160516828298569\n",
            "epoch:  813  loss:  0.07159484922885895\n",
            "epoch:  814  loss:  0.07158453017473221\n",
            "epoch:  815  loss:  0.07157422602176666\n",
            "epoch:  816  loss:  0.07156392931938171\n",
            "epoch:  817  loss:  0.07155363261699677\n",
            "epoch:  818  loss:  0.07154335081577301\n",
            "epoch:  819  loss:  0.07153307646512985\n",
            "epoch:  820  loss:  0.0715227946639061\n",
            "epoch:  821  loss:  0.07151254266500473\n",
            "epoch:  822  loss:  0.07150229066610336\n",
            "epoch:  823  loss:  0.0714920312166214\n",
            "epoch:  824  loss:  0.07148179411888123\n",
            "epoch:  825  loss:  0.07147155702114105\n",
            "epoch:  826  loss:  0.07146133482456207\n",
            "epoch:  827  loss:  0.07145112007856369\n",
            "epoch:  828  loss:  0.07144090533256531\n",
            "epoch:  829  loss:  0.07143070548772812\n",
            "epoch:  830  loss:  0.07142049819231033\n",
            "epoch:  831  loss:  0.07141031324863434\n",
            "epoch:  832  loss:  0.07140013575553894\n",
            "epoch:  833  loss:  0.07138995081186295\n",
            "epoch:  834  loss:  0.07137978076934814\n",
            "epoch:  835  loss:  0.07136963307857513\n",
            "epoch:  836  loss:  0.07135948538780212\n",
            "epoch:  837  loss:  0.07134933769702911\n",
            "epoch:  838  loss:  0.0713391974568367\n",
            "epoch:  839  loss:  0.07132906466722488\n",
            "epoch:  840  loss:  0.07131893932819366\n",
            "epoch:  841  loss:  0.07130882143974304\n",
            "epoch:  842  loss:  0.07129871845245361\n",
            "epoch:  843  loss:  0.07128862291574478\n",
            "epoch:  844  loss:  0.07127851992845535\n",
            "epoch:  845  loss:  0.07126843929290771\n",
            "epoch:  846  loss:  0.07125835120677948\n",
            "epoch:  847  loss:  0.07124828547239304\n",
            "epoch:  848  loss:  0.07123822718858719\n",
            "epoch:  849  loss:  0.07122815400362015\n",
            "epoch:  850  loss:  0.0712181106209755\n",
            "epoch:  851  loss:  0.07120806723833084\n",
            "epoch:  852  loss:  0.07119803130626678\n",
            "epoch:  853  loss:  0.07118800282478333\n",
            "epoch:  854  loss:  0.07117798179388046\n",
            "epoch:  855  loss:  0.0711679682135582\n",
            "epoch:  856  loss:  0.07115796953439713\n",
            "epoch:  857  loss:  0.07114796340465546\n",
            "epoch:  858  loss:  0.07113797217607498\n",
            "epoch:  859  loss:  0.0711279809474945\n",
            "epoch:  860  loss:  0.07111800462007523\n",
            "epoch:  861  loss:  0.07110804319381714\n",
            "epoch:  862  loss:  0.07109806686639786\n",
            "epoch:  863  loss:  0.07108812034130096\n",
            "epoch:  864  loss:  0.07107817381620407\n",
            "epoch:  865  loss:  0.07106821984052658\n",
            "epoch:  866  loss:  0.07105828821659088\n",
            "epoch:  867  loss:  0.07104836404323578\n",
            "epoch:  868  loss:  0.07103843986988068\n",
            "epoch:  869  loss:  0.07102852314710617\n",
            "epoch:  870  loss:  0.07101861387491226\n",
            "epoch:  871  loss:  0.07100871950387955\n",
            "epoch:  872  loss:  0.07099882513284683\n",
            "epoch:  873  loss:  0.07098894566297531\n",
            "epoch:  874  loss:  0.0709790587425232\n",
            "epoch:  875  loss:  0.07096919417381287\n",
            "epoch:  876  loss:  0.07095932960510254\n",
            "epoch:  877  loss:  0.07094947248697281\n",
            "epoch:  878  loss:  0.07093962281942368\n",
            "epoch:  879  loss:  0.07092978060245514\n",
            "epoch:  880  loss:  0.0709199383854866\n",
            "epoch:  881  loss:  0.07091011106967926\n",
            "epoch:  882  loss:  0.07090029120445251\n",
            "epoch:  883  loss:  0.07089047878980637\n",
            "epoch:  884  loss:  0.07088065892457962\n",
            "epoch:  885  loss:  0.07087086886167526\n",
            "epoch:  886  loss:  0.0708610787987709\n",
            "epoch:  887  loss:  0.07085128873586655\n",
            "epoch:  888  loss:  0.07084150612354279\n",
            "epoch:  889  loss:  0.07083173841238022\n",
            "epoch:  890  loss:  0.07082197815179825\n",
            "epoch:  891  loss:  0.07081221044063568\n",
            "epoch:  892  loss:  0.07080245763063431\n",
            "epoch:  893  loss:  0.07079271972179413\n",
            "epoch:  894  loss:  0.07078298181295395\n",
            "epoch:  895  loss:  0.07077324390411377\n",
            "epoch:  896  loss:  0.07076352834701538\n",
            "epoch:  897  loss:  0.07075381278991699\n",
            "epoch:  898  loss:  0.0707440972328186\n",
            "epoch:  899  loss:  0.07073439657688141\n",
            "epoch:  900  loss:  0.07072471082210541\n",
            "epoch:  901  loss:  0.07071501016616821\n",
            "epoch:  902  loss:  0.07070533186197281\n",
            "epoch:  903  loss:  0.0706956535577774\n",
            "epoch:  904  loss:  0.0706859901547432\n",
            "epoch:  905  loss:  0.07067631930112839\n",
            "epoch:  906  loss:  0.07066667079925537\n",
            "epoch:  907  loss:  0.07065702974796295\n",
            "epoch:  908  loss:  0.07064738124608994\n",
            "epoch:  909  loss:  0.07063775509595871\n",
            "epoch:  910  loss:  0.07062812894582748\n",
            "epoch:  911  loss:  0.07061851024627686\n",
            "epoch:  912  loss:  0.07060889154672623\n",
            "epoch:  913  loss:  0.07059928774833679\n",
            "epoch:  914  loss:  0.07058969140052795\n",
            "epoch:  915  loss:  0.07058010250329971\n",
            "epoch:  916  loss:  0.07057051360607147\n",
            "epoch:  917  loss:  0.07056093961000443\n",
            "epoch:  918  loss:  0.07055135816335678\n",
            "epoch:  919  loss:  0.07054179161787033\n",
            "epoch:  920  loss:  0.07053223252296448\n",
            "epoch:  921  loss:  0.07052268832921982\n",
            "epoch:  922  loss:  0.07051314413547516\n",
            "epoch:  923  loss:  0.0705036148428917\n",
            "epoch:  924  loss:  0.07049407809972763\n",
            "epoch:  925  loss:  0.07048454135656357\n",
            "epoch:  926  loss:  0.0704750344157219\n",
            "epoch:  927  loss:  0.07046552747488022\n",
            "epoch:  928  loss:  0.07045602053403854\n",
            "epoch:  929  loss:  0.07044652104377747\n",
            "epoch:  930  loss:  0.07043704390525818\n",
            "epoch:  931  loss:  0.0704275518655777\n",
            "epoch:  932  loss:  0.07041807472705841\n",
            "epoch:  933  loss:  0.07040860503911972\n",
            "epoch:  934  loss:  0.07039915025234222\n",
            "epoch:  935  loss:  0.07038968056440353\n",
            "epoch:  936  loss:  0.07038024067878723\n",
            "epoch:  937  loss:  0.07037078589200974\n",
            "epoch:  938  loss:  0.07036136090755463\n",
            "epoch:  939  loss:  0.07035192847251892\n",
            "epoch:  940  loss:  0.07034250348806381\n",
            "epoch:  941  loss:  0.0703331008553505\n",
            "epoch:  942  loss:  0.07032367587089539\n",
            "epoch:  943  loss:  0.07031428813934326\n",
            "epoch:  944  loss:  0.07030488550662994\n",
            "epoch:  945  loss:  0.07029550522565842\n",
            "epoch:  946  loss:  0.07028611749410629\n",
            "epoch:  947  loss:  0.07027674466371536\n",
            "epoch:  948  loss:  0.07026737928390503\n",
            "epoch:  949  loss:  0.0702580064535141\n",
            "epoch:  950  loss:  0.07024865597486496\n",
            "epoch:  951  loss:  0.07023930549621582\n",
            "epoch:  952  loss:  0.07022996246814728\n",
            "epoch:  953  loss:  0.07022062689065933\n",
            "epoch:  954  loss:  0.07021130621433258\n",
            "epoch:  955  loss:  0.07020197808742523\n",
            "epoch:  956  loss:  0.07019266486167908\n",
            "epoch:  957  loss:  0.07018335163593292\n",
            "epoch:  958  loss:  0.07017403841018677\n",
            "epoch:  959  loss:  0.070164754986763\n",
            "epoch:  960  loss:  0.07015546411275864\n",
            "epoch:  961  loss:  0.07014618813991547\n",
            "epoch:  962  loss:  0.0701369121670723\n",
            "epoch:  963  loss:  0.07012763619422913\n",
            "epoch:  964  loss:  0.07011836767196655\n",
            "epoch:  965  loss:  0.07010911405086517\n",
            "epoch:  966  loss:  0.07009986788034439\n",
            "epoch:  967  loss:  0.0700906291604042\n",
            "epoch:  968  loss:  0.07008139044046402\n",
            "epoch:  969  loss:  0.07007215917110443\n",
            "epoch:  970  loss:  0.07006293535232544\n",
            "epoch:  971  loss:  0.07005371898412704\n",
            "epoch:  972  loss:  0.07004451006650925\n",
            "epoch:  973  loss:  0.07003530859947205\n",
            "epoch:  974  loss:  0.07002610713243484\n",
            "epoch:  975  loss:  0.07001692056655884\n",
            "epoch:  976  loss:  0.07000773400068283\n",
            "epoch:  977  loss:  0.06999855488538742\n",
            "epoch:  978  loss:  0.0699893906712532\n",
            "epoch:  979  loss:  0.06998022645711899\n",
            "epoch:  980  loss:  0.06997106969356537\n",
            "epoch:  981  loss:  0.06996191293001175\n",
            "epoch:  982  loss:  0.06995276361703873\n",
            "epoch:  983  loss:  0.0699436366558075\n",
            "epoch:  984  loss:  0.06993449479341507\n",
            "epoch:  985  loss:  0.06992536783218384\n",
            "epoch:  986  loss:  0.0699162557721138\n",
            "epoch:  987  loss:  0.06990714371204376\n",
            "epoch:  988  loss:  0.06989802420139313\n",
            "epoch:  989  loss:  0.06988894194364548\n",
            "epoch:  990  loss:  0.06987983733415604\n",
            "epoch:  991  loss:  0.06987074762582779\n",
            "epoch:  992  loss:  0.06986168026924133\n",
            "epoch:  993  loss:  0.06985259801149368\n",
            "epoch:  994  loss:  0.06984353810548782\n",
            "epoch:  995  loss:  0.06983446329832077\n",
            "epoch:  996  loss:  0.0698254182934761\n",
            "epoch:  997  loss:  0.06981636583805084\n",
            "epoch:  998  loss:  0.06980732828378677\n",
            "epoch:  999  loss:  0.06979828327894211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(2000):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(features_sketch_)\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, feature_real_image_)\n",
        "   print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfnp4spFnI9r",
        "outputId": "042d10d2-96cf-43b3-8c09-6ca25c4ae733"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  0.3399188816547394\n",
            "epoch:  1  loss:  0.33915644884109497\n",
            "epoch:  2  loss:  0.33839845657348633\n",
            "epoch:  3  loss:  0.33764463663101196\n",
            "epoch:  4  loss:  0.33689507842063904\n",
            "epoch:  5  loss:  0.3361497223377228\n",
            "epoch:  6  loss:  0.3354085385799408\n",
            "epoch:  7  loss:  0.33467140793800354\n",
            "epoch:  8  loss:  0.3339384198188782\n",
            "epoch:  9  loss:  0.33320939540863037\n",
            "epoch:  10  loss:  0.3324843645095825\n",
            "epoch:  11  loss:  0.3317633271217346\n",
            "epoch:  12  loss:  0.3310461640357971\n",
            "epoch:  13  loss:  0.33033287525177\n",
            "epoch:  14  loss:  0.3296234607696533\n",
            "epoch:  15  loss:  0.32891786098480225\n",
            "epoch:  16  loss:  0.328216016292572\n",
            "epoch:  17  loss:  0.32751792669296265\n",
            "epoch:  18  loss:  0.32682347297668457\n",
            "epoch:  19  loss:  0.32613274455070496\n",
            "epoch:  20  loss:  0.32544562220573425\n",
            "epoch:  21  loss:  0.32476210594177246\n",
            "epoch:  22  loss:  0.3240821659564972\n",
            "epoch:  23  loss:  0.3234057128429413\n",
            "epoch:  24  loss:  0.3227327764034271\n",
            "epoch:  25  loss:  0.3220633566379547\n",
            "epoch:  26  loss:  0.3213973641395569\n",
            "epoch:  27  loss:  0.3207347095012665\n",
            "epoch:  28  loss:  0.32007551193237305\n",
            "epoch:  29  loss:  0.31941965222358704\n",
            "epoch:  30  loss:  0.31876713037490845\n",
            "epoch:  31  loss:  0.3181178867816925\n",
            "epoch:  32  loss:  0.31747186183929443\n",
            "epoch:  33  loss:  0.316829115152359\n",
            "epoch:  34  loss:  0.31618958711624146\n",
            "epoch:  35  loss:  0.3155531883239746\n",
            "epoch:  36  loss:  0.31492000818252563\n",
            "epoch:  37  loss:  0.3142898976802826\n",
            "epoch:  38  loss:  0.31366294622421265\n",
            "epoch:  39  loss:  0.31303903460502625\n",
            "epoch:  40  loss:  0.3124181926250458\n",
            "epoch:  41  loss:  0.31180036067962646\n",
            "epoch:  42  loss:  0.3111855089664459\n",
            "epoch:  43  loss:  0.3105736970901489\n",
            "epoch:  44  loss:  0.3099648058414459\n",
            "epoch:  45  loss:  0.3093588352203369\n",
            "epoch:  46  loss:  0.3087557256221771\n",
            "epoch:  47  loss:  0.3081555664539337\n",
            "epoch:  48  loss:  0.3075583279132843\n",
            "epoch:  49  loss:  0.30696380138397217\n",
            "epoch:  50  loss:  0.30637213587760925\n",
            "epoch:  51  loss:  0.3057832717895508\n",
            "epoch:  52  loss:  0.30519717931747437\n",
            "epoch:  53  loss:  0.3046138286590576\n",
            "epoch:  54  loss:  0.30403321981430054\n",
            "epoch:  55  loss:  0.3034553527832031\n",
            "epoch:  56  loss:  0.3028801679611206\n",
            "epoch:  57  loss:  0.3023076057434082\n",
            "epoch:  58  loss:  0.3017376959323883\n",
            "epoch:  59  loss:  0.3011704385280609\n",
            "epoch:  60  loss:  0.300605833530426\n",
            "epoch:  61  loss:  0.3000437915325165\n",
            "epoch:  62  loss:  0.2994843125343323\n",
            "epoch:  63  loss:  0.298927366733551\n",
            "epoch:  64  loss:  0.2983730137348175\n",
            "epoch:  65  loss:  0.29782113432884216\n",
            "epoch:  66  loss:  0.2972717881202698\n",
            "epoch:  67  loss:  0.29672491550445557\n",
            "epoch:  68  loss:  0.29618051648139954\n",
            "epoch:  69  loss:  0.2956385910511017\n",
            "epoch:  70  loss:  0.29509904980659485\n",
            "epoch:  71  loss:  0.2945619523525238\n",
            "epoch:  72  loss:  0.2940272092819214\n",
            "epoch:  73  loss:  0.29349493980407715\n",
            "epoch:  74  loss:  0.29296496510505676\n",
            "epoch:  75  loss:  0.2924373745918274\n",
            "epoch:  76  loss:  0.29191210865974426\n",
            "epoch:  77  loss:  0.2913891673088074\n",
            "epoch:  78  loss:  0.2908685505390167\n",
            "epoch:  79  loss:  0.29035019874572754\n",
            "epoch:  80  loss:  0.2898341417312622\n",
            "epoch:  81  loss:  0.28932031989097595\n",
            "epoch:  82  loss:  0.28880876302719116\n",
            "epoch:  83  loss:  0.28829944133758545\n",
            "epoch:  84  loss:  0.2877923548221588\n",
            "epoch:  85  loss:  0.28728747367858887\n",
            "epoch:  86  loss:  0.2867847681045532\n",
            "epoch:  87  loss:  0.2862842381000519\n",
            "epoch:  88  loss:  0.28578588366508484\n",
            "epoch:  89  loss:  0.2852896749973297\n",
            "epoch:  90  loss:  0.2847956120967865\n",
            "epoch:  91  loss:  0.2843036353588104\n",
            "epoch:  92  loss:  0.28381383419036865\n",
            "epoch:  93  loss:  0.283326119184494\n",
            "epoch:  94  loss:  0.2828404903411865\n",
            "epoch:  95  loss:  0.28235694766044617\n",
            "epoch:  96  loss:  0.2818754315376282\n",
            "epoch:  97  loss:  0.28139597177505493\n",
            "epoch:  98  loss:  0.28091856837272644\n",
            "epoch:  99  loss:  0.2804431617259979\n",
            "epoch:  100  loss:  0.27996981143951416\n",
            "epoch:  101  loss:  0.27949848771095276\n",
            "epoch:  102  loss:  0.27902910113334656\n",
            "epoch:  103  loss:  0.2785617411136627\n",
            "epoch:  104  loss:  0.27809637784957886\n",
            "epoch:  105  loss:  0.2776329219341278\n",
            "epoch:  106  loss:  0.27717143297195435\n",
            "epoch:  107  loss:  0.27671191096305847\n",
            "epoch:  108  loss:  0.276254266500473\n",
            "epoch:  109  loss:  0.27579861879348755\n",
            "epoch:  110  loss:  0.2753448188304901\n",
            "epoch:  111  loss:  0.2748929560184479\n",
            "epoch:  112  loss:  0.27444300055503845\n",
            "epoch:  113  loss:  0.2739948630332947\n",
            "epoch:  114  loss:  0.2735486328601837\n",
            "epoch:  115  loss:  0.27310431003570557\n",
            "epoch:  116  loss:  0.2726617753505707\n",
            "epoch:  117  loss:  0.2722211182117462\n",
            "epoch:  118  loss:  0.271782249212265\n",
            "epoch:  119  loss:  0.27134525775909424\n",
            "epoch:  120  loss:  0.2709100544452667\n",
            "epoch:  121  loss:  0.27047663927078247\n",
            "epoch:  122  loss:  0.27004504203796387\n",
            "epoch:  123  loss:  0.2696152329444885\n",
            "epoch:  124  loss:  0.26918718218803406\n",
            "epoch:  125  loss:  0.2687609791755676\n",
            "epoch:  126  loss:  0.2683364450931549\n",
            "epoch:  127  loss:  0.26791372895240784\n",
            "epoch:  128  loss:  0.26749274134635925\n",
            "epoch:  129  loss:  0.26707351207733154\n",
            "epoch:  130  loss:  0.2666560113430023\n",
            "epoch:  131  loss:  0.2662401795387268\n",
            "epoch:  132  loss:  0.26582610607147217\n",
            "epoch:  133  loss:  0.265413761138916\n",
            "epoch:  134  loss:  0.2650030553340912\n",
            "epoch:  135  loss:  0.26459407806396484\n",
            "epoch:  136  loss:  0.2641867697238922\n",
            "epoch:  137  loss:  0.2637811303138733\n",
            "epoch:  138  loss:  0.26337718963623047\n",
            "epoch:  139  loss:  0.26297488808631897\n",
            "epoch:  140  loss:  0.2625742554664612\n",
            "epoch:  141  loss:  0.2621752619743347\n",
            "epoch:  142  loss:  0.26177793741226196\n",
            "epoch:  143  loss:  0.26138219237327576\n",
            "epoch:  144  loss:  0.26098811626434326\n",
            "epoch:  145  loss:  0.2605956494808197\n",
            "epoch:  146  loss:  0.2602047622203827\n",
            "epoch:  147  loss:  0.2598155438899994\n",
            "epoch:  148  loss:  0.25942790508270264\n",
            "epoch:  149  loss:  0.25904184579849243\n",
            "epoch:  150  loss:  0.2586573660373688\n",
            "epoch:  151  loss:  0.25827449560165405\n",
            "epoch:  152  loss:  0.2578931748867035\n",
            "epoch:  153  loss:  0.2575134038925171\n",
            "epoch:  154  loss:  0.2571352422237396\n",
            "epoch:  155  loss:  0.2567586302757263\n",
            "epoch:  156  loss:  0.2563835084438324\n",
            "epoch:  157  loss:  0.2560099959373474\n",
            "epoch:  158  loss:  0.2556379735469818\n",
            "epoch:  159  loss:  0.25526750087738037\n",
            "epoch:  160  loss:  0.2548985481262207\n",
            "epoch:  161  loss:  0.2545310854911804\n",
            "epoch:  162  loss:  0.2541651725769043\n",
            "epoch:  163  loss:  0.25380074977874756\n",
            "epoch:  164  loss:  0.253437876701355\n",
            "epoch:  165  loss:  0.2530764043331146\n",
            "epoch:  166  loss:  0.2527165114879608\n",
            "epoch:  167  loss:  0.2523580491542816\n",
            "epoch:  168  loss:  0.2520011067390442\n",
            "epoch:  169  loss:  0.251645565032959\n",
            "epoch:  170  loss:  0.25129154324531555\n",
            "epoch:  171  loss:  0.25093895196914673\n",
            "epoch:  172  loss:  0.2505878806114197\n",
            "epoch:  173  loss:  0.25023818016052246\n",
            "epoch:  174  loss:  0.24988998472690582\n",
            "epoch:  175  loss:  0.2495432198047638\n",
            "epoch:  176  loss:  0.249197855591774\n",
            "epoch:  177  loss:  0.24885395169258118\n",
            "epoch:  178  loss:  0.2485114485025406\n",
            "epoch:  179  loss:  0.2481703907251358\n",
            "epoch:  180  loss:  0.24783074855804443\n",
            "epoch:  181  loss:  0.2474924921989441\n",
            "epoch:  182  loss:  0.24715568125247955\n",
            "epoch:  183  loss:  0.24682025611400604\n",
            "epoch:  184  loss:  0.24648620188236237\n",
            "epoch:  185  loss:  0.2461535781621933\n",
            "epoch:  186  loss:  0.24582228064537048\n",
            "epoch:  187  loss:  0.24549241364002228\n",
            "epoch:  188  loss:  0.2451639026403427\n",
            "epoch:  189  loss:  0.24483679234981537\n",
            "epoch:  190  loss:  0.24451100826263428\n",
            "epoch:  191  loss:  0.24418659508228302\n",
            "epoch:  192  loss:  0.2438635528087616\n",
            "epoch:  193  loss:  0.24354185163974762\n",
            "epoch:  194  loss:  0.24322152137756348\n",
            "epoch:  195  loss:  0.24290253221988678\n",
            "epoch:  196  loss:  0.24258483946323395\n",
            "epoch:  197  loss:  0.24226851761341095\n",
            "epoch:  198  loss:  0.2419535368680954\n",
            "epoch:  199  loss:  0.2416398674249649\n",
            "epoch:  200  loss:  0.24132750928401947\n",
            "epoch:  201  loss:  0.2410164773464203\n",
            "epoch:  202  loss:  0.24070675671100616\n",
            "epoch:  203  loss:  0.2403983473777771\n",
            "epoch:  204  loss:  0.2400912344455719\n",
            "epoch:  205  loss:  0.23978543281555176\n",
            "epoch:  206  loss:  0.23948092758655548\n",
            "epoch:  207  loss:  0.23917770385742188\n",
            "epoch:  208  loss:  0.23887576162815094\n",
            "epoch:  209  loss:  0.23857511579990387\n",
            "epoch:  210  loss:  0.23827573657035828\n",
            "epoch:  211  loss:  0.23797762393951416\n",
            "epoch:  212  loss:  0.2376808077096939\n",
            "epoch:  213  loss:  0.23738522827625275\n",
            "epoch:  214  loss:  0.23709093034267426\n",
            "epoch:  215  loss:  0.23679788410663605\n",
            "epoch:  216  loss:  0.23650610446929932\n",
            "epoch:  217  loss:  0.23621559143066406\n",
            "epoch:  218  loss:  0.2359263002872467\n",
            "epoch:  219  loss:  0.23563826084136963\n",
            "epoch:  220  loss:  0.23535144329071045\n",
            "epoch:  221  loss:  0.23506587743759155\n",
            "epoch:  222  loss:  0.23478150367736816\n",
            "epoch:  223  loss:  0.23449838161468506\n",
            "epoch:  224  loss:  0.23421649634838104\n",
            "epoch:  225  loss:  0.23393581807613373\n",
            "epoch:  226  loss:  0.2336563616991043\n",
            "epoch:  227  loss:  0.2333780825138092\n",
            "epoch:  228  loss:  0.23310105502605438\n",
            "epoch:  229  loss:  0.23282520473003387\n",
            "epoch:  230  loss:  0.23255057632923126\n",
            "epoch:  231  loss:  0.23227712512016296\n",
            "epoch:  232  loss:  0.23200488090515137\n",
            "epoch:  233  loss:  0.23173381388187408\n",
            "epoch:  234  loss:  0.23146390914916992\n",
            "epoch:  235  loss:  0.23119522631168365\n",
            "epoch:  236  loss:  0.2309277057647705\n",
            "epoch:  237  loss:  0.23066134750843048\n",
            "epoch:  238  loss:  0.23039616644382477\n",
            "epoch:  239  loss:  0.23013216257095337\n",
            "epoch:  240  loss:  0.2298692911863327\n",
            "epoch:  241  loss:  0.22960759699344635\n",
            "epoch:  242  loss:  0.22934703528881073\n",
            "epoch:  243  loss:  0.22908766567707062\n",
            "epoch:  244  loss:  0.22882941365242004\n",
            "epoch:  245  loss:  0.22857226431369781\n",
            "epoch:  246  loss:  0.22831633687019348\n",
            "epoch:  247  loss:  0.2280614972114563\n",
            "epoch:  248  loss:  0.22780780494213104\n",
            "epoch:  249  loss:  0.22755523025989532\n",
            "epoch:  250  loss:  0.22730380296707153\n",
            "epoch:  251  loss:  0.2270534634590149\n",
            "epoch:  252  loss:  0.2268042415380478\n",
            "epoch:  253  loss:  0.22655616700649261\n",
            "epoch:  254  loss:  0.22630921006202698\n",
            "epoch:  255  loss:  0.2260633260011673\n",
            "epoch:  256  loss:  0.22581854462623596\n",
            "epoch:  257  loss:  0.22557491064071655\n",
            "epoch:  258  loss:  0.22533230483531952\n",
            "epoch:  259  loss:  0.22509083151817322\n",
            "epoch:  260  loss:  0.22485044598579407\n",
            "epoch:  261  loss:  0.22461113333702087\n",
            "epoch:  262  loss:  0.22437292337417603\n",
            "epoch:  263  loss:  0.22413577139377594\n",
            "epoch:  264  loss:  0.2238996922969818\n",
            "epoch:  265  loss:  0.22366470098495483\n",
            "epoch:  266  loss:  0.22343075275421143\n",
            "epoch:  267  loss:  0.22319789230823517\n",
            "epoch:  268  loss:  0.22296608984470367\n",
            "epoch:  269  loss:  0.22273533046245575\n",
            "epoch:  270  loss:  0.2225056141614914\n",
            "epoch:  271  loss:  0.222276970744133\n",
            "epoch:  272  loss:  0.22204938530921936\n",
            "epoch:  273  loss:  0.2218228280544281\n",
            "epoch:  274  loss:  0.2215973287820816\n",
            "epoch:  275  loss:  0.22137285768985748\n",
            "epoch:  276  loss:  0.22114941477775574\n",
            "epoch:  277  loss:  0.22092700004577637\n",
            "epoch:  278  loss:  0.22070559859275818\n",
            "epoch:  279  loss:  0.22048522531986237\n",
            "epoch:  280  loss:  0.22026589512825012\n",
            "epoch:  281  loss:  0.22004756331443787\n",
            "epoch:  282  loss:  0.2198302298784256\n",
            "epoch:  283  loss:  0.2196139395236969\n",
            "epoch:  284  loss:  0.2193986475467682\n",
            "epoch:  285  loss:  0.21918430924415588\n",
            "epoch:  286  loss:  0.21897104382514954\n",
            "epoch:  287  loss:  0.2187587171792984\n",
            "epoch:  288  loss:  0.21854738891124725\n",
            "epoch:  289  loss:  0.2183370739221573\n",
            "epoch:  290  loss:  0.21812771260738373\n",
            "epoch:  291  loss:  0.21791933476924896\n",
            "epoch:  292  loss:  0.21771195530891418\n",
            "epoch:  293  loss:  0.2175055593252182\n",
            "epoch:  294  loss:  0.21730013191699982\n",
            "epoch:  295  loss:  0.21709567308425903\n",
            "epoch:  296  loss:  0.21689213812351227\n",
            "epoch:  297  loss:  0.2166895717382431\n",
            "epoch:  298  loss:  0.21648798882961273\n",
            "epoch:  299  loss:  0.21628734469413757\n",
            "epoch:  300  loss:  0.21608763933181763\n",
            "epoch:  301  loss:  0.21588891744613647\n",
            "epoch:  302  loss:  0.21569113433361053\n",
            "epoch:  303  loss:  0.21549426019191742\n",
            "epoch:  304  loss:  0.21529832482337952\n",
            "epoch:  305  loss:  0.21510334312915802\n",
            "epoch:  306  loss:  0.21490930020809174\n",
            "epoch:  307  loss:  0.21471615135669708\n",
            "epoch:  308  loss:  0.21452392637729645\n",
            "epoch:  309  loss:  0.21433265507221222\n",
            "epoch:  310  loss:  0.21414229273796082\n",
            "epoch:  311  loss:  0.21395280957221985\n",
            "epoch:  312  loss:  0.2137642651796341\n",
            "epoch:  313  loss:  0.21357662975788116\n",
            "epoch:  314  loss:  0.21338985860347748\n",
            "epoch:  315  loss:  0.2132040113210678\n",
            "epoch:  316  loss:  0.21301905810832977\n",
            "epoch:  317  loss:  0.21283501386642456\n",
            "epoch:  318  loss:  0.2126518338918686\n",
            "epoch:  319  loss:  0.21246957778930664\n",
            "epoch:  320  loss:  0.21228815615177155\n",
            "epoch:  321  loss:  0.21210762858390808\n",
            "epoch:  322  loss:  0.21192798018455505\n",
            "epoch:  323  loss:  0.21174922585487366\n",
            "epoch:  324  loss:  0.21157130599021912\n",
            "epoch:  325  loss:  0.2113942801952362\n",
            "epoch:  326  loss:  0.21121808886528015\n",
            "epoch:  327  loss:  0.21104280650615692\n",
            "epoch:  328  loss:  0.21086834371089935\n",
            "epoch:  329  loss:  0.21069473028182983\n",
            "epoch:  330  loss:  0.21052196621894836\n",
            "epoch:  331  loss:  0.21035005152225494\n",
            "epoch:  332  loss:  0.21017901599407196\n",
            "epoch:  333  loss:  0.21000875532627106\n",
            "epoch:  334  loss:  0.20983940362930298\n",
            "epoch:  335  loss:  0.2096708118915558\n",
            "epoch:  336  loss:  0.20950311422348022\n",
            "epoch:  337  loss:  0.20933620631694794\n",
            "epoch:  338  loss:  0.20917010307312012\n",
            "epoch:  339  loss:  0.20900484919548035\n",
            "epoch:  340  loss:  0.20884041488170624\n",
            "epoch:  341  loss:  0.2086767703294754\n",
            "epoch:  342  loss:  0.20851396024227142\n",
            "epoch:  343  loss:  0.2083519548177719\n",
            "epoch:  344  loss:  0.20819073915481567\n",
            "epoch:  345  loss:  0.2080303132534027\n",
            "epoch:  346  loss:  0.2078707069158554\n",
            "epoch:  347  loss:  0.20771189033985138\n",
            "epoch:  348  loss:  0.20755384862422943\n",
            "epoch:  349  loss:  0.20739658176898956\n",
            "epoch:  350  loss:  0.20724011957645416\n",
            "epoch:  351  loss:  0.20708444714546204\n",
            "epoch:  352  loss:  0.2069295197725296\n",
            "epoch:  353  loss:  0.20677538216114044\n",
            "epoch:  354  loss:  0.20662198960781097\n",
            "epoch:  355  loss:  0.20646938681602478\n",
            "epoch:  356  loss:  0.20631757378578186\n",
            "epoch:  357  loss:  0.20616652071475983\n",
            "epoch:  358  loss:  0.2060161530971527\n",
            "epoch:  359  loss:  0.20586660504341125\n",
            "epoch:  360  loss:  0.2057177722454071\n",
            "epoch:  361  loss:  0.20556969940662384\n",
            "epoch:  362  loss:  0.20542238652706146\n",
            "epoch:  363  loss:  0.20527580380439758\n",
            "epoch:  364  loss:  0.2051299661397934\n",
            "epoch:  365  loss:  0.2049848437309265\n",
            "epoch:  366  loss:  0.20484046638011932\n",
            "epoch:  367  loss:  0.20469678938388824\n",
            "epoch:  368  loss:  0.20455387234687805\n",
            "epoch:  369  loss:  0.20441165566444397\n",
            "epoch:  370  loss:  0.2042701542377472\n",
            "epoch:  371  loss:  0.2041293829679489\n",
            "epoch:  372  loss:  0.20398932695388794\n",
            "epoch:  373  loss:  0.20384997129440308\n",
            "epoch:  374  loss:  0.20371130108833313\n",
            "epoch:  375  loss:  0.2035733461380005\n",
            "epoch:  376  loss:  0.20343610644340515\n",
            "epoch:  377  loss:  0.20329955220222473\n",
            "epoch:  378  loss:  0.20316368341445923\n",
            "epoch:  379  loss:  0.20302850008010864\n",
            "epoch:  380  loss:  0.20289401710033417\n",
            "epoch:  381  loss:  0.20276020467281342\n",
            "epoch:  382  loss:  0.20262709259986877\n",
            "epoch:  383  loss:  0.20249465107917786\n",
            "epoch:  384  loss:  0.20236286520957947\n",
            "epoch:  385  loss:  0.2022317796945572\n",
            "epoch:  386  loss:  0.20210133492946625\n",
            "epoch:  387  loss:  0.20197154581546783\n",
            "epoch:  388  loss:  0.20184245705604553\n",
            "epoch:  389  loss:  0.20171399414539337\n",
            "epoch:  390  loss:  0.20158618688583374\n",
            "epoch:  391  loss:  0.20145905017852783\n",
            "epoch:  392  loss:  0.20133252441883087\n",
            "epoch:  393  loss:  0.20120666921138763\n",
            "epoch:  394  loss:  0.20108145475387573\n",
            "epoch:  395  loss:  0.20095689594745636\n",
            "epoch:  396  loss:  0.20083294808864594\n",
            "epoch:  397  loss:  0.20070964097976685\n",
            "epoch:  398  loss:  0.2005869597196579\n",
            "epoch:  399  loss:  0.2004649043083191\n",
            "epoch:  400  loss:  0.20034347474575043\n",
            "epoch:  401  loss:  0.2002226710319519\n",
            "epoch:  402  loss:  0.20010249316692352\n",
            "epoch:  403  loss:  0.1999829113483429\n",
            "epoch:  404  loss:  0.1998639553785324\n",
            "epoch:  405  loss:  0.19974564015865326\n",
            "epoch:  406  loss:  0.19962789118289948\n",
            "epoch:  407  loss:  0.19951072335243225\n",
            "epoch:  408  loss:  0.19939419627189636\n",
            "epoch:  409  loss:  0.19927826523780823\n",
            "epoch:  410  loss:  0.19916291534900665\n",
            "epoch:  411  loss:  0.19904813170433044\n",
            "epoch:  412  loss:  0.19893397390842438\n",
            "epoch:  413  loss:  0.19882038235664368\n",
            "epoch:  414  loss:  0.19870740175247192\n",
            "epoch:  415  loss:  0.19859495759010315\n",
            "epoch:  416  loss:  0.19848310947418213\n",
            "epoch:  417  loss:  0.19837182760238647\n",
            "epoch:  418  loss:  0.19826114177703857\n",
            "epoch:  419  loss:  0.19815102219581604\n",
            "epoch:  420  loss:  0.1980414241552353\n",
            "epoch:  421  loss:  0.1979324221611023\n",
            "epoch:  422  loss:  0.19782395660877228\n",
            "epoch:  423  loss:  0.19771605730056763\n",
            "epoch:  424  loss:  0.19760872423648834\n",
            "epoch:  425  loss:  0.19750192761421204\n",
            "epoch:  426  loss:  0.1973956674337387\n",
            "epoch:  427  loss:  0.19728994369506836\n",
            "epoch:  428  loss:  0.19718481600284576\n",
            "epoch:  429  loss:  0.19708018004894257\n",
            "epoch:  430  loss:  0.19697609543800354\n",
            "epoch:  431  loss:  0.1968725323677063\n",
            "epoch:  432  loss:  0.19676950573921204\n",
            "epoch:  433  loss:  0.19666700065135956\n",
            "epoch:  434  loss:  0.19656500220298767\n",
            "epoch:  435  loss:  0.19646355509757996\n",
            "epoch:  436  loss:  0.19636264443397522\n",
            "epoch:  437  loss:  0.19626222550868988\n",
            "epoch:  438  loss:  0.19616229832172394\n",
            "epoch:  439  loss:  0.19606290757656097\n",
            "epoch:  440  loss:  0.1959640234708786\n",
            "epoch:  441  loss:  0.19586563110351562\n",
            "epoch:  442  loss:  0.19576776027679443\n",
            "epoch:  443  loss:  0.19567038118839264\n",
            "epoch:  444  loss:  0.19557347893714905\n",
            "epoch:  445  loss:  0.19547709822654724\n",
            "epoch:  446  loss:  0.19538120925426483\n",
            "epoch:  447  loss:  0.19528576731681824\n",
            "epoch:  448  loss:  0.19519086182117462\n",
            "epoch:  449  loss:  0.1950964331626892\n",
            "epoch:  450  loss:  0.1950024515390396\n",
            "epoch:  451  loss:  0.1949089765548706\n",
            "epoch:  452  loss:  0.1948159635066986\n",
            "epoch:  453  loss:  0.19472342729568481\n",
            "epoch:  454  loss:  0.19463138282299042\n",
            "epoch:  455  loss:  0.19453977048397064\n",
            "epoch:  456  loss:  0.19444862008094788\n",
            "epoch:  457  loss:  0.1943579763174057\n",
            "epoch:  458  loss:  0.19426776468753815\n",
            "epoch:  459  loss:  0.1941780149936676\n",
            "epoch:  460  loss:  0.19408871233463287\n",
            "epoch:  461  loss:  0.19399982690811157\n",
            "epoch:  462  loss:  0.19391144812107086\n",
            "epoch:  463  loss:  0.19382351636886597\n",
            "epoch:  464  loss:  0.1937359869480133\n",
            "epoch:  465  loss:  0.19364891946315765\n",
            "epoch:  466  loss:  0.193562313914299\n",
            "epoch:  467  loss:  0.1934761255979538\n",
            "epoch:  468  loss:  0.193390354514122\n",
            "epoch:  469  loss:  0.19330503046512604\n",
            "epoch:  470  loss:  0.1932201236486435\n",
            "epoch:  471  loss:  0.19313566386699677\n",
            "epoch:  472  loss:  0.19305162131786346\n",
            "epoch:  473  loss:  0.1929679960012436\n",
            "epoch:  474  loss:  0.19288478791713715\n",
            "epoch:  475  loss:  0.19280198216438293\n",
            "epoch:  476  loss:  0.19271965324878693\n",
            "epoch:  477  loss:  0.19263766705989838\n",
            "epoch:  478  loss:  0.19255611300468445\n",
            "epoch:  479  loss:  0.19247499108314514\n",
            "epoch:  480  loss:  0.1923942118883133\n",
            "epoch:  481  loss:  0.19231389462947845\n",
            "epoch:  482  loss:  0.19223393499851227\n",
            "epoch:  483  loss:  0.1921543926000595\n",
            "epoch:  484  loss:  0.19207525253295898\n",
            "epoch:  485  loss:  0.1919964849948883\n",
            "epoch:  486  loss:  0.19191813468933105\n",
            "epoch:  487  loss:  0.19184015691280365\n",
            "epoch:  488  loss:  0.19176256656646729\n",
            "epoch:  489  loss:  0.19168536365032196\n",
            "epoch:  490  loss:  0.19160853326320648\n",
            "epoch:  491  loss:  0.19153207540512085\n",
            "epoch:  492  loss:  0.19145601987838745\n",
            "epoch:  493  loss:  0.1913803368806839\n",
            "epoch:  494  loss:  0.191305011510849\n",
            "epoch:  495  loss:  0.19123007357120514\n",
            "epoch:  496  loss:  0.19115547835826874\n",
            "epoch:  497  loss:  0.191081240773201\n",
            "epoch:  498  loss:  0.19100740551948547\n",
            "epoch:  499  loss:  0.19093391299247742\n",
            "epoch:  500  loss:  0.19086076319217682\n",
            "epoch:  501  loss:  0.19078798592090607\n",
            "epoch:  502  loss:  0.19071556627750397\n",
            "epoch:  503  loss:  0.19064348936080933\n",
            "epoch:  504  loss:  0.19057177007198334\n",
            "epoch:  505  loss:  0.1905003935098648\n",
            "epoch:  506  loss:  0.19042932987213135\n",
            "epoch:  507  loss:  0.19035866856575012\n",
            "epoch:  508  loss:  0.19028834998607635\n",
            "epoch:  509  loss:  0.19021831452846527\n",
            "epoch:  510  loss:  0.19014866650104523\n",
            "epoch:  511  loss:  0.19007933139801025\n",
            "epoch:  512  loss:  0.19001030921936035\n",
            "epoch:  513  loss:  0.1899416446685791\n",
            "epoch:  514  loss:  0.18987330794334412\n",
            "epoch:  515  loss:  0.1898053139448166\n",
            "epoch:  516  loss:  0.18973761796951294\n",
            "epoch:  517  loss:  0.18967026472091675\n",
            "epoch:  518  loss:  0.18960322439670563\n",
            "epoch:  519  loss:  0.18953648209571838\n",
            "epoch:  520  loss:  0.1894700825214386\n",
            "epoch:  521  loss:  0.18940399587154388\n",
            "epoch:  522  loss:  0.18933820724487305\n",
            "epoch:  523  loss:  0.18927273154258728\n",
            "epoch:  524  loss:  0.18920758366584778\n",
            "epoch:  525  loss:  0.18914273381233215\n",
            "epoch:  526  loss:  0.18907815217971802\n",
            "epoch:  527  loss:  0.18901392817497253\n",
            "epoch:  528  loss:  0.18894998729228973\n",
            "epoch:  529  loss:  0.1888863444328308\n",
            "epoch:  530  loss:  0.18882298469543457\n",
            "epoch:  531  loss:  0.1887599378824234\n",
            "epoch:  532  loss:  0.18869717419147491\n",
            "epoch:  533  loss:  0.1886347234249115\n",
            "epoch:  534  loss:  0.18857252597808838\n",
            "epoch:  535  loss:  0.18851067125797272\n",
            "epoch:  536  loss:  0.18844902515411377\n",
            "epoch:  537  loss:  0.18838772177696228\n",
            "epoch:  538  loss:  0.18832668662071228\n",
            "epoch:  539  loss:  0.18826591968536377\n",
            "epoch:  540  loss:  0.18820545077323914\n",
            "epoch:  541  loss:  0.1881452351808548\n",
            "epoch:  542  loss:  0.18808531761169434\n",
            "epoch:  543  loss:  0.18802566826343536\n",
            "epoch:  544  loss:  0.1879662722349167\n",
            "epoch:  545  loss:  0.1879071593284607\n",
            "epoch:  546  loss:  0.18784832954406738\n",
            "epoch:  547  loss:  0.18778973817825317\n",
            "epoch:  548  loss:  0.18773145973682404\n",
            "epoch:  549  loss:  0.1876734048128128\n",
            "epoch:  550  loss:  0.18761560320854187\n",
            "epoch:  551  loss:  0.18755806982517242\n",
            "epoch:  552  loss:  0.18750080466270447\n",
            "epoch:  553  loss:  0.1874437928199768\n",
            "epoch:  554  loss:  0.18738701939582825\n",
            "epoch:  555  loss:  0.18733049929141998\n",
            "epoch:  556  loss:  0.1872742623090744\n",
            "epoch:  557  loss:  0.18721826374530792\n",
            "epoch:  558  loss:  0.18716250360012054\n",
            "epoch:  559  loss:  0.18710696697235107\n",
            "epoch:  560  loss:  0.1870517134666443\n",
            "epoch:  561  loss:  0.1869966685771942\n",
            "epoch:  562  loss:  0.18694189190864563\n",
            "epoch:  563  loss:  0.18688733875751495\n",
            "epoch:  564  loss:  0.18683303892612457\n",
            "epoch:  565  loss:  0.1867789775133133\n",
            "epoch:  566  loss:  0.18672513961791992\n",
            "epoch:  567  loss:  0.18667154014110565\n",
            "epoch:  568  loss:  0.1866181641817093\n",
            "epoch:  569  loss:  0.18656502664089203\n",
            "epoch:  570  loss:  0.18651212751865387\n",
            "epoch:  571  loss:  0.18645943701267242\n",
            "epoch:  572  loss:  0.18640698492527008\n",
            "epoch:  573  loss:  0.18635474145412445\n",
            "epoch:  574  loss:  0.18630273640155792\n",
            "epoch:  575  loss:  0.1862509399652481\n",
            "epoch:  576  loss:  0.1861993670463562\n",
            "epoch:  577  loss:  0.1861480176448822\n",
            "epoch:  578  loss:  0.18609686195850372\n",
            "epoch:  579  loss:  0.18604597449302673\n",
            "epoch:  580  loss:  0.18599526584148407\n",
            "epoch:  581  loss:  0.18594476580619812\n",
            "epoch:  582  loss:  0.18589448928833008\n",
            "epoch:  583  loss:  0.18584442138671875\n",
            "epoch:  584  loss:  0.18579454720020294\n",
            "epoch:  585  loss:  0.18574488162994385\n",
            "epoch:  586  loss:  0.18569545447826385\n",
            "epoch:  587  loss:  0.185646191239357\n",
            "epoch:  588  loss:  0.18559715151786804\n",
            "epoch:  589  loss:  0.1855483055114746\n",
            "epoch:  590  loss:  0.1854996681213379\n",
            "epoch:  591  loss:  0.1854512244462967\n",
            "epoch:  592  loss:  0.185402974486351\n",
            "epoch:  593  loss:  0.18535494804382324\n",
            "epoch:  594  loss:  0.1853070706129074\n",
            "epoch:  595  loss:  0.1852594017982483\n",
            "epoch:  596  loss:  0.1852119117975235\n",
            "epoch:  597  loss:  0.1851646602153778\n",
            "epoch:  598  loss:  0.18511755764484406\n",
            "epoch:  599  loss:  0.18507066369056702\n",
            "epoch:  600  loss:  0.1850239336490631\n",
            "epoch:  601  loss:  0.18497741222381592\n",
            "epoch:  602  loss:  0.18493105471134186\n",
            "epoch:  603  loss:  0.18488489091396332\n",
            "epoch:  604  loss:  0.1848389208316803\n",
            "epoch:  605  loss:  0.1847931146621704\n",
            "epoch:  606  loss:  0.18474750220775604\n",
            "epoch:  607  loss:  0.1847020536661148\n",
            "epoch:  608  loss:  0.1846567988395691\n",
            "epoch:  609  loss:  0.18461169302463531\n",
            "epoch:  610  loss:  0.18456678092479706\n",
            "epoch:  611  loss:  0.18452203273773193\n",
            "epoch:  612  loss:  0.18447747826576233\n",
            "epoch:  613  loss:  0.18443307280540466\n",
            "epoch:  614  loss:  0.18438886106014252\n",
            "epoch:  615  loss:  0.1843447983264923\n",
            "epoch:  616  loss:  0.18430088460445404\n",
            "epoch:  617  loss:  0.1842571347951889\n",
            "epoch:  618  loss:  0.18421359360218048\n",
            "epoch:  619  loss:  0.1841701865196228\n",
            "epoch:  620  loss:  0.18412695825099945\n",
            "epoch:  621  loss:  0.18408389389514923\n",
            "epoch:  622  loss:  0.18404097855091095\n",
            "epoch:  623  loss:  0.1839982271194458\n",
            "epoch:  624  loss:  0.18395563960075378\n",
            "epoch:  625  loss:  0.1839132010936737\n",
            "epoch:  626  loss:  0.18387092649936676\n",
            "epoch:  627  loss:  0.18382880091667175\n",
            "epoch:  628  loss:  0.18378682434558868\n",
            "epoch:  629  loss:  0.18374501168727875\n",
            "epoch:  630  loss:  0.18370336294174194\n",
            "epoch:  631  loss:  0.18366184830665588\n",
            "epoch:  632  loss:  0.18362048268318176\n",
            "epoch:  633  loss:  0.1835792362689972\n",
            "epoch:  634  loss:  0.18353819847106934\n",
            "epoch:  635  loss:  0.18349726498126984\n",
            "epoch:  636  loss:  0.18345651030540466\n",
            "epoch:  637  loss:  0.18341585993766785\n",
            "epoch:  638  loss:  0.18337538838386536\n",
            "epoch:  639  loss:  0.18333503603935242\n",
            "epoch:  640  loss:  0.18329483270645142\n",
            "epoch:  641  loss:  0.18325479328632355\n",
            "epoch:  642  loss:  0.18321487307548523\n",
            "epoch:  643  loss:  0.18317507207393646\n",
            "epoch:  644  loss:  0.18313546478748322\n",
            "epoch:  645  loss:  0.18309594690799713\n",
            "epoch:  646  loss:  0.18305657804012299\n",
            "epoch:  647  loss:  0.18301734328269958\n",
            "epoch:  648  loss:  0.18297825753688812\n",
            "epoch:  649  loss:  0.1829392910003662\n",
            "epoch:  650  loss:  0.18290044367313385\n",
            "epoch:  651  loss:  0.18286176025867462\n",
            "epoch:  652  loss:  0.18282319605350494\n",
            "epoch:  653  loss:  0.18278475105762482\n",
            "epoch:  654  loss:  0.18274644017219543\n",
            "epoch:  655  loss:  0.1827082484960556\n",
            "epoch:  656  loss:  0.1826702058315277\n",
            "epoch:  657  loss:  0.18263228237628937\n",
            "epoch:  658  loss:  0.18259446322917938\n",
            "epoch:  659  loss:  0.18255680799484253\n",
            "epoch:  660  loss:  0.18251924216747284\n",
            "epoch:  661  loss:  0.1824818104505539\n",
            "epoch:  662  loss:  0.1824444830417633\n",
            "epoch:  663  loss:  0.18240733444690704\n",
            "epoch:  664  loss:  0.18237026035785675\n",
            "epoch:  665  loss:  0.1823333203792572\n",
            "epoch:  666  loss:  0.182296484708786\n",
            "epoch:  667  loss:  0.18225978314876556\n",
            "epoch:  668  loss:  0.18222317099571228\n",
            "epoch:  669  loss:  0.18218670785427094\n",
            "epoch:  670  loss:  0.18215033411979675\n",
            "epoch:  671  loss:  0.18211407959461212\n",
            "epoch:  672  loss:  0.18207795917987823\n",
            "epoch:  673  loss:  0.1820419430732727\n",
            "epoch:  674  loss:  0.18200603127479553\n",
            "epoch:  675  loss:  0.1819702386856079\n",
            "epoch:  676  loss:  0.18193453550338745\n",
            "epoch:  677  loss:  0.18189896643161774\n",
            "epoch:  678  loss:  0.18186353147029877\n",
            "epoch:  679  loss:  0.18182818591594696\n",
            "epoch:  680  loss:  0.18179292976856232\n",
            "epoch:  681  loss:  0.18175776302814484\n",
            "epoch:  682  loss:  0.1817227452993393\n",
            "epoch:  683  loss:  0.1816878318786621\n",
            "epoch:  684  loss:  0.1816530078649521\n",
            "epoch:  685  loss:  0.18161827325820923\n",
            "epoch:  686  loss:  0.1815836876630783\n",
            "epoch:  687  loss:  0.18154916167259216\n",
            "epoch:  688  loss:  0.18151476979255676\n",
            "epoch:  689  loss:  0.18148045241832733\n",
            "epoch:  690  loss:  0.18144625425338745\n",
            "epoch:  691  loss:  0.18141213059425354\n",
            "epoch:  692  loss:  0.18137812614440918\n",
            "epoch:  693  loss:  0.18134421110153198\n",
            "epoch:  694  loss:  0.18131044507026672\n",
            "epoch:  695  loss:  0.18127672374248505\n",
            "epoch:  696  loss:  0.18124310672283173\n",
            "epoch:  697  loss:  0.18120957911014557\n",
            "epoch:  698  loss:  0.18117615580558777\n",
            "epoch:  699  loss:  0.18114283680915833\n",
            "epoch:  700  loss:  0.18110962212085724\n",
            "epoch:  701  loss:  0.18107645213603973\n",
            "epoch:  702  loss:  0.18104343116283417\n",
            "epoch:  703  loss:  0.18101046979427338\n",
            "epoch:  704  loss:  0.18097759783267975\n",
            "epoch:  705  loss:  0.18094484508037567\n",
            "epoch:  706  loss:  0.18091215193271637\n",
            "epoch:  707  loss:  0.18087957799434662\n",
            "epoch:  708  loss:  0.18084706366062164\n",
            "epoch:  709  loss:  0.1808146834373474\n",
            "epoch:  710  loss:  0.18078233301639557\n",
            "epoch:  711  loss:  0.18075011670589447\n",
            "epoch:  712  loss:  0.18071796000003815\n",
            "epoch:  713  loss:  0.18068590760231018\n",
            "epoch:  714  loss:  0.18065392971038818\n",
            "epoch:  715  loss:  0.18062202632427216\n",
            "epoch:  716  loss:  0.18059024214744568\n",
            "epoch:  717  loss:  0.18055850267410278\n",
            "epoch:  718  loss:  0.18052686750888824\n",
            "epoch:  719  loss:  0.18049533665180206\n",
            "epoch:  720  loss:  0.18046385049819946\n",
            "epoch:  721  loss:  0.18043245375156403\n",
            "epoch:  722  loss:  0.18040114641189575\n",
            "epoch:  723  loss:  0.18036991357803345\n",
            "epoch:  724  loss:  0.1803387701511383\n",
            "epoch:  725  loss:  0.18030768632888794\n",
            "epoch:  726  loss:  0.18027673661708832\n",
            "epoch:  727  loss:  0.18024581670761108\n",
            "epoch:  728  loss:  0.1802150011062622\n",
            "epoch:  729  loss:  0.1801842451095581\n",
            "epoch:  730  loss:  0.18015356361865997\n",
            "epoch:  731  loss:  0.1801229566335678\n",
            "epoch:  732  loss:  0.1800924390554428\n",
            "epoch:  733  loss:  0.18006199598312378\n",
            "epoch:  734  loss:  0.18003161251544952\n",
            "epoch:  735  loss:  0.18000134825706482\n",
            "epoch:  736  loss:  0.1799711138010025\n",
            "epoch:  737  loss:  0.17994098365306854\n",
            "epoch:  738  loss:  0.17991088330745697\n",
            "epoch:  739  loss:  0.17988088726997375\n",
            "epoch:  740  loss:  0.1798509657382965\n",
            "epoch:  741  loss:  0.17982110381126404\n",
            "epoch:  742  loss:  0.17979131639003754\n",
            "epoch:  743  loss:  0.1797616183757782\n",
            "epoch:  744  loss:  0.17973196506500244\n",
            "epoch:  745  loss:  0.17970238626003265\n",
            "epoch:  746  loss:  0.17967288196086884\n",
            "epoch:  747  loss:  0.179643452167511\n",
            "epoch:  748  loss:  0.1796140819787979\n",
            "epoch:  749  loss:  0.179584801197052\n",
            "epoch:  750  loss:  0.17955555021762848\n",
            "epoch:  751  loss:  0.1795264184474945\n",
            "epoch:  752  loss:  0.17949731647968292\n",
            "epoch:  753  loss:  0.1794682890176773\n",
            "epoch:  754  loss:  0.17943933606147766\n",
            "epoch:  755  loss:  0.1794104278087616\n",
            "epoch:  756  loss:  0.1793815940618515\n",
            "epoch:  757  loss:  0.17935283482074738\n",
            "epoch:  758  loss:  0.17932412028312683\n",
            "epoch:  759  loss:  0.17929546535015106\n",
            "epoch:  760  loss:  0.17926694452762604\n",
            "epoch:  761  loss:  0.17923840880393982\n",
            "epoch:  762  loss:  0.17920996248722076\n",
            "epoch:  763  loss:  0.1791815608739853\n",
            "epoch:  764  loss:  0.17915326356887817\n",
            "epoch:  765  loss:  0.17912501096725464\n",
            "epoch:  766  loss:  0.17909680306911469\n",
            "epoch:  767  loss:  0.1790686696767807\n",
            "epoch:  768  loss:  0.1790405809879303\n",
            "epoch:  769  loss:  0.17901258170604706\n",
            "epoch:  770  loss:  0.178984597325325\n",
            "epoch:  771  loss:  0.17895673215389252\n",
            "epoch:  772  loss:  0.1789288967847824\n",
            "epoch:  773  loss:  0.17890112102031708\n",
            "epoch:  774  loss:  0.17887338995933533\n",
            "epoch:  775  loss:  0.17884573340415955\n",
            "epoch:  776  loss:  0.17881813645362854\n",
            "epoch:  777  loss:  0.17879058420658112\n",
            "epoch:  778  loss:  0.17876309156417847\n",
            "epoch:  779  loss:  0.1787356734275818\n",
            "epoch:  780  loss:  0.17870831489562988\n",
            "epoch:  781  loss:  0.17868097126483917\n",
            "epoch:  782  loss:  0.17865370213985443\n",
            "epoch:  783  loss:  0.17862649261951447\n",
            "epoch:  784  loss:  0.17859934270381927\n",
            "epoch:  785  loss:  0.17857226729393005\n",
            "epoch:  786  loss:  0.17854520678520203\n",
            "epoch:  787  loss:  0.17851822078227997\n",
            "epoch:  788  loss:  0.1784912645816803\n",
            "epoch:  789  loss:  0.1784643828868866\n",
            "epoch:  790  loss:  0.17843756079673767\n",
            "epoch:  791  loss:  0.17841078341007233\n",
            "epoch:  792  loss:  0.17838405072689056\n",
            "epoch:  793  loss:  0.17835740745067596\n",
            "epoch:  794  loss:  0.17833077907562256\n",
            "epoch:  795  loss:  0.17830421030521393\n",
            "epoch:  796  loss:  0.17827770113945007\n",
            "epoch:  797  loss:  0.1782512217760086\n",
            "epoch:  798  loss:  0.1782248169183731\n",
            "epoch:  799  loss:  0.1781984567642212\n",
            "epoch:  800  loss:  0.17817215621471405\n",
            "epoch:  801  loss:  0.1781458705663681\n",
            "epoch:  802  loss:  0.17811967432498932\n",
            "epoch:  803  loss:  0.17809349298477173\n",
            "epoch:  804  loss:  0.1780673861503601\n",
            "epoch:  805  loss:  0.17804132401943207\n",
            "epoch:  806  loss:  0.1780153065919876\n",
            "epoch:  807  loss:  0.17798931896686554\n",
            "epoch:  808  loss:  0.17796340584754944\n",
            "epoch:  809  loss:  0.1779375523328781\n",
            "epoch:  810  loss:  0.1779116988182068\n",
            "epoch:  811  loss:  0.17788593471050262\n",
            "epoch:  812  loss:  0.17786018550395966\n",
            "epoch:  813  loss:  0.17783449590206146\n",
            "epoch:  814  loss:  0.17780885100364685\n",
            "epoch:  815  loss:  0.1777832806110382\n",
            "epoch:  816  loss:  0.17775774002075195\n",
            "epoch:  817  loss:  0.17773222923278809\n",
            "epoch:  818  loss:  0.1777067482471466\n",
            "epoch:  819  loss:  0.1776813566684723\n",
            "epoch:  820  loss:  0.17765597999095917\n",
            "epoch:  821  loss:  0.17763064801692963\n",
            "epoch:  822  loss:  0.17760537564754486\n",
            "epoch:  823  loss:  0.1775801181793213\n",
            "epoch:  824  loss:  0.17755495011806488\n",
            "epoch:  825  loss:  0.17752978205680847\n",
            "epoch:  826  loss:  0.17750470340251923\n",
            "epoch:  827  loss:  0.1774796098470688\n",
            "epoch:  828  loss:  0.1774546056985855\n",
            "epoch:  829  loss:  0.17742964625358582\n",
            "epoch:  830  loss:  0.17740467190742493\n",
            "epoch:  831  loss:  0.1773798018693924\n",
            "epoch:  832  loss:  0.17735493183135986\n",
            "epoch:  833  loss:  0.1773301362991333\n",
            "epoch:  834  loss:  0.17730537056922913\n",
            "epoch:  835  loss:  0.17728063464164734\n",
            "epoch:  836  loss:  0.17725597321987152\n",
            "epoch:  837  loss:  0.1772313117980957\n",
            "epoch:  838  loss:  0.17720670998096466\n",
            "epoch:  839  loss:  0.1771821528673172\n",
            "epoch:  840  loss:  0.17715761065483093\n",
            "epoch:  841  loss:  0.17713314294815063\n",
            "epoch:  842  loss:  0.17710869014263153\n",
            "epoch:  843  loss:  0.1770842969417572\n",
            "epoch:  844  loss:  0.17705991864204407\n",
            "epoch:  845  loss:  0.1770355999469757\n",
            "epoch:  846  loss:  0.17701132595539093\n",
            "epoch:  847  loss:  0.17698706686496735\n",
            "epoch:  848  loss:  0.17696286737918854\n",
            "epoch:  849  loss:  0.17693868279457092\n",
            "epoch:  850  loss:  0.17691455781459808\n",
            "epoch:  851  loss:  0.17689044773578644\n",
            "epoch:  852  loss:  0.17686639726161957\n",
            "epoch:  853  loss:  0.17684239149093628\n",
            "epoch:  854  loss:  0.176818385720253\n",
            "epoch:  855  loss:  0.17679445445537567\n",
            "epoch:  856  loss:  0.17677053809165955\n",
            "epoch:  857  loss:  0.176746666431427\n",
            "epoch:  858  loss:  0.17672285437583923\n",
            "epoch:  859  loss:  0.17669904232025146\n",
            "epoch:  860  loss:  0.17667526006698608\n",
            "epoch:  861  loss:  0.17665156722068787\n",
            "epoch:  862  loss:  0.17662787437438965\n",
            "epoch:  863  loss:  0.17660421133041382\n",
            "epoch:  864  loss:  0.17658059298992157\n",
            "epoch:  865  loss:  0.1765570044517517\n",
            "epoch:  866  loss:  0.17653346061706543\n",
            "epoch:  867  loss:  0.17650993168354034\n",
            "epoch:  868  loss:  0.17648647725582123\n",
            "epoch:  869  loss:  0.17646300792694092\n",
            "epoch:  870  loss:  0.17643961310386658\n",
            "epoch:  871  loss:  0.17641624808311462\n",
            "epoch:  872  loss:  0.17639289796352386\n",
            "epoch:  873  loss:  0.1763695925474167\n",
            "epoch:  874  loss:  0.1763463318347931\n",
            "epoch:  875  loss:  0.1763230562210083\n",
            "epoch:  876  loss:  0.17629988491535187\n",
            "epoch:  877  loss:  0.17627669870853424\n",
            "epoch:  878  loss:  0.1762535572052002\n",
            "epoch:  879  loss:  0.17623046040534973\n",
            "epoch:  880  loss:  0.17620739340782166\n",
            "epoch:  881  loss:  0.17618435621261597\n",
            "epoch:  882  loss:  0.17616133391857147\n",
            "epoch:  883  loss:  0.17613835632801056\n",
            "epoch:  884  loss:  0.17611540853977203\n",
            "epoch:  885  loss:  0.1760925054550171\n",
            "epoch:  886  loss:  0.17606964707374573\n",
            "epoch:  887  loss:  0.17604678869247437\n",
            "epoch:  888  loss:  0.17602397501468658\n",
            "epoch:  889  loss:  0.1760011911392212\n",
            "epoch:  890  loss:  0.175978422164917\n",
            "epoch:  891  loss:  0.17595569789409637\n",
            "epoch:  892  loss:  0.17593301832675934\n",
            "epoch:  893  loss:  0.1759103536605835\n",
            "epoch:  894  loss:  0.17588771879673004\n",
            "epoch:  895  loss:  0.17586511373519897\n",
            "epoch:  896  loss:  0.1758425533771515\n",
            "epoch:  897  loss:  0.1758200228214264\n",
            "epoch:  898  loss:  0.17579752206802368\n",
            "epoch:  899  loss:  0.17577505111694336\n",
            "epoch:  900  loss:  0.17575259506702423\n",
            "epoch:  901  loss:  0.17573018372058868\n",
            "epoch:  902  loss:  0.17570780217647552\n",
            "epoch:  903  loss:  0.17568542063236237\n",
            "epoch:  904  loss:  0.17566311359405518\n",
            "epoch:  905  loss:  0.17564083635807037\n",
            "epoch:  906  loss:  0.17561854422092438\n",
            "epoch:  907  loss:  0.17559631168842316\n",
            "epoch:  908  loss:  0.17557409405708313\n",
            "epoch:  909  loss:  0.1755519062280655\n",
            "epoch:  910  loss:  0.17552977800369263\n",
            "epoch:  911  loss:  0.17550766468048096\n",
            "epoch:  912  loss:  0.1754855513572693\n",
            "epoch:  913  loss:  0.1754634827375412\n",
            "epoch:  914  loss:  0.17544147372245789\n",
            "epoch:  915  loss:  0.17541944980621338\n",
            "epoch:  916  loss:  0.17539745569229126\n",
            "epoch:  917  loss:  0.17537550628185272\n",
            "epoch:  918  loss:  0.17535358667373657\n",
            "epoch:  919  loss:  0.1753316968679428\n",
            "epoch:  920  loss:  0.17530980706214905\n",
            "epoch:  921  loss:  0.17528797686100006\n",
            "epoch:  922  loss:  0.17526616156101227\n",
            "epoch:  923  loss:  0.17524439096450806\n",
            "epoch:  924  loss:  0.17522260546684265\n",
            "epoch:  925  loss:  0.17520087957382202\n",
            "epoch:  926  loss:  0.17517916858196259\n",
            "epoch:  927  loss:  0.17515748739242554\n",
            "epoch:  928  loss:  0.17513583600521088\n",
            "epoch:  929  loss:  0.1751141995191574\n",
            "epoch:  930  loss:  0.17509260773658752\n",
            "epoch:  931  loss:  0.17507104575634003\n",
            "epoch:  932  loss:  0.17504949867725372\n",
            "epoch:  933  loss:  0.17502795159816742\n",
            "epoch:  934  loss:  0.1750064492225647\n",
            "epoch:  935  loss:  0.17498500645160675\n",
            "epoch:  936  loss:  0.1749635487794876\n",
            "epoch:  937  loss:  0.17494213581085205\n",
            "epoch:  938  loss:  0.17492075264453888\n",
            "epoch:  939  loss:  0.1748994141817093\n",
            "epoch:  940  loss:  0.1748780459165573\n",
            "epoch:  941  loss:  0.1748567521572113\n",
            "epoch:  942  loss:  0.1748354434967041\n",
            "epoch:  943  loss:  0.1748141646385193\n",
            "epoch:  944  loss:  0.17479296028614044\n",
            "epoch:  945  loss:  0.1747717410326004\n",
            "epoch:  946  loss:  0.17475055158138275\n",
            "epoch:  947  loss:  0.1747293919324875\n",
            "epoch:  948  loss:  0.17470824718475342\n",
            "epoch:  949  loss:  0.17468711733818054\n",
            "epoch:  950  loss:  0.17466604709625244\n",
            "epoch:  951  loss:  0.17464497685432434\n",
            "epoch:  952  loss:  0.17462393641471863\n",
            "epoch:  953  loss:  0.1746029257774353\n",
            "epoch:  954  loss:  0.17458194494247437\n",
            "epoch:  955  loss:  0.17456096410751343\n",
            "epoch:  956  loss:  0.17454002797603607\n",
            "epoch:  957  loss:  0.1745191067457199\n",
            "epoch:  958  loss:  0.17449818551540375\n",
            "epoch:  959  loss:  0.17447732388973236\n",
            "epoch:  960  loss:  0.17445647716522217\n",
            "epoch:  961  loss:  0.17443567514419556\n",
            "epoch:  962  loss:  0.17441485822200775\n",
            "epoch:  963  loss:  0.17439405620098114\n",
            "epoch:  964  loss:  0.17437335848808289\n",
            "epoch:  965  loss:  0.17435260117053986\n",
            "epoch:  966  loss:  0.1743319034576416\n",
            "epoch:  967  loss:  0.17431122064590454\n",
            "epoch:  968  loss:  0.17429055273532867\n",
            "epoch:  969  loss:  0.174269899725914\n",
            "epoch:  970  loss:  0.1742492914199829\n",
            "epoch:  971  loss:  0.17422868311405182\n",
            "epoch:  972  loss:  0.1742081195116043\n",
            "epoch:  973  loss:  0.1741875857114792\n",
            "epoch:  974  loss:  0.17416705191135406\n",
            "epoch:  975  loss:  0.17414654791355133\n",
            "epoch:  976  loss:  0.17412607371807098\n",
            "epoch:  977  loss:  0.17410562932491302\n",
            "epoch:  978  loss:  0.17408518493175507\n",
            "epoch:  979  loss:  0.1740647703409195\n",
            "epoch:  980  loss:  0.1740443855524063\n",
            "epoch:  981  loss:  0.17402401566505432\n",
            "epoch:  982  loss:  0.17400367558002472\n",
            "epoch:  983  loss:  0.1739833503961563\n",
            "epoch:  984  loss:  0.1739630401134491\n",
            "epoch:  985  loss:  0.17394277453422546\n",
            "epoch:  986  loss:  0.17392250895500183\n",
            "epoch:  987  loss:  0.17390227317810059\n",
            "epoch:  988  loss:  0.17388208210468292\n",
            "epoch:  989  loss:  0.17386187613010406\n",
            "epoch:  990  loss:  0.1738416999578476\n",
            "epoch:  991  loss:  0.1738215684890747\n",
            "epoch:  992  loss:  0.17380142211914062\n",
            "epoch:  993  loss:  0.17378133535385132\n",
            "epoch:  994  loss:  0.173761248588562\n",
            "epoch:  995  loss:  0.1737411767244339\n",
            "epoch:  996  loss:  0.17372113466262817\n",
            "epoch:  997  loss:  0.17370110750198364\n",
            "epoch:  998  loss:  0.1736811250448227\n",
            "epoch:  999  loss:  0.17366111278533936\n",
            "epoch:  1000  loss:  0.1736411452293396\n",
            "epoch:  1001  loss:  0.17362122237682343\n",
            "epoch:  1002  loss:  0.17360129952430725\n",
            "epoch:  1003  loss:  0.17358139157295227\n",
            "epoch:  1004  loss:  0.17356152832508087\n",
            "epoch:  1005  loss:  0.17354166507720947\n",
            "epoch:  1006  loss:  0.17352183163166046\n",
            "epoch:  1007  loss:  0.17350202798843384\n",
            "epoch:  1008  loss:  0.1734822392463684\n",
            "epoch:  1009  loss:  0.17346246540546417\n",
            "epoch:  1010  loss:  0.17344270646572113\n",
            "epoch:  1011  loss:  0.17342297732830048\n",
            "epoch:  1012  loss:  0.1734032779932022\n",
            "epoch:  1013  loss:  0.17338356375694275\n",
            "epoch:  1014  loss:  0.17336389422416687\n",
            "epoch:  1015  loss:  0.17334423959255219\n",
            "epoch:  1016  loss:  0.1733245998620987\n",
            "epoch:  1017  loss:  0.1733049899339676\n",
            "epoch:  1018  loss:  0.17328539490699768\n",
            "epoch:  1019  loss:  0.17326584458351135\n",
            "epoch:  1020  loss:  0.17324627935886383\n",
            "epoch:  1021  loss:  0.1732267290353775\n",
            "epoch:  1022  loss:  0.17320720851421356\n",
            "epoch:  1023  loss:  0.173187717795372\n",
            "epoch:  1024  loss:  0.17316825687885284\n",
            "epoch:  1025  loss:  0.17314878106117249\n",
            "epoch:  1026  loss:  0.1731293499469757\n",
            "epoch:  1027  loss:  0.17310993373394012\n",
            "epoch:  1028  loss:  0.17309054732322693\n",
            "epoch:  1029  loss:  0.17307114601135254\n",
            "epoch:  1030  loss:  0.17305178940296173\n",
            "epoch:  1031  loss:  0.1730324625968933\n",
            "epoch:  1032  loss:  0.1730131357908249\n",
            "epoch:  1033  loss:  0.17299385368824005\n",
            "epoch:  1034  loss:  0.17297455668449402\n",
            "epoch:  1035  loss:  0.17295528948307037\n",
            "epoch:  1036  loss:  0.17293605208396912\n",
            "epoch:  1037  loss:  0.17291682958602905\n",
            "epoch:  1038  loss:  0.17289762198925018\n",
            "epoch:  1039  loss:  0.1728784143924713\n",
            "epoch:  1040  loss:  0.17285925149917603\n",
            "epoch:  1041  loss:  0.17284008860588074\n",
            "epoch:  1042  loss:  0.17282095551490784\n",
            "epoch:  1043  loss:  0.17280185222625732\n",
            "epoch:  1044  loss:  0.1727827787399292\n",
            "epoch:  1045  loss:  0.1727636754512787\n",
            "epoch:  1046  loss:  0.17274463176727295\n",
            "epoch:  1047  loss:  0.1727255880832672\n",
            "epoch:  1048  loss:  0.17270655930042267\n",
            "epoch:  1049  loss:  0.1726875603199005\n",
            "epoch:  1050  loss:  0.17266857624053955\n",
            "epoch:  1051  loss:  0.17264960706233978\n",
            "epoch:  1052  loss:  0.1726306527853012\n",
            "epoch:  1053  loss:  0.17261172831058502\n",
            "epoch:  1054  loss:  0.17259281873703003\n",
            "epoch:  1055  loss:  0.17257390916347504\n",
            "epoch:  1056  loss:  0.17255504429340363\n",
            "epoch:  1057  loss:  0.17253617942333221\n",
            "epoch:  1058  loss:  0.17251735925674438\n",
            "epoch:  1059  loss:  0.17249850928783417\n",
            "epoch:  1060  loss:  0.17247971892356873\n",
            "epoch:  1061  loss:  0.17246094346046448\n",
            "epoch:  1062  loss:  0.17244215309619904\n",
            "epoch:  1063  loss:  0.17242340743541718\n",
            "epoch:  1064  loss:  0.1724046766757965\n",
            "epoch:  1065  loss:  0.17238596081733704\n",
            "epoch:  1066  loss:  0.17236727476119995\n",
            "epoch:  1067  loss:  0.17234857380390167\n",
            "epoch:  1068  loss:  0.17232993245124817\n",
            "epoch:  1069  loss:  0.17231127619743347\n",
            "epoch:  1070  loss:  0.17229266464710236\n",
            "epoch:  1071  loss:  0.17227403819561005\n",
            "epoch:  1072  loss:  0.17225545644760132\n",
            "epoch:  1073  loss:  0.1722368597984314\n",
            "epoch:  1074  loss:  0.17221832275390625\n",
            "epoch:  1075  loss:  0.1721997708082199\n",
            "epoch:  1076  loss:  0.17218124866485596\n",
            "epoch:  1077  loss:  0.1721627414226532\n",
            "epoch:  1078  loss:  0.17214423418045044\n",
            "epoch:  1079  loss:  0.17212578654289246\n",
            "epoch:  1080  loss:  0.17210730910301208\n",
            "epoch:  1081  loss:  0.1720888763666153\n",
            "epoch:  1082  loss:  0.1720704436302185\n",
            "epoch:  1083  loss:  0.1720520555973053\n",
            "epoch:  1084  loss:  0.17203368246555328\n",
            "epoch:  1085  loss:  0.17201532423496246\n",
            "epoch:  1086  loss:  0.17199695110321045\n",
            "epoch:  1087  loss:  0.17197862267494202\n",
            "epoch:  1088  loss:  0.17196029424667358\n",
            "epoch:  1089  loss:  0.17194199562072754\n",
            "epoch:  1090  loss:  0.1719237118959427\n",
            "epoch:  1091  loss:  0.17190545797348022\n",
            "epoch:  1092  loss:  0.17188718914985657\n",
            "epoch:  1093  loss:  0.1718689501285553\n",
            "epoch:  1094  loss:  0.17185074090957642\n",
            "epoch:  1095  loss:  0.17183253169059753\n",
            "epoch:  1096  loss:  0.17181435227394104\n",
            "epoch:  1097  loss:  0.17179617285728455\n",
            "epoch:  1098  loss:  0.17177802324295044\n",
            "epoch:  1099  loss:  0.17175987362861633\n",
            "epoch:  1100  loss:  0.17174175381660461\n",
            "epoch:  1101  loss:  0.1717236489057541\n",
            "epoch:  1102  loss:  0.17170558869838715\n",
            "epoch:  1103  loss:  0.17168749868869781\n",
            "epoch:  1104  loss:  0.17166945338249207\n",
            "epoch:  1105  loss:  0.17165139317512512\n",
            "epoch:  1106  loss:  0.17163339257240295\n",
            "epoch:  1107  loss:  0.1716153770685196\n",
            "epoch:  1108  loss:  0.17159739136695862\n",
            "epoch:  1109  loss:  0.17157940566539764\n",
            "epoch:  1110  loss:  0.17156144976615906\n",
            "epoch:  1111  loss:  0.17154353857040405\n",
            "epoch:  1112  loss:  0.17152558267116547\n",
            "epoch:  1113  loss:  0.17150767147541046\n",
            "epoch:  1114  loss:  0.17148979008197784\n",
            "epoch:  1115  loss:  0.17147189378738403\n",
            "epoch:  1116  loss:  0.1714540421962738\n",
            "epoch:  1117  loss:  0.17143620550632477\n",
            "epoch:  1118  loss:  0.17141836881637573\n",
            "epoch:  1119  loss:  0.17140056192874908\n",
            "epoch:  1120  loss:  0.17138276994228363\n",
            "epoch:  1121  loss:  0.17136497795581818\n",
            "epoch:  1122  loss:  0.17134720087051392\n",
            "epoch:  1123  loss:  0.17132943868637085\n",
            "epoch:  1124  loss:  0.17131170630455017\n",
            "epoch:  1125  loss:  0.17129400372505188\n",
            "epoch:  1126  loss:  0.1712762713432312\n",
            "epoch:  1127  loss:  0.1712585985660553\n",
            "epoch:  1128  loss:  0.1712409257888794\n",
            "epoch:  1129  loss:  0.17122326791286469\n",
            "epoch:  1130  loss:  0.17120562493801117\n",
            "epoch:  1131  loss:  0.17118798196315765\n",
            "epoch:  1132  loss:  0.17117038369178772\n",
            "epoch:  1133  loss:  0.17115278542041779\n",
            "epoch:  1134  loss:  0.17113520205020905\n",
            "epoch:  1135  loss:  0.1711176335811615\n",
            "epoch:  1136  loss:  0.17110008001327515\n",
            "epoch:  1137  loss:  0.17108257114887238\n",
            "epoch:  1138  loss:  0.17106503248214722\n",
            "epoch:  1139  loss:  0.17104750871658325\n",
            "epoch:  1140  loss:  0.17103002965450287\n",
            "epoch:  1141  loss:  0.17101259529590607\n",
            "epoch:  1142  loss:  0.1709951013326645\n",
            "epoch:  1143  loss:  0.1709776669740677\n",
            "epoch:  1144  loss:  0.1709602326154709\n",
            "epoch:  1145  loss:  0.17094281315803528\n",
            "epoch:  1146  loss:  0.17092540860176086\n",
            "epoch:  1147  loss:  0.17090803384780884\n",
            "epoch:  1148  loss:  0.170890673995018\n",
            "epoch:  1149  loss:  0.17087331414222717\n",
            "epoch:  1150  loss:  0.17085596919059753\n",
            "epoch:  1151  loss:  0.17083865404129028\n",
            "epoch:  1152  loss:  0.17082135379314423\n",
            "epoch:  1153  loss:  0.17080405354499817\n",
            "epoch:  1154  loss:  0.1707867681980133\n",
            "epoch:  1155  loss:  0.17076951265335083\n",
            "epoch:  1156  loss:  0.17075225710868835\n",
            "epoch:  1157  loss:  0.17073504626750946\n",
            "epoch:  1158  loss:  0.17071782052516937\n",
            "epoch:  1159  loss:  0.17070059478282928\n",
            "epoch:  1160  loss:  0.17068341374397278\n",
            "epoch:  1161  loss:  0.17066623270511627\n",
            "epoch:  1162  loss:  0.17064908146858215\n",
            "epoch:  1163  loss:  0.17063193023204803\n",
            "epoch:  1164  loss:  0.1706148087978363\n",
            "epoch:  1165  loss:  0.17059768736362457\n",
            "epoch:  1166  loss:  0.17058056592941284\n",
            "epoch:  1167  loss:  0.1705634891986847\n",
            "epoch:  1168  loss:  0.17054641246795654\n",
            "epoch:  1169  loss:  0.1705293506383896\n",
            "epoch:  1170  loss:  0.17051230370998383\n",
            "epoch:  1171  loss:  0.17049527168273926\n",
            "epoch:  1172  loss:  0.17047825455665588\n",
            "epoch:  1173  loss:  0.1704612523317337\n",
            "epoch:  1174  loss:  0.1704442799091339\n",
            "epoch:  1175  loss:  0.17042729258537292\n",
            "epoch:  1176  loss:  0.17041033506393433\n",
            "epoch:  1177  loss:  0.17039340734481812\n",
            "epoch:  1178  loss:  0.1703764647245407\n",
            "epoch:  1179  loss:  0.1703595519065857\n",
            "epoch:  1180  loss:  0.17034265398979187\n",
            "epoch:  1181  loss:  0.17032577097415924\n",
            "epoch:  1182  loss:  0.1703088879585266\n",
            "epoch:  1183  loss:  0.17029203474521637\n",
            "epoch:  1184  loss:  0.17027519643306732\n",
            "epoch:  1185  loss:  0.17025835812091827\n",
            "epoch:  1186  loss:  0.1702415496110916\n",
            "epoch:  1187  loss:  0.17022474110126495\n",
            "epoch:  1188  loss:  0.17020796239376068\n",
            "epoch:  1189  loss:  0.1701911985874176\n",
            "epoch:  1190  loss:  0.17017441987991333\n",
            "epoch:  1191  loss:  0.17015768587589264\n",
            "epoch:  1192  loss:  0.17014096677303314\n",
            "epoch:  1193  loss:  0.17012423276901245\n",
            "epoch:  1194  loss:  0.17010752856731415\n",
            "epoch:  1195  loss:  0.17009083926677704\n",
            "epoch:  1196  loss:  0.17007416486740112\n",
            "epoch:  1197  loss:  0.1700574904680252\n",
            "epoch:  1198  loss:  0.17004086077213287\n",
            "epoch:  1199  loss:  0.17002421617507935\n",
            "epoch:  1200  loss:  0.1700076013803482\n",
            "epoch:  1201  loss:  0.16999100148677826\n",
            "epoch:  1202  loss:  0.1699744015932083\n",
            "epoch:  1203  loss:  0.16995783150196075\n",
            "epoch:  1204  loss:  0.16994129121303558\n",
            "epoch:  1205  loss:  0.16992472112178802\n",
            "epoch:  1206  loss:  0.16990816593170166\n",
            "epoch:  1207  loss:  0.16989165544509888\n",
            "epoch:  1208  loss:  0.1698751598596573\n",
            "epoch:  1209  loss:  0.1698586642742157\n",
            "epoch:  1210  loss:  0.1698421835899353\n",
            "epoch:  1211  loss:  0.1698257178068161\n",
            "epoch:  1212  loss:  0.1698092520236969\n",
            "epoch:  1213  loss:  0.16979281604290009\n",
            "epoch:  1214  loss:  0.16977638006210327\n",
            "epoch:  1215  loss:  0.16975994408130646\n",
            "epoch:  1216  loss:  0.1697435826063156\n",
            "epoch:  1217  loss:  0.16972716152668\n",
            "epoch:  1218  loss:  0.16971080005168915\n",
            "epoch:  1219  loss:  0.1696944385766983\n",
            "epoch:  1220  loss:  0.16967810690402985\n",
            "epoch:  1221  loss:  0.1696617752313614\n",
            "epoch:  1222  loss:  0.16964544355869293\n",
            "epoch:  1223  loss:  0.16962914168834686\n",
            "epoch:  1224  loss:  0.169612854719162\n",
            "epoch:  1225  loss:  0.16959655284881592\n",
            "epoch:  1226  loss:  0.16958031058311462\n",
            "epoch:  1227  loss:  0.16956405341625214\n",
            "epoch:  1228  loss:  0.16954782605171204\n",
            "epoch:  1229  loss:  0.16953158378601074\n",
            "epoch:  1230  loss:  0.16951538622379303\n",
            "epoch:  1231  loss:  0.16949917376041412\n",
            "epoch:  1232  loss:  0.1694829910993576\n",
            "epoch:  1233  loss:  0.16946682333946228\n",
            "epoch:  1234  loss:  0.16945065557956696\n",
            "epoch:  1235  loss:  0.16943451762199402\n",
            "epoch:  1236  loss:  0.16941839456558228\n",
            "epoch:  1237  loss:  0.16940227150917053\n",
            "epoch:  1238  loss:  0.1693861335515976\n",
            "epoch:  1239  loss:  0.16937005519866943\n",
            "epoch:  1240  loss:  0.16935399174690247\n",
            "epoch:  1241  loss:  0.1693379282951355\n",
            "epoch:  1242  loss:  0.16932186484336853\n",
            "epoch:  1243  loss:  0.16930581629276276\n",
            "epoch:  1244  loss:  0.16928978264331818\n",
            "epoch:  1245  loss:  0.16927377879619598\n",
            "epoch:  1246  loss:  0.1692577749490738\n",
            "epoch:  1247  loss:  0.1692417860031128\n",
            "epoch:  1248  loss:  0.169225811958313\n",
            "epoch:  1249  loss:  0.16920983791351318\n",
            "epoch:  1250  loss:  0.16919389367103577\n",
            "epoch:  1251  loss:  0.16917794942855835\n",
            "epoch:  1252  loss:  0.16916203498840332\n",
            "epoch:  1253  loss:  0.16914613544940948\n",
            "epoch:  1254  loss:  0.16913023591041565\n",
            "epoch:  1255  loss:  0.16911433637142181\n",
            "epoch:  1256  loss:  0.16909846663475037\n",
            "epoch:  1257  loss:  0.1690826117992401\n",
            "epoch:  1258  loss:  0.16906675696372986\n",
            "epoch:  1259  loss:  0.169050931930542\n",
            "epoch:  1260  loss:  0.16903509199619293\n",
            "epoch:  1261  loss:  0.16901929676532745\n",
            "epoch:  1262  loss:  0.1690034717321396\n",
            "epoch:  1263  loss:  0.1689877063035965\n",
            "epoch:  1264  loss:  0.1689719557762146\n",
            "epoch:  1265  loss:  0.1689561903476715\n",
            "epoch:  1266  loss:  0.16894042491912842\n",
            "epoch:  1267  loss:  0.1689247339963913\n",
            "epoch:  1268  loss:  0.1689089983701706\n",
            "epoch:  1269  loss:  0.16889329254627228\n",
            "epoch:  1270  loss:  0.16887757182121277\n",
            "epoch:  1271  loss:  0.16886191070079803\n",
            "epoch:  1272  loss:  0.1688462346792221\n",
            "epoch:  1273  loss:  0.16883058845996857\n",
            "epoch:  1274  loss:  0.16881494224071503\n",
            "epoch:  1275  loss:  0.1687992960214615\n",
            "epoch:  1276  loss:  0.16878369450569153\n",
            "epoch:  1277  loss:  0.16876807808876038\n",
            "epoch:  1278  loss:  0.1687524914741516\n",
            "epoch:  1279  loss:  0.16873690485954285\n",
            "epoch:  1280  loss:  0.16872133314609528\n",
            "epoch:  1281  loss:  0.1687057763338089\n",
            "epoch:  1282  loss:  0.1686902493238449\n",
            "epoch:  1283  loss:  0.16867470741271973\n",
            "epoch:  1284  loss:  0.16865919530391693\n",
            "epoch:  1285  loss:  0.16864368319511414\n",
            "epoch:  1286  loss:  0.16862820088863373\n",
            "epoch:  1287  loss:  0.16861270368099213\n",
            "epoch:  1288  loss:  0.1685972362756729\n",
            "epoch:  1289  loss:  0.1685817837715149\n",
            "epoch:  1290  loss:  0.16856634616851807\n",
            "epoch:  1291  loss:  0.16855090856552124\n",
            "epoch:  1292  loss:  0.1685354858636856\n",
            "epoch:  1293  loss:  0.16852006316184998\n",
            "epoch:  1294  loss:  0.16850468516349792\n",
            "epoch:  1295  loss:  0.16848929226398468\n",
            "epoch:  1296  loss:  0.16847394406795502\n",
            "epoch:  1297  loss:  0.16845855116844177\n",
            "epoch:  1298  loss:  0.1684432178735733\n",
            "epoch:  1299  loss:  0.16842786967754364\n",
            "epoch:  1300  loss:  0.16841255128383636\n",
            "epoch:  1301  loss:  0.16839724779129028\n",
            "epoch:  1302  loss:  0.1683819442987442\n",
            "epoch:  1303  loss:  0.16836664080619812\n",
            "epoch:  1304  loss:  0.16835138201713562\n",
            "epoch:  1305  loss:  0.16833613812923431\n",
            "epoch:  1306  loss:  0.16832087934017181\n",
            "epoch:  1307  loss:  0.1683056354522705\n",
            "epoch:  1308  loss:  0.1682904213666916\n",
            "epoch:  1309  loss:  0.16827519237995148\n",
            "epoch:  1310  loss:  0.16825999319553375\n",
            "epoch:  1311  loss:  0.16824480891227722\n",
            "epoch:  1312  loss:  0.1682296246290207\n",
            "epoch:  1313  loss:  0.16821445524692535\n",
            "epoch:  1314  loss:  0.1681993156671524\n",
            "epoch:  1315  loss:  0.16818417608737946\n",
            "epoch:  1316  loss:  0.1681690365076065\n",
            "epoch:  1317  loss:  0.16815392673015594\n",
            "epoch:  1318  loss:  0.1681388020515442\n",
            "epoch:  1319  loss:  0.1681237369775772\n",
            "epoch:  1320  loss:  0.16810864210128784\n",
            "epoch:  1321  loss:  0.16809356212615967\n",
            "epoch:  1322  loss:  0.16807852685451508\n",
            "epoch:  1323  loss:  0.1680634766817093\n",
            "epoch:  1324  loss:  0.1680484265089035\n",
            "epoch:  1325  loss:  0.1680334210395813\n",
            "epoch:  1326  loss:  0.1680183857679367\n",
            "epoch:  1327  loss:  0.1680034101009369\n",
            "epoch:  1328  loss:  0.16798840463161469\n",
            "epoch:  1329  loss:  0.16797342896461487\n",
            "epoch:  1330  loss:  0.16795848309993744\n",
            "epoch:  1331  loss:  0.16794350743293762\n",
            "epoch:  1332  loss:  0.1679285764694214\n",
            "epoch:  1333  loss:  0.16791363060474396\n",
            "epoch:  1334  loss:  0.1678987443447113\n",
            "epoch:  1335  loss:  0.16788381338119507\n",
            "epoch:  1336  loss:  0.1678689420223236\n",
            "epoch:  1337  loss:  0.16785404086112976\n",
            "epoch:  1338  loss:  0.1678391695022583\n",
            "epoch:  1339  loss:  0.16782432794570923\n",
            "epoch:  1340  loss:  0.16780947148799896\n",
            "epoch:  1341  loss:  0.16779464483261108\n",
            "epoch:  1342  loss:  0.1677798181772232\n",
            "epoch:  1343  loss:  0.16776500642299652\n",
            "epoch:  1344  loss:  0.16775019466876984\n",
            "epoch:  1345  loss:  0.16773541271686554\n",
            "epoch:  1346  loss:  0.16772064566612244\n",
            "epoch:  1347  loss:  0.16770587861537933\n",
            "epoch:  1348  loss:  0.16769112646579742\n",
            "epoch:  1349  loss:  0.1676763892173767\n",
            "epoch:  1350  loss:  0.167661651968956\n",
            "epoch:  1351  loss:  0.16764692962169647\n",
            "epoch:  1352  loss:  0.16763220727443695\n",
            "epoch:  1353  loss:  0.167617529630661\n",
            "epoch:  1354  loss:  0.16760283708572388\n",
            "epoch:  1355  loss:  0.16758815944194794\n",
            "epoch:  1356  loss:  0.16757351160049438\n",
            "epoch:  1357  loss:  0.16755884885787964\n",
            "epoch:  1358  loss:  0.16754421591758728\n",
            "epoch:  1359  loss:  0.16752959787845612\n",
            "epoch:  1360  loss:  0.16751496493816376\n",
            "epoch:  1361  loss:  0.1675003618001938\n",
            "epoch:  1362  loss:  0.16748575866222382\n",
            "epoch:  1363  loss:  0.16747118532657623\n",
            "epoch:  1364  loss:  0.16745661199092865\n",
            "epoch:  1365  loss:  0.16744206845760345\n",
            "epoch:  1366  loss:  0.16742752492427826\n",
            "epoch:  1367  loss:  0.16741296648979187\n",
            "epoch:  1368  loss:  0.16739845275878906\n",
            "epoch:  1369  loss:  0.16738393902778625\n",
            "epoch:  1370  loss:  0.16736942529678345\n",
            "epoch:  1371  loss:  0.16735494136810303\n",
            "epoch:  1372  loss:  0.1673404574394226\n",
            "epoch:  1373  loss:  0.16732598841190338\n",
            "epoch:  1374  loss:  0.16731151938438416\n",
            "epoch:  1375  loss:  0.16729708015918732\n",
            "epoch:  1376  loss:  0.16728265583515167\n",
            "epoch:  1377  loss:  0.16726820170879364\n",
            "epoch:  1378  loss:  0.1672537922859192\n",
            "epoch:  1379  loss:  0.16723941266536713\n",
            "epoch:  1380  loss:  0.16722501814365387\n",
            "epoch:  1381  loss:  0.167210653424263\n",
            "epoch:  1382  loss:  0.16719628870487213\n",
            "epoch:  1383  loss:  0.16718190908432007\n",
            "epoch:  1384  loss:  0.1671675741672516\n",
            "epoch:  1385  loss:  0.1671532243490219\n",
            "epoch:  1386  loss:  0.16713890433311462\n",
            "epoch:  1387  loss:  0.16712459921836853\n",
            "epoch:  1388  loss:  0.16711029410362244\n",
            "epoch:  1389  loss:  0.16709598898887634\n",
            "epoch:  1390  loss:  0.16708172857761383\n",
            "epoch:  1391  loss:  0.16706745326519012\n",
            "epoch:  1392  loss:  0.1670531928539276\n",
            "epoch:  1393  loss:  0.1670389622449875\n",
            "epoch:  1394  loss:  0.16702471673488617\n",
            "epoch:  1395  loss:  0.16701048612594604\n",
            "epoch:  1396  loss:  0.1669962853193283\n",
            "epoch:  1397  loss:  0.16698206961154938\n",
            "epoch:  1398  loss:  0.16696788370609283\n",
            "epoch:  1399  loss:  0.1669536978006363\n",
            "epoch:  1400  loss:  0.16693951189517975\n",
            "epoch:  1401  loss:  0.1669253706932068\n",
            "epoch:  1402  loss:  0.16691122949123383\n",
            "epoch:  1403  loss:  0.16689707338809967\n",
            "epoch:  1404  loss:  0.1668829619884491\n",
            "epoch:  1405  loss:  0.16686883568763733\n",
            "epoch:  1406  loss:  0.16685473918914795\n",
            "epoch:  1407  loss:  0.16684064269065857\n",
            "epoch:  1408  loss:  0.16682657599449158\n",
            "epoch:  1409  loss:  0.1668124943971634\n",
            "epoch:  1410  loss:  0.1667984277009964\n",
            "epoch:  1411  loss:  0.1667843908071518\n",
            "epoch:  1412  loss:  0.166770339012146\n",
            "epoch:  1413  loss:  0.16675631701946259\n",
            "epoch:  1414  loss:  0.16674229502677917\n",
            "epoch:  1415  loss:  0.16672827303409576\n",
            "epoch:  1416  loss:  0.16671431064605713\n",
            "epoch:  1417  loss:  0.1667003184556961\n",
            "epoch:  1418  loss:  0.16668632626533508\n",
            "epoch:  1419  loss:  0.16667236387729645\n",
            "epoch:  1420  loss:  0.166658416390419\n",
            "epoch:  1421  loss:  0.16664446890354156\n",
            "epoch:  1422  loss:  0.16663053631782532\n",
            "epoch:  1423  loss:  0.16661660373210907\n",
            "epoch:  1424  loss:  0.1666027009487152\n",
            "epoch:  1425  loss:  0.16658881306648254\n",
            "epoch:  1426  loss:  0.16657491028308868\n",
            "epoch:  1427  loss:  0.1665610373020172\n",
            "epoch:  1428  loss:  0.16654717922210693\n",
            "epoch:  1429  loss:  0.16653329133987427\n",
            "epoch:  1430  loss:  0.16651946306228638\n",
            "epoch:  1431  loss:  0.1665056347846985\n",
            "epoch:  1432  loss:  0.1664918065071106\n",
            "epoch:  1433  loss:  0.1664779782295227\n",
            "epoch:  1434  loss:  0.1664641946554184\n",
            "epoch:  1435  loss:  0.1664503961801529\n",
            "epoch:  1436  loss:  0.1664365977048874\n",
            "epoch:  1437  loss:  0.16642285883426666\n",
            "epoch:  1438  loss:  0.16640907526016235\n",
            "epoch:  1439  loss:  0.16639530658721924\n",
            "epoch:  1440  loss:  0.1663815826177597\n",
            "epoch:  1441  loss:  0.16636784374713898\n",
            "epoch:  1442  loss:  0.16635413467884064\n",
            "epoch:  1443  loss:  0.1663404256105423\n",
            "epoch:  1444  loss:  0.16632671654224396\n",
            "epoch:  1445  loss:  0.166313037276268\n",
            "epoch:  1446  loss:  0.16629937291145325\n",
            "epoch:  1447  loss:  0.1662856787443161\n",
            "epoch:  1448  loss:  0.16627202928066254\n",
            "epoch:  1449  loss:  0.16625837981700897\n",
            "epoch:  1450  loss:  0.1662447452545166\n",
            "epoch:  1451  loss:  0.16623112559318542\n",
            "epoch:  1452  loss:  0.16621750593185425\n",
            "epoch:  1453  loss:  0.16620390117168427\n",
            "epoch:  1454  loss:  0.16619032621383667\n",
            "epoch:  1455  loss:  0.1661767214536667\n",
            "epoch:  1456  loss:  0.1661631464958191\n",
            "epoch:  1457  loss:  0.16614960134029388\n",
            "epoch:  1458  loss:  0.1661360263824463\n",
            "epoch:  1459  loss:  0.16612248122692108\n",
            "epoch:  1460  loss:  0.16610895097255707\n",
            "epoch:  1461  loss:  0.16609543561935425\n",
            "epoch:  1462  loss:  0.16608193516731262\n",
            "epoch:  1463  loss:  0.1660684198141098\n",
            "epoch:  1464  loss:  0.16605493426322937\n",
            "epoch:  1465  loss:  0.16604144871234894\n",
            "epoch:  1466  loss:  0.1660279780626297\n",
            "epoch:  1467  loss:  0.16601450741291046\n",
            "epoch:  1468  loss:  0.16600105166435242\n",
            "epoch:  1469  loss:  0.16598761081695557\n",
            "epoch:  1470  loss:  0.1659741848707199\n",
            "epoch:  1471  loss:  0.16596077382564545\n",
            "epoch:  1472  loss:  0.16594736278057098\n",
            "epoch:  1473  loss:  0.1659339815378189\n",
            "epoch:  1474  loss:  0.16592055559158325\n",
            "epoch:  1475  loss:  0.16590718924999237\n",
            "epoch:  1476  loss:  0.1658938229084015\n",
            "epoch:  1477  loss:  0.1658804565668106\n",
            "epoch:  1478  loss:  0.16586710512638092\n",
            "epoch:  1479  loss:  0.16585376858711243\n",
            "epoch:  1480  loss:  0.16584043204784393\n",
            "epoch:  1481  loss:  0.16582714021205902\n",
            "epoch:  1482  loss:  0.16581383347511292\n",
            "epoch:  1483  loss:  0.16580051183700562\n",
            "epoch:  1484  loss:  0.1657872498035431\n",
            "epoch:  1485  loss:  0.16577394306659698\n",
            "epoch:  1486  loss:  0.16576068103313446\n",
            "epoch:  1487  loss:  0.16574741899967194\n",
            "epoch:  1488  loss:  0.1657341867685318\n",
            "epoch:  1489  loss:  0.16572095453739166\n",
            "epoch:  1490  loss:  0.16570772230625153\n",
            "epoch:  1491  loss:  0.16569450497627258\n",
            "epoch:  1492  loss:  0.16568128764629364\n",
            "epoch:  1493  loss:  0.1656680852174759\n",
            "epoch:  1494  loss:  0.16565491259098053\n",
            "epoch:  1495  loss:  0.16564172506332397\n",
            "epoch:  1496  loss:  0.1656285524368286\n",
            "epoch:  1497  loss:  0.16561537981033325\n",
            "epoch:  1498  loss:  0.16560226678848267\n",
            "epoch:  1499  loss:  0.1655891090631485\n",
            "epoch:  1500  loss:  0.16557596623897552\n",
            "epoch:  1501  loss:  0.16556286811828613\n",
            "epoch:  1502  loss:  0.16554976999759674\n",
            "epoch:  1503  loss:  0.16553667187690735\n",
            "epoch:  1504  loss:  0.16552357375621796\n",
            "epoch:  1505  loss:  0.16551047563552856\n",
            "epoch:  1506  loss:  0.16549742221832275\n",
            "epoch:  1507  loss:  0.16548436880111694\n",
            "epoch:  1508  loss:  0.16547131538391113\n",
            "epoch:  1509  loss:  0.16545827686786652\n",
            "epoch:  1510  loss:  0.1654452383518219\n",
            "epoch:  1511  loss:  0.16543222963809967\n",
            "epoch:  1512  loss:  0.16541922092437744\n",
            "epoch:  1513  loss:  0.1654062122106552\n",
            "epoch:  1514  loss:  0.16539324820041656\n",
            "epoch:  1515  loss:  0.16538025438785553\n",
            "epoch:  1516  loss:  0.1653672754764557\n",
            "epoch:  1517  loss:  0.16535432636737823\n",
            "epoch:  1518  loss:  0.1653413623571396\n",
            "epoch:  1519  loss:  0.16532842814922333\n",
            "epoch:  1520  loss:  0.16531550884246826\n",
            "epoch:  1521  loss:  0.165302574634552\n",
            "epoch:  1522  loss:  0.16528965532779694\n",
            "epoch:  1523  loss:  0.16527676582336426\n",
            "epoch:  1524  loss:  0.16526386141777039\n",
            "epoch:  1525  loss:  0.1652509868144989\n",
            "epoch:  1526  loss:  0.16523811221122742\n",
            "epoch:  1527  loss:  0.16522525250911713\n",
            "epoch:  1528  loss:  0.16521240770816803\n",
            "epoch:  1529  loss:  0.16519954800605774\n",
            "epoch:  1530  loss:  0.16518671810626984\n",
            "epoch:  1531  loss:  0.16517390310764313\n",
            "epoch:  1532  loss:  0.16516107320785522\n",
            "epoch:  1533  loss:  0.1651482880115509\n",
            "epoch:  1534  loss:  0.1651354730129242\n",
            "epoch:  1535  loss:  0.16512270271778107\n",
            "epoch:  1536  loss:  0.16510990262031555\n",
            "epoch:  1537  loss:  0.16509713232517242\n",
            "epoch:  1538  loss:  0.16508440673351288\n",
            "epoch:  1539  loss:  0.16507165133953094\n",
            "epoch:  1540  loss:  0.165058895945549\n",
            "epoch:  1541  loss:  0.16504618525505066\n",
            "epoch:  1542  loss:  0.16503344476222992\n",
            "epoch:  1543  loss:  0.16502073407173157\n",
            "epoch:  1544  loss:  0.1650080382823944\n",
            "epoch:  1545  loss:  0.16499534249305725\n",
            "epoch:  1546  loss:  0.16498267650604248\n",
            "epoch:  1547  loss:  0.1649700105190277\n",
            "epoch:  1548  loss:  0.16495734453201294\n",
            "epoch:  1549  loss:  0.16494469344615936\n",
            "epoch:  1550  loss:  0.16493205726146698\n",
            "epoch:  1551  loss:  0.1649194210767746\n",
            "epoch:  1552  loss:  0.1649067997932434\n",
            "epoch:  1553  loss:  0.16489416360855103\n",
            "epoch:  1554  loss:  0.16488155722618103\n",
            "epoch:  1555  loss:  0.16486895084381104\n",
            "epoch:  1556  loss:  0.16485637426376343\n",
            "epoch:  1557  loss:  0.16484379768371582\n",
            "epoch:  1558  loss:  0.1648312360048294\n",
            "epoch:  1559  loss:  0.1648186594247818\n",
            "epoch:  1560  loss:  0.16480612754821777\n",
            "epoch:  1561  loss:  0.16479359567165375\n",
            "epoch:  1562  loss:  0.16478106379508972\n",
            "epoch:  1563  loss:  0.1647685170173645\n",
            "epoch:  1564  loss:  0.16475604474544525\n",
            "epoch:  1565  loss:  0.16474352777004242\n",
            "epoch:  1566  loss:  0.16473105549812317\n",
            "epoch:  1567  loss:  0.16471855342388153\n",
            "epoch:  1568  loss:  0.16470608115196228\n",
            "epoch:  1569  loss:  0.16469362378120422\n",
            "epoch:  1570  loss:  0.16468115150928497\n",
            "epoch:  1571  loss:  0.1646687090396881\n",
            "epoch:  1572  loss:  0.16465628147125244\n",
            "epoch:  1573  loss:  0.16464382410049438\n",
            "epoch:  1574  loss:  0.1646314114332199\n",
            "epoch:  1575  loss:  0.16461901366710663\n",
            "epoch:  1576  loss:  0.16460661590099335\n",
            "epoch:  1577  loss:  0.16459421813488007\n",
            "epoch:  1578  loss:  0.16458185017108917\n",
            "epoch:  1579  loss:  0.1645694524049759\n",
            "epoch:  1580  loss:  0.1645570993423462\n",
            "epoch:  1581  loss:  0.16454476118087769\n",
            "epoch:  1582  loss:  0.1645323932170868\n",
            "epoch:  1583  loss:  0.16452008485794067\n",
            "epoch:  1584  loss:  0.16450773179531097\n",
            "epoch:  1585  loss:  0.16449543833732605\n",
            "epoch:  1586  loss:  0.16448312997817993\n",
            "epoch:  1587  loss:  0.1644708216190338\n",
            "epoch:  1588  loss:  0.16445855796337128\n",
            "epoch:  1589  loss:  0.16444626450538635\n",
            "epoch:  1590  loss:  0.16443398594856262\n",
            "epoch:  1591  loss:  0.1644217073917389\n",
            "epoch:  1592  loss:  0.16440947353839874\n",
            "epoch:  1593  loss:  0.1643972098827362\n",
            "epoch:  1594  loss:  0.16438497602939606\n",
            "epoch:  1595  loss:  0.1643727719783783\n",
            "epoch:  1596  loss:  0.16436053812503815\n",
            "epoch:  1597  loss:  0.16434833407402039\n",
            "epoch:  1598  loss:  0.16433613002300262\n",
            "epoch:  1599  loss:  0.16432392597198486\n",
            "epoch:  1600  loss:  0.16431176662445068\n",
            "epoch:  1601  loss:  0.1642996072769165\n",
            "epoch:  1602  loss:  0.16428743302822113\n",
            "epoch:  1603  loss:  0.16427527368068695\n",
            "epoch:  1604  loss:  0.16426312923431396\n",
            "epoch:  1605  loss:  0.16425099968910217\n",
            "epoch:  1606  loss:  0.16423887014389038\n",
            "epoch:  1607  loss:  0.1642267405986786\n",
            "epoch:  1608  loss:  0.16421464085578918\n",
            "epoch:  1609  loss:  0.16420254111289978\n",
            "epoch:  1610  loss:  0.16419045627117157\n",
            "epoch:  1611  loss:  0.16417838633060455\n",
            "epoch:  1612  loss:  0.16416630148887634\n",
            "epoch:  1613  loss:  0.16415424644947052\n",
            "epoch:  1614  loss:  0.1641421765089035\n",
            "epoch:  1615  loss:  0.16413012146949768\n",
            "epoch:  1616  loss:  0.16411809623241425\n",
            "epoch:  1617  loss:  0.164106085896492\n",
            "epoch:  1618  loss:  0.16409406065940857\n",
            "epoch:  1619  loss:  0.16408203542232513\n",
            "epoch:  1620  loss:  0.16407005488872528\n",
            "epoch:  1621  loss:  0.16405804455280304\n",
            "epoch:  1622  loss:  0.16404606401920319\n",
            "epoch:  1623  loss:  0.16403408348560333\n",
            "epoch:  1624  loss:  0.16402211785316467\n",
            "epoch:  1625  loss:  0.1640101820230484\n",
            "epoch:  1626  loss:  0.16399821639060974\n",
            "epoch:  1627  loss:  0.16398628056049347\n",
            "epoch:  1628  loss:  0.1639743447303772\n",
            "epoch:  1629  loss:  0.1639624387025833\n",
            "epoch:  1630  loss:  0.16395051777362823\n",
            "epoch:  1631  loss:  0.16393861174583435\n",
            "epoch:  1632  loss:  0.16392670571804047\n",
            "epoch:  1633  loss:  0.16391482949256897\n",
            "epoch:  1634  loss:  0.16390295326709747\n",
            "epoch:  1635  loss:  0.16389106214046478\n",
            "epoch:  1636  loss:  0.16387923061847687\n",
            "epoch:  1637  loss:  0.16386736929416656\n",
            "epoch:  1638  loss:  0.16385550796985626\n",
            "epoch:  1639  loss:  0.16384369134902954\n",
            "epoch:  1640  loss:  0.16383187472820282\n",
            "epoch:  1641  loss:  0.1638200581073761\n",
            "epoch:  1642  loss:  0.16380824148654938\n",
            "epoch:  1643  loss:  0.16379645466804504\n",
            "epoch:  1644  loss:  0.16378465294837952\n",
            "epoch:  1645  loss:  0.16377286612987518\n",
            "epoch:  1646  loss:  0.16376110911369324\n",
            "epoch:  1647  loss:  0.1637493222951889\n",
            "epoch:  1648  loss:  0.16373756527900696\n",
            "epoch:  1649  loss:  0.1637258231639862\n",
            "epoch:  1650  loss:  0.16371408104896545\n",
            "epoch:  1651  loss:  0.1637023687362671\n",
            "epoch:  1652  loss:  0.16369064152240753\n",
            "epoch:  1653  loss:  0.16367892920970917\n",
            "epoch:  1654  loss:  0.163667231798172\n",
            "epoch:  1655  loss:  0.16365553438663483\n",
            "epoch:  1656  loss:  0.16364382207393646\n",
            "epoch:  1657  loss:  0.16363216936588287\n",
            "epoch:  1658  loss:  0.1636205017566681\n",
            "epoch:  1659  loss:  0.16360881924629211\n",
            "epoch:  1660  loss:  0.16359716653823853\n",
            "epoch:  1661  loss:  0.16358554363250732\n",
            "epoch:  1662  loss:  0.16357389092445374\n",
            "epoch:  1663  loss:  0.16356226801872253\n",
            "epoch:  1664  loss:  0.16355066001415253\n",
            "epoch:  1665  loss:  0.16353905200958252\n",
            "epoch:  1666  loss:  0.1635274440050125\n",
            "epoch:  1667  loss:  0.1635158509016037\n",
            "epoch:  1668  loss:  0.16350427269935608\n",
            "epoch:  1669  loss:  0.16349270939826965\n",
            "epoch:  1670  loss:  0.16348114609718323\n",
            "epoch:  1671  loss:  0.1634695678949356\n",
            "epoch:  1672  loss:  0.16345800459384918\n",
            "epoch:  1673  loss:  0.16344648599624634\n",
            "epoch:  1674  loss:  0.1634349524974823\n",
            "epoch:  1675  loss:  0.16342341899871826\n",
            "epoch:  1676  loss:  0.16341190040111542\n",
            "epoch:  1677  loss:  0.16340039670467377\n",
            "epoch:  1678  loss:  0.1633889079093933\n",
            "epoch:  1679  loss:  0.16337740421295166\n",
            "epoch:  1680  loss:  0.16336590051651\n",
            "epoch:  1681  loss:  0.16335444152355194\n",
            "epoch:  1682  loss:  0.16334298253059387\n",
            "epoch:  1683  loss:  0.1633315086364746\n",
            "epoch:  1684  loss:  0.16332007944583893\n",
            "epoch:  1685  loss:  0.16330862045288086\n",
            "epoch:  1686  loss:  0.16329719126224518\n",
            "epoch:  1687  loss:  0.1632857769727707\n",
            "epoch:  1688  loss:  0.163274347782135\n",
            "epoch:  1689  loss:  0.16326293349266052\n",
            "epoch:  1690  loss:  0.16325153410434723\n",
            "epoch:  1691  loss:  0.16324016451835632\n",
            "epoch:  1692  loss:  0.16322879493236542\n",
            "epoch:  1693  loss:  0.16321739554405212\n",
            "epoch:  1694  loss:  0.1632060408592224\n",
            "epoch:  1695  loss:  0.1631946861743927\n",
            "epoch:  1696  loss:  0.163183331489563\n",
            "epoch:  1697  loss:  0.16317200660705566\n",
            "epoch:  1698  loss:  0.16316066682338715\n",
            "epoch:  1699  loss:  0.16314932703971863\n",
            "epoch:  1700  loss:  0.1631380319595337\n",
            "epoch:  1701  loss:  0.16312669217586517\n",
            "epoch:  1702  loss:  0.16311541199684143\n",
            "epoch:  1703  loss:  0.1631041318178177\n",
            "epoch:  1704  loss:  0.16309283673763275\n",
            "epoch:  1705  loss:  0.1630815714597702\n",
            "epoch:  1706  loss:  0.16307027637958527\n",
            "epoch:  1707  loss:  0.1630590558052063\n",
            "epoch:  1708  loss:  0.16304779052734375\n",
            "epoch:  1709  loss:  0.1630365550518036\n",
            "epoch:  1710  loss:  0.16302533447742462\n",
            "epoch:  1711  loss:  0.16301409900188446\n",
            "epoch:  1712  loss:  0.1630028784275055\n",
            "epoch:  1713  loss:  0.16299167275428772\n",
            "epoch:  1714  loss:  0.16298046708106995\n",
            "epoch:  1715  loss:  0.16296926140785217\n",
            "epoch:  1716  loss:  0.1629580855369568\n",
            "epoch:  1717  loss:  0.1629468947649002\n",
            "epoch:  1718  loss:  0.1629357486963272\n",
            "epoch:  1719  loss:  0.16292458772659302\n",
            "epoch:  1720  loss:  0.16291342675685883\n",
            "epoch:  1721  loss:  0.16290229558944702\n",
            "epoch:  1722  loss:  0.16289114952087402\n",
            "epoch:  1723  loss:  0.1628800332546234\n",
            "epoch:  1724  loss:  0.1628689169883728\n",
            "epoch:  1725  loss:  0.162857785820961\n",
            "epoch:  1726  loss:  0.16284669935703278\n",
            "epoch:  1727  loss:  0.16283559799194336\n",
            "epoch:  1728  loss:  0.16282451152801514\n",
            "epoch:  1729  loss:  0.16281342506408691\n",
            "epoch:  1730  loss:  0.16280236840248108\n",
            "epoch:  1731  loss:  0.16279129683971405\n",
            "epoch:  1732  loss:  0.16278024017810822\n",
            "epoch:  1733  loss:  0.16276921331882477\n",
            "epoch:  1734  loss:  0.16275815665721893\n",
            "epoch:  1735  loss:  0.16274712979793549\n",
            "epoch:  1736  loss:  0.16273610293865204\n",
            "epoch:  1737  loss:  0.16272509098052979\n",
            "epoch:  1738  loss:  0.16271407902240753\n",
            "epoch:  1739  loss:  0.16270309686660767\n",
            "epoch:  1740  loss:  0.1626920849084854\n",
            "epoch:  1741  loss:  0.16268108785152435\n",
            "epoch:  1742  loss:  0.16267013549804688\n",
            "epoch:  1743  loss:  0.1626591831445694\n",
            "epoch:  1744  loss:  0.16264821588993073\n",
            "epoch:  1745  loss:  0.16263723373413086\n",
            "epoch:  1746  loss:  0.16262631118297577\n",
            "epoch:  1747  loss:  0.16261537373065948\n",
            "epoch:  1748  loss:  0.1626044362783432\n",
            "epoch:  1749  loss:  0.1625935137271881\n",
            "epoch:  1750  loss:  0.1625826060771942\n",
            "epoch:  1751  loss:  0.1625717133283615\n",
            "epoch:  1752  loss:  0.1625608205795288\n",
            "epoch:  1753  loss:  0.1625499278306961\n",
            "epoch:  1754  loss:  0.1625390499830246\n",
            "epoch:  1755  loss:  0.1625281721353531\n",
            "epoch:  1756  loss:  0.16251732409000397\n",
            "epoch:  1757  loss:  0.16250646114349365\n",
            "epoch:  1758  loss:  0.16249559819698334\n",
            "epoch:  1759  loss:  0.1624847650527954\n",
            "epoch:  1760  loss:  0.16247393190860748\n",
            "epoch:  1761  loss:  0.16246309876441956\n",
            "epoch:  1762  loss:  0.16245229542255402\n",
            "epoch:  1763  loss:  0.16244147717952728\n",
            "epoch:  1764  loss:  0.16243068873882294\n",
            "epoch:  1765  loss:  0.1624198853969574\n",
            "epoch:  1766  loss:  0.16240909695625305\n",
            "epoch:  1767  loss:  0.1623983532190323\n",
            "epoch:  1768  loss:  0.16238754987716675\n",
            "epoch:  1769  loss:  0.16237680613994598\n",
            "epoch:  1770  loss:  0.16236603260040283\n",
            "epoch:  1771  loss:  0.16235530376434326\n",
            "epoch:  1772  loss:  0.1623445451259613\n",
            "epoch:  1773  loss:  0.16233383119106293\n",
            "epoch:  1774  loss:  0.16232308745384216\n",
            "epoch:  1775  loss:  0.1623123735189438\n",
            "epoch:  1776  loss:  0.1623016744852066\n",
            "epoch:  1777  loss:  0.16229097545146942\n",
            "epoch:  1778  loss:  0.16228029131889343\n",
            "epoch:  1779  loss:  0.16226959228515625\n",
            "epoch:  1780  loss:  0.16225892305374146\n",
            "epoch:  1781  loss:  0.16224823892116547\n",
            "epoch:  1782  loss:  0.16223759949207306\n",
            "epoch:  1783  loss:  0.16222694516181946\n",
            "epoch:  1784  loss:  0.16221627593040466\n",
            "epoch:  1785  loss:  0.16220565140247345\n",
            "epoch:  1786  loss:  0.16219504177570343\n",
            "epoch:  1787  loss:  0.16218441724777222\n",
            "epoch:  1788  loss:  0.16217376291751862\n",
            "epoch:  1789  loss:  0.16216318309307098\n",
            "epoch:  1790  loss:  0.16215258836746216\n",
            "epoch:  1791  loss:  0.16214197874069214\n",
            "epoch:  1792  loss:  0.1621314138174057\n",
            "epoch:  1793  loss:  0.16212081909179688\n",
            "epoch:  1794  loss:  0.16211023926734924\n",
            "epoch:  1795  loss:  0.1620997041463852\n",
            "epoch:  1796  loss:  0.16208913922309875\n",
            "epoch:  1797  loss:  0.16207857429981232\n",
            "epoch:  1798  loss:  0.16206803917884827\n",
            "epoch:  1799  loss:  0.16205748915672302\n",
            "epoch:  1800  loss:  0.16204699873924255\n",
            "epoch:  1801  loss:  0.1620364636182785\n",
            "epoch:  1802  loss:  0.16202595829963684\n",
            "epoch:  1803  loss:  0.16201545298099518\n",
            "epoch:  1804  loss:  0.1620049774646759\n",
            "epoch:  1805  loss:  0.16199448704719543\n",
            "epoch:  1806  loss:  0.16198401153087616\n",
            "epoch:  1807  loss:  0.1619735211133957\n",
            "epoch:  1808  loss:  0.1619630753993988\n",
            "epoch:  1809  loss:  0.16195259988307953\n",
            "epoch:  1810  loss:  0.16194216907024384\n",
            "epoch:  1811  loss:  0.16193172335624695\n",
            "epoch:  1812  loss:  0.16192129254341125\n",
            "epoch:  1813  loss:  0.16191086173057556\n",
            "epoch:  1814  loss:  0.16190044581890106\n",
            "epoch:  1815  loss:  0.16189004480838776\n",
            "epoch:  1816  loss:  0.16187964379787445\n",
            "epoch:  1817  loss:  0.16186922788619995\n",
            "epoch:  1818  loss:  0.16185885667800903\n",
            "epoch:  1819  loss:  0.16184848546981812\n",
            "epoch:  1820  loss:  0.1618380844593048\n",
            "epoch:  1821  loss:  0.1618277132511139\n",
            "epoch:  1822  loss:  0.16181737184524536\n",
            "epoch:  1823  loss:  0.16180700063705444\n",
            "epoch:  1824  loss:  0.1617966592311859\n",
            "epoch:  1825  loss:  0.16178633272647858\n",
            "epoch:  1826  loss:  0.16177600622177124\n",
            "epoch:  1827  loss:  0.1617656797170639\n",
            "epoch:  1828  loss:  0.16175535321235657\n",
            "epoch:  1829  loss:  0.16174505650997162\n",
            "epoch:  1830  loss:  0.16173475980758667\n",
            "epoch:  1831  loss:  0.16172446310520172\n",
            "epoch:  1832  loss:  0.16171416640281677\n",
            "epoch:  1833  loss:  0.1617039144039154\n",
            "epoch:  1834  loss:  0.16169363260269165\n",
            "epoch:  1835  loss:  0.16168338060379028\n",
            "epoch:  1836  loss:  0.16167311370372772\n",
            "epoch:  1837  loss:  0.16166287660598755\n",
            "epoch:  1838  loss:  0.16165262460708618\n",
            "epoch:  1839  loss:  0.1616424024105072\n",
            "epoch:  1840  loss:  0.16163216531276703\n",
            "epoch:  1841  loss:  0.16162194311618805\n",
            "epoch:  1842  loss:  0.16161173582077026\n",
            "epoch:  1843  loss:  0.16160152852535248\n",
            "epoch:  1844  loss:  0.1615913212299347\n",
            "epoch:  1845  loss:  0.1615811437368393\n",
            "epoch:  1846  loss:  0.1615709513425827\n",
            "epoch:  1847  loss:  0.1615607887506485\n",
            "epoch:  1848  loss:  0.1615506261587143\n",
            "epoch:  1849  loss:  0.1615404635667801\n",
            "epoch:  1850  loss:  0.1615303009748459\n",
            "epoch:  1851  loss:  0.16152015328407288\n",
            "epoch:  1852  loss:  0.16151000559329987\n",
            "epoch:  1853  loss:  0.16149987280368805\n",
            "epoch:  1854  loss:  0.16148975491523743\n",
            "epoch:  1855  loss:  0.161479651927948\n",
            "epoch:  1856  loss:  0.16146953403949738\n",
            "epoch:  1857  loss:  0.16145943105220795\n",
            "epoch:  1858  loss:  0.1614493429660797\n",
            "epoch:  1859  loss:  0.16143925487995148\n",
            "epoch:  1860  loss:  0.16142916679382324\n",
            "epoch:  1861  loss:  0.161419078707695\n",
            "epoch:  1862  loss:  0.16140902042388916\n",
            "epoch:  1863  loss:  0.1613989621400833\n",
            "epoch:  1864  loss:  0.16138891875743866\n",
            "epoch:  1865  loss:  0.161378875374794\n",
            "epoch:  1866  loss:  0.16136883199214935\n",
            "epoch:  1867  loss:  0.1613587886095047\n",
            "epoch:  1868  loss:  0.16134878993034363\n",
            "epoch:  1869  loss:  0.16133874654769897\n",
            "epoch:  1870  loss:  0.1613287478685379\n",
            "epoch:  1871  loss:  0.16131871938705444\n",
            "epoch:  1872  loss:  0.16130875051021576\n",
            "epoch:  1873  loss:  0.16129876673221588\n",
            "epoch:  1874  loss:  0.161288782954216\n",
            "epoch:  1875  loss:  0.16127881407737732\n",
            "epoch:  1876  loss:  0.16126884520053864\n",
            "epoch:  1877  loss:  0.16125887632369995\n",
            "epoch:  1878  loss:  0.16124893724918365\n",
            "epoch:  1879  loss:  0.16123898327350616\n",
            "epoch:  1880  loss:  0.16122904419898987\n",
            "epoch:  1881  loss:  0.16121910512447357\n",
            "epoch:  1882  loss:  0.16120916604995728\n",
            "epoch:  1883  loss:  0.16119927167892456\n",
            "epoch:  1884  loss:  0.16118934750556946\n",
            "epoch:  1885  loss:  0.16117943823337555\n",
            "epoch:  1886  loss:  0.16116954386234283\n",
            "epoch:  1887  loss:  0.1611596643924713\n",
            "epoch:  1888  loss:  0.161149799823761\n",
            "epoch:  1889  loss:  0.16113990545272827\n",
            "epoch:  1890  loss:  0.16113002598285675\n",
            "epoch:  1891  loss:  0.16112017631530762\n",
            "epoch:  1892  loss:  0.1611103117465973\n",
            "epoch:  1893  loss:  0.16110047698020935\n",
            "epoch:  1894  loss:  0.1610906422138214\n",
            "epoch:  1895  loss:  0.16108080744743347\n",
            "epoch:  1896  loss:  0.16107098758220673\n",
            "epoch:  1897  loss:  0.1610611528158188\n",
            "epoch:  1898  loss:  0.16105134785175323\n",
            "epoch:  1899  loss:  0.1610415279865265\n",
            "epoch:  1900  loss:  0.16103173792362213\n",
            "epoch:  1901  loss:  0.16102196276187897\n",
            "epoch:  1902  loss:  0.16101215779781342\n",
            "epoch:  1903  loss:  0.16100239753723145\n",
            "epoch:  1904  loss:  0.1609926074743271\n",
            "epoch:  1905  loss:  0.16098284721374512\n",
            "epoch:  1906  loss:  0.16097310185432434\n",
            "epoch:  1907  loss:  0.16096332669258118\n",
            "epoch:  1908  loss:  0.1609536111354828\n",
            "epoch:  1909  loss:  0.160943865776062\n",
            "epoch:  1910  loss:  0.16093415021896362\n",
            "epoch:  1911  loss:  0.16092440485954285\n",
            "epoch:  1912  loss:  0.16091470420360565\n",
            "epoch:  1913  loss:  0.16090500354766846\n",
            "epoch:  1914  loss:  0.16089528799057007\n",
            "epoch:  1915  loss:  0.16088558733463287\n",
            "epoch:  1916  loss:  0.16087590157985687\n",
            "epoch:  1917  loss:  0.16086621582508087\n",
            "epoch:  1918  loss:  0.16085654497146606\n",
            "epoch:  1919  loss:  0.16084687411785126\n",
            "epoch:  1920  loss:  0.16083723306655884\n",
            "epoch:  1921  loss:  0.16082756221294403\n",
            "epoch:  1922  loss:  0.1608179211616516\n",
            "epoch:  1923  loss:  0.16080829501152039\n",
            "epoch:  1924  loss:  0.16079863905906677\n",
            "epoch:  1925  loss:  0.16078901290893555\n",
            "epoch:  1926  loss:  0.16077938675880432\n",
            "epoch:  1927  loss:  0.1607697755098343\n",
            "epoch:  1928  loss:  0.16076019406318665\n",
            "epoch:  1929  loss:  0.16075056791305542\n",
            "epoch:  1930  loss:  0.16074098646640778\n",
            "epoch:  1931  loss:  0.16073141992092133\n",
            "epoch:  1932  loss:  0.1607218235731125\n",
            "epoch:  1933  loss:  0.16071225702762604\n",
            "epoch:  1934  loss:  0.1607026755809784\n",
            "epoch:  1935  loss:  0.16069312393665314\n",
            "epoch:  1936  loss:  0.16068357229232788\n",
            "epoch:  1937  loss:  0.16067402064800262\n",
            "epoch:  1938  loss:  0.16066448390483856\n",
            "epoch:  1939  loss:  0.1606549620628357\n",
            "epoch:  1940  loss:  0.16064541041851044\n",
            "epoch:  1941  loss:  0.16063588857650757\n",
            "epoch:  1942  loss:  0.1606263965368271\n",
            "epoch:  1943  loss:  0.1606168895959854\n",
            "epoch:  1944  loss:  0.16060741245746613\n",
            "epoch:  1945  loss:  0.16059790551662445\n",
            "epoch:  1946  loss:  0.16058842837810516\n",
            "epoch:  1947  loss:  0.16057893633842468\n",
            "epoch:  1948  loss:  0.1605694591999054\n",
            "epoch:  1949  loss:  0.1605599969625473\n",
            "epoch:  1950  loss:  0.1605505496263504\n",
            "epoch:  1951  loss:  0.16054107248783112\n",
            "epoch:  1952  loss:  0.1605316400527954\n",
            "epoch:  1953  loss:  0.1605222076177597\n",
            "epoch:  1954  loss:  0.1605127900838852\n",
            "epoch:  1955  loss:  0.1605033576488495\n",
            "epoch:  1956  loss:  0.16049394011497498\n",
            "epoch:  1957  loss:  0.16048450767993927\n",
            "epoch:  1958  loss:  0.16047513484954834\n",
            "epoch:  1959  loss:  0.16046571731567383\n",
            "epoch:  1960  loss:  0.1604563146829605\n",
            "epoch:  1961  loss:  0.1604469269514084\n",
            "epoch:  1962  loss:  0.16043755412101746\n",
            "epoch:  1963  loss:  0.16042819619178772\n",
            "epoch:  1964  loss:  0.1604188233613968\n",
            "epoch:  1965  loss:  0.16040948033332825\n",
            "epoch:  1966  loss:  0.1604001224040985\n",
            "epoch:  1967  loss:  0.16039076447486877\n",
            "epoch:  1968  loss:  0.16038142144680023\n",
            "epoch:  1969  loss:  0.16037210822105408\n",
            "epoch:  1970  loss:  0.16036276519298553\n",
            "epoch:  1971  loss:  0.16035343706607819\n",
            "epoch:  1972  loss:  0.16034413874149323\n",
            "epoch:  1973  loss:  0.16033481061458588\n",
            "epoch:  1974  loss:  0.16032551229000092\n",
            "epoch:  1975  loss:  0.16031622886657715\n",
            "epoch:  1976  loss:  0.16030694544315338\n",
            "epoch:  1977  loss:  0.16029764711856842\n",
            "epoch:  1978  loss:  0.16028839349746704\n",
            "epoch:  1979  loss:  0.16027911007404327\n",
            "epoch:  1980  loss:  0.1602698564529419\n",
            "epoch:  1981  loss:  0.16026060283184052\n",
            "epoch:  1982  loss:  0.16025136411190033\n",
            "epoch:  1983  loss:  0.16024209558963776\n",
            "epoch:  1984  loss:  0.16023288667201996\n",
            "epoch:  1985  loss:  0.16022363305091858\n",
            "epoch:  1986  loss:  0.1602144092321396\n",
            "epoch:  1987  loss:  0.1602052003145218\n",
            "epoch:  1988  loss:  0.16019600629806519\n",
            "epoch:  1989  loss:  0.1601867973804474\n",
            "epoch:  1990  loss:  0.16017760336399078\n",
            "epoch:  1991  loss:  0.16016840934753418\n",
            "epoch:  1992  loss:  0.16015921533107758\n",
            "epoch:  1993  loss:  0.16015005111694336\n",
            "epoch:  1994  loss:  0.16014087200164795\n",
            "epoch:  1995  loss:  0.16013170778751373\n",
            "epoch:  1996  loss:  0.16012254357337952\n",
            "epoch:  1997  loss:  0.1601133942604065\n",
            "epoch:  1998  loss:  0.16010424494743347\n",
            "epoch:  1999  loss:  0.16009511053562164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgSz4QI2rigC",
        "outputId": "9e56d1d4-060f-4f9c-e4aa-1c01fc42fb20"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.687\n",
            "accuracy 0.755\n",
            "accuracy 0.785\n",
            "accuracy 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(features_sketch_)\n",
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xBBwwYE2kBB",
        "outputId": "7a5c0ec0-30ea-4c4d-8eaa-489029441f17"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.693\n",
            "accuracy 0.755\n",
            "accuracy 0.780\n",
            "accuracy 0.828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(features_sketch_)\n",
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xjLrXD-30wB",
        "outputId": "99f09cac-f5d2-43d3-d990-65f964064aea"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.699\n",
            "accuracy 0.758\n",
            "accuracy 0.783\n",
            "accuracy 0.823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model(features_sketch_)\n",
        "def cos_similarity(k,image_feature,sketch_feature):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  real_matrix = F.normalize(image_feature) \n",
        "  for idx, sketch_ in enumerate(sketch_feature):\n",
        "    feature1 = sketch_.reshape(1,-1)\n",
        "    a = F.normalize(feature1)\n",
        "    res = F.cosine_similarity(a,real_matrix)\n",
        "    _,predict = res.topk(k, largest=True, sorted=True)\n",
        "    for i in predict:\n",
        "      if idx//100 == i // 100:\n",
        "        correct += 1\n",
        "        break\n",
        "    total += 1\n",
        "\n",
        "  print(f\"accuracy {(correct/total):.3f}\")  \n",
        "\n",
        "cos_similarity(1,feature_real_image_,y_pred)\n",
        "cos_similarity(3,feature_real_image_,y_pred)\n",
        "cos_similarity(5,feature_real_image_,y_pred)\n",
        "cos_similarity(10,feature_real_image_,y_pred)\n",
        "# cos_similarity(3,fea1,fea2)\n",
        "# cos_similarity(5,fea1,fea2)\n",
        "# cos_similarity(10,fea1,fea2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ak_VTY7AZnu",
        "outputId": "21a2c532-e333-44c7-ca46-721c194c83b1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.700\n",
            "accuracy 0.759\n",
            "accuracy 0.782\n",
            "accuracy 0.819\n"
          ]
        }
      ]
    }
  ]
}